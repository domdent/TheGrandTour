{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/dom/Documents/MPhys/TheGrandTour/wine_data.txt\", sep=\"\\t\");\n",
    "data = np.array(df);\n",
    "data = np.delete(data, 0, 0)\n",
    "data = data.astype(float)\n",
    "data = np.swapaxes(data,0,1)\n",
    "\n",
    "classification = data[13]\n",
    "data = np.delete(data, 13, axis=0)\n",
    "# Normalizes the data        \n",
    "for i in range(0, np.shape(data)[0]):\n",
    "    data[i,:] = (data[i,:] / np.ndarray.max(data[i,:])) * 2 - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepSize = 0.01\n",
    "nSteps = 10000\n",
    "\n",
    "def getAlpha(d):\n",
    "    \"\"\"\n",
    "    NEEDS IMPLEMENTATION\n",
    "    Should produce 1xd(d-1)/2 array of position in grand tour.\n",
    "    \"\"\"\n",
    "    p = d*(d-1)/2     \n",
    "    primeList = []\n",
    "    count = 1\n",
    "    while len(primeList) < p:\n",
    "        count += 1\n",
    "        primeBool = False\n",
    "        for i in range(2, count - 1):\n",
    "            if count % i == 0:\n",
    "                primeBool = True\n",
    "        if primeBool == False:\n",
    "            irrational = (np.sqrt(count)%1)\n",
    "            primeList.append(irrational)\n",
    "            \n",
    "    primeList = np.asarray(primeList)\n",
    "    primeList = primeList.dot(stepSize)\n",
    "    \"\"\"\n",
    "    Irrational number generation using exponentials, not being used\n",
    "    p = int(d*(d-1)/2)\n",
    "    alpha = np.zeros(p) #alpha(t) parameters defining grand tour in G2,d\n",
    "    for i in range(0,p):\n",
    "        alpha[i] = (np.exp(i) % 1) * 2 * np.pi\n",
    "        \n",
    "    alpha = alpha.dot(0.001)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return primeList\n",
    "\n",
    "\n",
    "def getAngles(alpha,d):\n",
    "    \"\"\"\"\"\n",
    "    Inputs: \n",
    "    alpha = 1xd(d-1)/2 array defining position on grand tour\n",
    "    d = dimensions of data\n",
    "    Outputs a dxd array of angles required for the transformation\n",
    "    \"\"\"\n",
    "    theta = np.zeros((d,d));\n",
    "    i = 0;\n",
    "    k = 0;\n",
    "    \n",
    "    while i < d-1:\n",
    "        j = i + 1;\n",
    "        \n",
    "        while j < d:\n",
    "            theta[i][j] = alpha[k];\n",
    "            j += 1;\n",
    "            k += 1;\n",
    "    \n",
    "        i+= 1;\n",
    "        \n",
    "    return theta;\n",
    "\n",
    "\n",
    "def RotationMatrix(i, j, d, theta):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    i = first indicie of rotating plane\n",
    "    j = second indicie of rotating plane\n",
    "    d = dimensions of data\n",
    "    theta = dxd array of angle of rotation of rotating plane\n",
    "    Outputs a rotating matrix to rotate plane of ixj plane by theta_ij\n",
    "    \"\"\"\n",
    "    R = np.identity(d)\n",
    "    R[i,i] = np.cos(theta)\n",
    "    R[i,j] = -1*np.sin(theta)\n",
    "    R[j,i] = np.sin(theta)\n",
    "    R[j,j] = np.cos(theta)\n",
    "    return R\n",
    "\n",
    "\n",
    "def BetaFn(d, theta):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    d = dimensions of data\n",
    "    theta = dxd array of angle of rotation ixj plane\n",
    "    Outputs the full matrix transformation for all rotations\n",
    "    \"\"\"\n",
    "    b = RotationMatrix(1, 2, d, theta[1,2])\n",
    "    i = 1\n",
    "    j = 2\n",
    "    for i in range(d):\n",
    "        for j in range(d):\n",
    "            if j <= i:\n",
    "                continue\n",
    "            if i==1 and j==2:\n",
    "                continue\n",
    "            b = np.matmul(b, RotationMatrix(i, j, d, theta[i,j]))\n",
    "            \n",
    "    return b\n",
    "\n",
    "\n",
    "def GrandTour(data, nSteps):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    data = array of data points, dimensions x npoints\n",
    "    Outputs a 3D array number of points x t x dimensions, where t\n",
    "    the time step at that point in the tour\n",
    "    \"\"\"\n",
    "\n",
    "    d = np.shape(data)[0] #dimensions of data\n",
    "    nPoints = np.shape(data)[1] #number of data points\n",
    "    tData = np.zeros((nSteps,d,nPoints)) #initialise 3d matrix to store stransforemd data at each timestep\n",
    "    tBeta = np.zeros((nSteps,d,d))\n",
    "    Alpha = getAlpha(d)\n",
    "\n",
    "    \n",
    "    for t in range(0, nSteps):\n",
    "        \n",
    "        \n",
    "        alpha = Alpha.dot(t)\n",
    "        theta = getAngles(alpha, d)\n",
    "        b = BetaFn(d, theta)\n",
    "        a = np.matmul(b, data)\n",
    "        tData[t,:,:] = a\n",
    "        tBeta[t,:,:] = b\n",
    "        \n",
    "    return tData, tBeta\n",
    "\n",
    "\n",
    "tData, tBeta = GrandTour(data, nSteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetData = np.zeros((len(tData[0][0]), 3))\n",
    "for counter, i in enumerate(classification):\n",
    "    targetData[counter][int(i-1)] = 1\n",
    "targetData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(data)\n",
    "y = torch.from_numpy(targetData)\n",
    "print(x.size()[0])\n",
    "print(y.size()[1])\n",
    "x.transpose_(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([177, 13])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.size())\n",
    "x.transpose_(0, 1)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7802, 0.7748, 0.9380, 0.7856, 0.9150, 0.9407, 0.8962, 1.0000, 0.8692,\n",
      "        0.9016, 0.9042, 0.8543, 0.9892, 0.9393, 0.8382, 0.9285, 0.8651, 0.9137,\n",
      "        0.8395, 0.8962, 0.7438, 0.8490, 0.7330, 0.8206, 0.7599, 0.8058, 0.7937,\n",
      "        0.8705, 0.8908, 0.8517, 0.8314, 0.8449, 0.8557, 0.8220, 0.8179, 0.7910,\n",
      "        0.7599, 0.7626, 0.9177, 0.8287, 0.8085, 0.8719, 0.7856, 0.7599, 0.9164,\n",
      "        0.9393, 0.8746, 0.9016, 0.8800, 0.7599, 0.8651, 0.8638, 0.8570, 0.8530,\n",
      "        0.8287, 0.9177, 0.7923, 0.8503, 0.6682, 0.6628, 0.7047, 0.8436, 0.6682,\n",
      "        0.6413, 0.6682, 0.7680, 0.6682, 0.7991, 0.6467, 0.6575, 0.8692, 0.8193,\n",
      "        0.7519, 0.6129, 0.5725, 0.7572, 0.5968, 0.6628, 0.7127, 0.6183, 0.7154,\n",
      "        0.6291, 0.7599, 0.5968, 0.7087, 0.6399, 0.5711, 0.5698, 0.6291, 0.6291,\n",
      "        0.6183, 0.7114, 0.6575, 0.5671, 0.6817, 0.5927, 0.6575, 0.6682, 0.6575,\n",
      "        0.6291, 0.6993, 0.6642, 0.5941, 0.6871, 0.6750, 0.6521, 0.7154, 0.6480,\n",
      "        0.5657, 0.5455, 0.6885, 0.5860, 0.5388, 0.6291, 0.4875, 0.5941, 0.6750,\n",
      "        0.7222, 0.6183, 0.5442, 0.5590, 0.6750, 0.7599, 0.6008, 0.6278, 0.6763,\n",
      "        0.5900, 0.6682, 0.6237, 0.7343, 0.7370, 0.7276, 0.7127, 0.6871, 0.6993,\n",
      "        0.6521, 0.6898, 0.8193, 0.7316, 0.7438, 0.8018, 0.8233, 0.8368, 0.6521,\n",
      "        0.7748, 0.8719, 0.7357, 0.7964, 0.7640, 0.8206, 0.7249, 0.7680, 0.7842,\n",
      "        0.6966, 0.7761, 0.8665, 0.6790, 0.9339, 0.8179, 0.6669, 0.8463, 0.7330,\n",
      "        0.7478, 0.8584, 0.8517, 0.8139, 0.7289, 0.8314, 0.8071, 0.6453, 0.7222,\n",
      "        0.9096, 0.8490, 0.8071, 0.7896, 0.7761, 0.9056], dtype=torch.float64)\n",
      "tensor([-0.3862, -0.1862, -0.3276, -0.1069, -0.3931, -0.3552, -0.2586, -0.4345,\n",
      "        -0.5345, -0.2552, -0.4897, -0.4034, -0.4034, -0.3552, -0.3759, -0.3379,\n",
      "        -0.4586, -0.4517,  0.0690, -0.4379,  0.3103, -0.3586, -0.4483, -0.3759,\n",
      "        -0.2931, -0.3897, -0.4069, -0.3448, -0.4207, -0.4828, -0.4276, -0.3690,\n",
      "        -0.4724, -0.3793, -0.3759, -0.4345, -0.4310, -0.4828,  0.3759, -0.4103,\n",
      "         0.3241, -0.3483,  0.3724, -0.3897,  0.3931,  0.2379, -0.4207, -0.3034,\n",
      "        -0.4034, -0.4034, -0.4310, -0.3966, -0.3448, -0.4241, -0.4034, -0.4138,\n",
      "        -0.3207, -0.5069, -0.6759, -0.6207, -0.5310, -0.5690, -0.6103, -0.5000,\n",
      "        -0.5828, -0.6517, -0.5966, -0.6759, -0.5897, -0.4448, -0.4793, -0.4276,\n",
      "        -0.4241, -0.6241, -0.3517, -0.6897, -0.0034, -0.6586,  0.3345, -0.6828,\n",
      "        -0.3759, -0.6103,  0.3310, -0.6931, -0.6621, -0.4448, -0.4241, -0.2897,\n",
      "        -0.5414, -0.3690, -0.4793, -0.4724, -0.0241, -0.3138, -0.4759, -0.2690,\n",
      "        -0.5138, -0.6310,  0.0931, -0.2828, -0.5379, -0.1552, -0.4069, -0.4034,\n",
      "        -0.1207, -0.4034, -0.3966, -0.5552, -0.5345,  0.2897, -0.1621, -0.0759,\n",
      "        -0.7448, -0.5207, -0.4793, -0.4931, -0.4448,  0.1828,  0.1828, -0.1724,\n",
      "        -0.2931,  0.5276,  1.0000,  0.4862, -0.2552, -0.4724, -0.2655, -0.4379,\n",
      "         0.4828, -0.5345,  0.0310, -0.2034,  0.2241, -0.5724, -0.1517,  0.6276,\n",
      "         0.9000,  0.2379,  0.0207, -0.0310, -0.1172,  0.0931,  0.7069,  0.3379,\n",
      "         0.2310,  0.7379,  0.5897,  0.1172,  0.3448,  0.0759, -0.0793, -0.3448,\n",
      "         0.1379, -0.5552,  0.7897,  0.4207,  0.0448, -0.4207, -0.4241,  0.3207,\n",
      "         0.1241,  0.1276,  0.1897, -0.0483,  0.5034,  0.2759,  0.1621, -0.1103,\n",
      "         0.5862,  0.0448, -0.1759, -0.1345,  0.9483,  0.3483,  0.4759, -0.1069,\n",
      "         0.4138], dtype=torch.float64)\n",
      "torch.Size([13, 177])\n",
      "torch.Size([177, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(x[1])\n",
    "print(x.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        # parameters\n",
    "        # TODO: parameters can be parameterized instead of declaring them here\n",
    "        self.inputSize = x.size()[1]\n",
    "        self.outputSize = y.size()[1]\n",
    "        self.hiddenSize = 20\n",
    "        \n",
    "        # weights\n",
    "        self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor\n",
    "        self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        #print(X.size())\n",
    "        #print(self.W1.size())\n",
    "        self.z = torch.matmul(X, self.W1) # 3 X 3 \".dot\" does not broadcast in PyTorch\n",
    "        self.z2 = self.sigmoid(self.z) # activation function\n",
    "        self.z3 = torch.matmul(self.z2, self.W2)\n",
    "        o = self.sigmoid(self.z3) # final activation function\n",
    "        return o\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        # derivative of sigmoid\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error\n",
    "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
    "        self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
    "        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        # forward + backward pass for training\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "        \n",
    "    def saveWeights(self, model):\n",
    "        # we will use the PyTorch internal storage functions\n",
    "        torch.save(model, \"NN\")\n",
    "        # you can reload model with all the weights and so forth with:\n",
    "        # torch.load(\"NN\")\n",
    "        \n",
    "    def predict(self, X):\n",
    "        print (\"Predicted data based on trained weights: \")\n",
    "        print (\"Input (scaled): \\n\" + str(X))\n",
    "        print (\"Output: \\n\" + str(self.forward(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Neural_Network()\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Loss: 0.36123135685920715\n",
      "#1 Loss: 0.34010592103004456\n",
      "#2 Loss: 0.3992437422275543\n",
      "#3 Loss: 0.39924368262290955\n",
      "#4 Loss: 0.39924365282058716\n",
      "#5 Loss: 0.3992435932159424\n",
      "#6 Loss: 0.39924356341362\n",
      "#7 Loss: 0.3992435038089752\n",
      "#8 Loss: 0.39924344420433044\n",
      "#9 Loss: 0.39924338459968567\n",
      "#10 Loss: 0.3992433547973633\n",
      "#11 Loss: 0.3992433249950409\n",
      "#12 Loss: 0.39924323558807373\n",
      "#13 Loss: 0.39924317598342896\n",
      "#14 Loss: 0.3992430865764618\n",
      "#15 Loss: 0.399243026971817\n",
      "#16 Loss: 0.39924296736717224\n",
      "#17 Loss: 0.39924290776252747\n",
      "#18 Loss: 0.3992428183555603\n",
      "#19 Loss: 0.3992427587509155\n",
      "#20 Loss: 0.39924269914627075\n",
      "#21 Loss: 0.3992426097393036\n",
      "#22 Loss: 0.3992425799369812\n",
      "#23 Loss: 0.39924249053001404\n",
      "#24 Loss: 0.3992424011230469\n",
      "#25 Loss: 0.3992423415184021\n",
      "#26 Loss: 0.39924222230911255\n",
      "#27 Loss: 0.3992421329021454\n",
      "#28 Loss: 0.3992420434951782\n",
      "#29 Loss: 0.39924192428588867\n",
      "#30 Loss: 0.3992418348789215\n",
      "#31 Loss: 0.39924174547195435\n",
      "#32 Loss: 0.3992415964603424\n",
      "#33 Loss: 0.39924144744873047\n",
      "#34 Loss: 0.39924129843711853\n",
      "#35 Loss: 0.399241179227829\n",
      "#36 Loss: 0.39924103021621704\n",
      "#37 Loss: 0.3992409110069275\n",
      "#38 Loss: 0.39924076199531555\n",
      "#39 Loss: 0.3992405831813812\n",
      "#40 Loss: 0.3992404043674469\n",
      "#41 Loss: 0.3992401957511902\n",
      "#42 Loss: 0.39924004673957825\n",
      "#43 Loss: 0.39923980832099915\n",
      "#44 Loss: 0.39923956990242004\n",
      "#45 Loss: 0.39923933148384094\n",
      "#46 Loss: 0.39923909306526184\n",
      "#47 Loss: 0.39923879504203796\n",
      "#48 Loss: 0.3992385268211365\n",
      "#49 Loss: 0.3992382287979126\n",
      "#50 Loss: 0.39923781156539917\n",
      "#51 Loss: 0.3992374539375305\n",
      "#52 Loss: 0.3992370069026947\n",
      "#53 Loss: 0.3992365896701813\n",
      "#54 Loss: 0.3992360532283783\n",
      "#55 Loss: 0.3992355167865753\n",
      "#56 Loss: 0.39923495054244995\n",
      "#57 Loss: 0.39923426508903503\n",
      "#58 Loss: 0.39923349022865295\n",
      "#59 Loss: 0.3992326855659485\n",
      "#60 Loss: 0.3992317020893097\n",
      "#61 Loss: 0.39923059940338135\n",
      "#62 Loss: 0.39922940731048584\n",
      "#63 Loss: 0.39922794699668884\n",
      "#64 Loss: 0.39922621846199036\n",
      "#65 Loss: 0.3992241621017456\n",
      "#66 Loss: 0.3992216885089874\n",
      "#67 Loss: 0.3992185592651367\n",
      "#68 Loss: 0.3992146849632263\n",
      "#69 Loss: 0.399209588766098\n",
      "#70 Loss: 0.3992026448249817\n",
      "#71 Loss: 0.3991927206516266\n",
      "#72 Loss: 0.3991776406764984\n",
      "#73 Loss: 0.39915239810943604\n",
      "#74 Loss: 0.3991032540798187\n",
      "#75 Loss: 0.39898213744163513\n",
      "#76 Loss: 0.3985229730606079\n",
      "#77 Loss: 0.39573657512664795\n",
      "#78 Loss: 0.3772796392440796\n",
      "#79 Loss: 0.3073316812515259\n",
      "#80 Loss: 0.30656954646110535\n",
      "#81 Loss: 0.3151775002479553\n",
      "#82 Loss: 0.3226271867752075\n",
      "#83 Loss: 0.30585813522338867\n",
      "#84 Loss: 0.26248863339424133\n",
      "#85 Loss: 0.22652667760849\n",
      "#86 Loss: 0.21321377158164978\n",
      "#87 Loss: 0.24614383280277252\n",
      "#88 Loss: 0.25125429034233093\n",
      "#89 Loss: 0.24295814335346222\n",
      "#90 Loss: 0.21081408858299255\n",
      "#91 Loss: 0.20704156160354614\n",
      "#92 Loss: 0.20404508709907532\n",
      "#93 Loss: 0.20701390504837036\n",
      "#94 Loss: 0.21017424762248993\n",
      "#95 Loss: 0.2153651863336563\n",
      "#96 Loss: 0.23357611894607544\n",
      "#97 Loss: 0.20567885041236877\n",
      "#98 Loss: 0.20241481065750122\n",
      "#99 Loss: 0.20518073439598083\n",
      "#100 Loss: 0.20266026258468628\n",
      "#101 Loss: 0.2026846706867218\n",
      "#102 Loss: 0.21376198530197144\n",
      "#103 Loss: 0.21028883755207062\n",
      "#104 Loss: 0.21190990507602692\n",
      "#105 Loss: 0.20680806040763855\n",
      "#106 Loss: 0.2046450674533844\n",
      "#107 Loss: 0.20404110848903656\n",
      "#108 Loss: 0.20194458961486816\n",
      "#109 Loss: 0.20333118736743927\n",
      "#110 Loss: 0.20801864564418793\n",
      "#111 Loss: 0.2047393023967743\n",
      "#112 Loss: 0.20722821354866028\n",
      "#113 Loss: 0.2055465281009674\n",
      "#114 Loss: 0.2056916505098343\n",
      "#115 Loss: 0.20170345902442932\n",
      "#116 Loss: 0.20063897967338562\n",
      "#117 Loss: 0.2018180787563324\n",
      "#118 Loss: 0.21013405919075012\n",
      "#119 Loss: 0.20769841969013214\n",
      "#120 Loss: 0.20310603082180023\n",
      "#121 Loss: 0.204359769821167\n",
      "#122 Loss: 0.20137432217597961\n",
      "#123 Loss: 0.20297116041183472\n",
      "#124 Loss: 0.20125019550323486\n",
      "#125 Loss: 0.20419718325138092\n",
      "#126 Loss: 0.20032024383544922\n",
      "#127 Loss: 0.20055177807807922\n",
      "#128 Loss: 0.20069469511508942\n",
      "#129 Loss: 0.20245622098445892\n",
      "#130 Loss: 0.19975139200687408\n",
      "#131 Loss: 0.19966131448745728\n",
      "#132 Loss: 0.19965822994709015\n",
      "#133 Loss: 0.19965659081935883\n",
      "#134 Loss: 0.19965533912181854\n",
      "#135 Loss: 0.19965428113937378\n",
      "#136 Loss: 0.19965340197086334\n",
      "#137 Loss: 0.1996525675058365\n",
      "#138 Loss: 0.19965185225009918\n",
      "#139 Loss: 0.19965119659900665\n",
      "#140 Loss: 0.1996506154537201\n",
      "#141 Loss: 0.19965006411075592\n",
      "#142 Loss: 0.19964958727359772\n",
      "#143 Loss: 0.19964909553527832\n",
      "#144 Loss: 0.1996486634016037\n",
      "#145 Loss: 0.19964823126792908\n",
      "#146 Loss: 0.19964787364006042\n",
      "#147 Loss: 0.19964750111103058\n",
      "#148 Loss: 0.19964715838432312\n",
      "#149 Loss: 0.19964683055877686\n",
      "#150 Loss: 0.19964651763439178\n",
      "#151 Loss: 0.1996462196111679\n",
      "#152 Loss: 0.19964595139026642\n",
      "#153 Loss: 0.19964565336704254\n",
      "#154 Loss: 0.19964540004730225\n",
      "#155 Loss: 0.19964514672756195\n",
      "#156 Loss: 0.19964490830898285\n",
      "#157 Loss: 0.19964466989040375\n",
      "#158 Loss: 0.19964444637298584\n",
      "#159 Loss: 0.19964422285556793\n",
      "#160 Loss: 0.1996440291404724\n",
      "#161 Loss: 0.1996438205242157\n",
      "#162 Loss: 0.19964361190795898\n",
      "#163 Loss: 0.19964343309402466\n",
      "#164 Loss: 0.19964325428009033\n",
      "#165 Loss: 0.199643075466156\n",
      "#166 Loss: 0.19964288175106049\n",
      "#167 Loss: 0.19964270293712616\n",
      "#168 Loss: 0.19964256882667542\n",
      "#169 Loss: 0.1996423751115799\n",
      "#170 Loss: 0.19964224100112915\n",
      "#171 Loss: 0.19964206218719482\n",
      "#172 Loss: 0.19964194297790527\n",
      "#173 Loss: 0.19964177906513214\n",
      "#174 Loss: 0.1996416449546814\n",
      "#175 Loss: 0.19964148104190826\n",
      "#176 Loss: 0.1996413618326187\n",
      "#177 Loss: 0.19964121282100677\n",
      "#178 Loss: 0.19964109361171722\n",
      "#179 Loss: 0.19964095950126648\n",
      "#180 Loss: 0.19964084029197693\n",
      "#181 Loss: 0.19964070618152618\n",
      "#182 Loss: 0.19964060187339783\n",
      "#183 Loss: 0.19964048266410828\n",
      "#184 Loss: 0.19964036345481873\n",
      "#185 Loss: 0.19964025914669037\n",
      "#186 Loss: 0.19964013993740082\n",
      "#187 Loss: 0.19964003562927246\n",
      "#188 Loss: 0.1996399164199829\n",
      "#189 Loss: 0.19963979721069336\n",
      "#190 Loss: 0.1996397078037262\n",
      "#191 Loss: 0.19963961839675903\n",
      "#192 Loss: 0.19963951408863068\n",
      "#193 Loss: 0.19963939487934113\n",
      "#194 Loss: 0.19963932037353516\n",
      "#195 Loss: 0.1996392160654068\n",
      "#196 Loss: 0.19963911175727844\n",
      "#197 Loss: 0.19963903725147247\n",
      "#198 Loss: 0.19963893294334412\n",
      "#199 Loss: 0.19963885843753815\n",
      "#200 Loss: 0.19963876903057098\n",
      "#201 Loss: 0.19963867962360382\n",
      "#202 Loss: 0.19963857531547546\n",
      "#203 Loss: 0.19963853061199188\n",
      "#204 Loss: 0.19963841140270233\n",
      "#205 Loss: 0.19963833689689636\n",
      "#206 Loss: 0.1996382623910904\n",
      "#207 Loss: 0.19963817298412323\n",
      "#208 Loss: 0.19963812828063965\n",
      "#209 Loss: 0.19963803887367249\n",
      "#210 Loss: 0.19963794946670532\n",
      "#211 Loss: 0.19963788986206055\n",
      "#212 Loss: 0.19963780045509338\n",
      "#213 Loss: 0.1996377557516098\n",
      "#214 Loss: 0.19963766634464264\n",
      "#215 Loss: 0.19963760673999786\n",
      "#216 Loss: 0.1996375322341919\n",
      "#217 Loss: 0.19963744282722473\n",
      "#218 Loss: 0.19963736832141876\n",
      "#219 Loss: 0.19963733851909637\n",
      "#220 Loss: 0.1996372789144516\n",
      "#221 Loss: 0.19963718950748444\n",
      "#222 Loss: 0.19963712990283966\n",
      "#223 Loss: 0.19963707029819489\n",
      "#224 Loss: 0.19963698089122772\n",
      "#225 Loss: 0.19963693618774414\n",
      "#226 Loss: 0.19963687658309937\n",
      "#227 Loss: 0.1996368169784546\n",
      "#228 Loss: 0.199636772274971\n",
      "#229 Loss: 0.19963669776916504\n",
      "#230 Loss: 0.19963662326335907\n",
      "#231 Loss: 0.1996365636587143\n",
      "#232 Loss: 0.1996365487575531\n",
      "#233 Loss: 0.19963645935058594\n",
      "#234 Loss: 0.19963639974594116\n",
      "#235 Loss: 0.19963635504245758\n",
      "#236 Loss: 0.1996362954378128\n",
      "#237 Loss: 0.19963625073432922\n",
      "#238 Loss: 0.19963619112968445\n",
      "#239 Loss: 0.19963614642620087\n",
      "#240 Loss: 0.19963611662387848\n",
      "#241 Loss: 0.1996360421180725\n",
      "#242 Loss: 0.19963599741458893\n",
      "#243 Loss: 0.19963593780994415\n",
      "#244 Loss: 0.19963587820529938\n",
      "#245 Loss: 0.199635848402977\n",
      "#246 Loss: 0.19963578879833221\n",
      "#247 Loss: 0.19963575899600983\n",
      "#248 Loss: 0.19963569939136505\n",
      "#249 Loss: 0.19963565468788147\n",
      "#250 Loss: 0.1996356099843979\n",
      "#251 Loss: 0.1996355652809143\n",
      "#252 Loss: 0.19963552057743073\n",
      "#253 Loss: 0.19963546097278595\n",
      "#254 Loss: 0.19963540136814117\n",
      "#255 Loss: 0.1996353715658188\n",
      "#256 Loss: 0.1996353417634964\n",
      "#257 Loss: 0.19963528215885162\n",
      "#258 Loss: 0.19963523745536804\n",
      "#259 Loss: 0.19963520765304565\n",
      "#260 Loss: 0.19963514804840088\n",
      "#261 Loss: 0.1996350884437561\n",
      "#262 Loss: 0.19963504374027252\n",
      "#263 Loss: 0.19963501393795013\n",
      "#264 Loss: 0.19963496923446655\n",
      "#265 Loss: 0.19963493943214417\n",
      "#266 Loss: 0.19963489472866058\n",
      "#267 Loss: 0.1996348351240158\n",
      "#268 Loss: 0.19963482022285461\n",
      "#269 Loss: 0.19963476061820984\n",
      "#270 Loss: 0.19963473081588745\n",
      "#271 Loss: 0.19963470101356506\n",
      "#272 Loss: 0.1996346414089203\n",
      "#273 Loss: 0.1996346265077591\n",
      "#274 Loss: 0.19963456690311432\n",
      "#275 Loss: 0.19963453710079193\n",
      "#276 Loss: 0.19963452219963074\n",
      "#277 Loss: 0.19963447749614716\n",
      "#278 Loss: 0.19963443279266357\n",
      "#279 Loss: 0.1996344029903412\n",
      "#280 Loss: 0.1996343582868576\n",
      "#281 Loss: 0.19963432848453522\n",
      "#282 Loss: 0.19963428378105164\n",
      "#283 Loss: 0.19963425397872925\n",
      "#284 Loss: 0.19963422417640686\n",
      "#285 Loss: 0.19963419437408447\n",
      "#286 Loss: 0.1996341496706009\n",
      "#287 Loss: 0.1996341347694397\n",
      "#288 Loss: 0.19963409006595612\n",
      "#289 Loss: 0.19963406026363373\n",
      "#290 Loss: 0.19963401556015015\n",
      "#291 Loss: 0.19963400065898895\n",
      "#292 Loss: 0.19963395595550537\n",
      "#293 Loss: 0.19963394105434418\n",
      "#294 Loss: 0.1996338814496994\n",
      "#295 Loss: 0.19963385164737701\n",
      "#296 Loss: 0.19963382184505463\n",
      "#297 Loss: 0.19963379204273224\n",
      "#298 Loss: 0.19963374733924866\n",
      "#299 Loss: 0.19963374733924866\n",
      "#300 Loss: 0.19963371753692627\n",
      "#301 Loss: 0.19963368773460388\n",
      "#302 Loss: 0.1996336281299591\n",
      "#303 Loss: 0.1996336132287979\n",
      "#304 Loss: 0.19963356852531433\n",
      "#305 Loss: 0.19963353872299194\n",
      "#306 Loss: 0.19963352382183075\n",
      "#307 Loss: 0.19963349401950836\n",
      "#308 Loss: 0.19963347911834717\n",
      "#309 Loss: 0.1996334344148636\n",
      "#310 Loss: 0.1996334195137024\n",
      "#311 Loss: 0.19963335990905762\n",
      "#312 Loss: 0.19963335990905762\n",
      "#313 Loss: 0.19963331520557404\n",
      "#314 Loss: 0.19963330030441284\n",
      "#315 Loss: 0.19963327050209045\n",
      "#316 Loss: 0.19963325560092926\n",
      "#317 Loss: 0.19963322579860687\n",
      "#318 Loss: 0.19963319599628448\n",
      "#319 Loss: 0.1996331661939621\n",
      "#320 Loss: 0.1996331363916397\n",
      "#321 Loss: 0.19963309168815613\n",
      "#322 Loss: 0.19963307678699493\n",
      "#323 Loss: 0.19963304698467255\n",
      "#324 Loss: 0.19963303208351135\n",
      "#325 Loss: 0.19963300228118896\n",
      "#326 Loss: 0.19963297247886658\n",
      "#327 Loss: 0.19963295757770538\n",
      "#328 Loss: 0.199632927775383\n",
      "#329 Loss: 0.1996329128742218\n",
      "#330 Loss: 0.19963288307189941\n",
      "#331 Loss: 0.19963285326957703\n",
      "#332 Loss: 0.19963283836841583\n",
      "#333 Loss: 0.19963280856609344\n",
      "#334 Loss: 0.19963277876377106\n",
      "#335 Loss: 0.19963274896144867\n",
      "#336 Loss: 0.19963273406028748\n",
      "#337 Loss: 0.1996327042579651\n",
      "#338 Loss: 0.1996326893568039\n",
      "#339 Loss: 0.1996326595544815\n",
      "#340 Loss: 0.19963262975215912\n",
      "#341 Loss: 0.19963261485099792\n",
      "#342 Loss: 0.19963258504867554\n",
      "#343 Loss: 0.19963255524635315\n",
      "#344 Loss: 0.19963254034519196\n",
      "#345 Loss: 0.19963252544403076\n",
      "#346 Loss: 0.19963251054286957\n",
      "#347 Loss: 0.19963249564170837\n",
      "#348 Loss: 0.1996324509382248\n",
      "#349 Loss: 0.1996324211359024\n",
      "#350 Loss: 0.1996324062347412\n",
      "#351 Loss: 0.19963239133358002\n",
      "#352 Loss: 0.19963237643241882\n",
      "#353 Loss: 0.19963234663009644\n",
      "#354 Loss: 0.19963234663009644\n",
      "#355 Loss: 0.19963228702545166\n",
      "#356 Loss: 0.19963227212429047\n",
      "#357 Loss: 0.19963225722312927\n",
      "#358 Loss: 0.19963222742080688\n",
      "#359 Loss: 0.1996322125196457\n",
      "#360 Loss: 0.1996321827173233\n",
      "#361 Loss: 0.19963215291500092\n",
      "#362 Loss: 0.19963215291500092\n",
      "#363 Loss: 0.19963213801383972\n",
      "#364 Loss: 0.19963213801383972\n",
      "#365 Loss: 0.19963210821151733\n",
      "#366 Loss: 0.19963207840919495\n",
      "#367 Loss: 0.19963206350803375\n",
      "#368 Loss: 0.19963203370571136\n",
      "#369 Loss: 0.19963200390338898\n",
      "#370 Loss: 0.19963198900222778\n",
      "#371 Loss: 0.1996319741010666\n",
      "#372 Loss: 0.199631929397583\n",
      "#373 Loss: 0.199631929397583\n",
      "#374 Loss: 0.19963191449642181\n",
      "#375 Loss: 0.19963191449642181\n",
      "#376 Loss: 0.19963186979293823\n",
      "#377 Loss: 0.19963185489177704\n",
      "#378 Loss: 0.19963185489177704\n",
      "#379 Loss: 0.19963182508945465\n",
      "#380 Loss: 0.19963179528713226\n",
      "#381 Loss: 0.19963178038597107\n",
      "#382 Loss: 0.19963176548480988\n",
      "#383 Loss: 0.19963175058364868\n",
      "#384 Loss: 0.1996317356824875\n",
      "#385 Loss: 0.1996317207813263\n",
      "#386 Loss: 0.1996316909790039\n",
      "#387 Loss: 0.19963166117668152\n",
      "#388 Loss: 0.19963166117668152\n",
      "#389 Loss: 0.19963164627552032\n",
      "#390 Loss: 0.19963163137435913\n",
      "#391 Loss: 0.19963160157203674\n",
      "#392 Loss: 0.19963158667087555\n",
      "#393 Loss: 0.19963157176971436\n",
      "#394 Loss: 0.19963155686855316\n",
      "#395 Loss: 0.19963152706623077\n",
      "#396 Loss: 0.19963151216506958\n",
      "#397 Loss: 0.19963151216506958\n",
      "#398 Loss: 0.1996314823627472\n",
      "#399 Loss: 0.1996314823627472\n",
      "#400 Loss: 0.19963142275810242\n",
      "#401 Loss: 0.19963142275810242\n",
      "#402 Loss: 0.19963139295578003\n",
      "#403 Loss: 0.19963137805461884\n",
      "#404 Loss: 0.19963136315345764\n",
      "#405 Loss: 0.19963134825229645\n",
      "#406 Loss: 0.19963133335113525\n",
      "#407 Loss: 0.19963133335113525\n",
      "#408 Loss: 0.19963130354881287\n",
      "#409 Loss: 0.19963128864765167\n",
      "#410 Loss: 0.19963127374649048\n",
      "#411 Loss: 0.19963127374649048\n",
      "#412 Loss: 0.19963125884532928\n",
      "#413 Loss: 0.1996312439441681\n",
      "#414 Loss: 0.1996312141418457\n",
      "#415 Loss: 0.1996312141418457\n",
      "#416 Loss: 0.1996311992406845\n",
      "#417 Loss: 0.19963116943836212\n",
      "#418 Loss: 0.19963113963603973\n",
      "#419 Loss: 0.19963110983371735\n",
      "#420 Loss: 0.19963110983371735\n",
      "#421 Loss: 0.19963110983371735\n",
      "#422 Loss: 0.19963109493255615\n",
      "#423 Loss: 0.19963105022907257\n",
      "#424 Loss: 0.19963105022907257\n",
      "#425 Loss: 0.19963103532791138\n",
      "#426 Loss: 0.199631005525589\n",
      "#427 Loss: 0.199631005525589\n",
      "#428 Loss: 0.1996309608221054\n",
      "#429 Loss: 0.1996309608221054\n",
      "#430 Loss: 0.1996309459209442\n",
      "#431 Loss: 0.19963093101978302\n",
      "#432 Loss: 0.19963093101978302\n",
      "#433 Loss: 0.19963091611862183\n",
      "#434 Loss: 0.19963090121746063\n",
      "#435 Loss: 0.19963088631629944\n",
      "#436 Loss: 0.19963087141513824\n",
      "#437 Loss: 0.19963085651397705\n",
      "#438 Loss: 0.19963084161281586\n",
      "#439 Loss: 0.19963082671165466\n",
      "#440 Loss: 0.19963081181049347\n",
      "#441 Loss: 0.19963079690933228\n",
      "#442 Loss: 0.19963079690933228\n",
      "#443 Loss: 0.19963078200817108\n",
      "#444 Loss: 0.1996307373046875\n",
      "#445 Loss: 0.1996307224035263\n",
      "#446 Loss: 0.1996307224035263\n",
      "#447 Loss: 0.1996307224035263\n",
      "#448 Loss: 0.1996307075023651\n",
      "#449 Loss: 0.19963069260120392\n",
      "#450 Loss: 0.19963067770004272\n",
      "#451 Loss: 0.19963066279888153\n",
      "#452 Loss: 0.19963064789772034\n",
      "#453 Loss: 0.19963064789772034\n",
      "#454 Loss: 0.19963061809539795\n",
      "#455 Loss: 0.19963061809539795\n",
      "#456 Loss: 0.19963058829307556\n",
      "#457 Loss: 0.19963058829307556\n",
      "#458 Loss: 0.19963055849075317\n",
      "#459 Loss: 0.19963054358959198\n",
      "#460 Loss: 0.19963054358959198\n",
      "#461 Loss: 0.19963054358959198\n",
      "#462 Loss: 0.1996305137872696\n",
      "#463 Loss: 0.1996305137872696\n",
      "#464 Loss: 0.1996304839849472\n",
      "#465 Loss: 0.1996304839849472\n",
      "#466 Loss: 0.1996304839849472\n",
      "#467 Loss: 0.199630469083786\n",
      "#468 Loss: 0.19963043928146362\n",
      "#469 Loss: 0.19963043928146362\n",
      "#470 Loss: 0.19963042438030243\n",
      "#471 Loss: 0.19963040947914124\n",
      "#472 Loss: 0.19963040947914124\n",
      "#473 Loss: 0.19963040947914124\n",
      "#474 Loss: 0.19963037967681885\n",
      "#475 Loss: 0.19963036477565765\n",
      "#476 Loss: 0.19963033497333527\n",
      "#477 Loss: 0.19963033497333527\n",
      "#478 Loss: 0.19963033497333527\n",
      "#479 Loss: 0.19963032007217407\n",
      "#480 Loss: 0.19963032007217407\n",
      "#481 Loss: 0.19963029026985168\n",
      "#482 Loss: 0.1996302753686905\n",
      "#483 Loss: 0.1996302753686905\n",
      "#484 Loss: 0.1996302455663681\n",
      "#485 Loss: 0.1996302455663681\n",
      "#486 Loss: 0.1996302306652069\n",
      "#487 Loss: 0.19963021576404572\n",
      "#488 Loss: 0.19963018596172333\n",
      "#489 Loss: 0.19963017106056213\n",
      "#490 Loss: 0.19963017106056213\n",
      "#491 Loss: 0.19963017106056213\n",
      "#492 Loss: 0.19963015615940094\n",
      "#493 Loss: 0.19963015615940094\n",
      "#494 Loss: 0.19963014125823975\n",
      "#495 Loss: 0.19963012635707855\n",
      "#496 Loss: 0.19963012635707855\n",
      "#497 Loss: 0.19963011145591736\n",
      "#498 Loss: 0.19963011145591736\n",
      "#499 Loss: 0.19963009655475616\n",
      "#500 Loss: 0.19963008165359497\n",
      "#501 Loss: 0.19963006675243378\n",
      "#502 Loss: 0.19963005185127258\n",
      "#503 Loss: 0.19963005185127258\n",
      "#504 Loss: 0.1996300369501114\n",
      "#505 Loss: 0.1996300369501114\n",
      "#506 Loss: 0.1996300220489502\n",
      "#507 Loss: 0.1996300220489502\n",
      "#508 Loss: 0.1996299922466278\n",
      "#509 Loss: 0.1996299922466278\n",
      "#510 Loss: 0.19962996244430542\n",
      "#511 Loss: 0.19962994754314423\n",
      "#512 Loss: 0.19962994754314423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#513 Loss: 0.19962994754314423\n",
      "#514 Loss: 0.19962991774082184\n",
      "#515 Loss: 0.19962990283966064\n",
      "#516 Loss: 0.19962990283966064\n",
      "#517 Loss: 0.19962988793849945\n",
      "#518 Loss: 0.19962987303733826\n",
      "#519 Loss: 0.19962987303733826\n",
      "#520 Loss: 0.19962985813617706\n",
      "#521 Loss: 0.19962985813617706\n",
      "#522 Loss: 0.19962984323501587\n",
      "#523 Loss: 0.19962984323501587\n",
      "#524 Loss: 0.19962984323501587\n",
      "#525 Loss: 0.19962981343269348\n",
      "#526 Loss: 0.1996297985315323\n",
      "#527 Loss: 0.1996297985315323\n",
      "#528 Loss: 0.1996297836303711\n",
      "#529 Loss: 0.1996297687292099\n",
      "#530 Loss: 0.1996297538280487\n",
      "#531 Loss: 0.1996297538280487\n",
      "#532 Loss: 0.1996297538280487\n",
      "#533 Loss: 0.1996297389268875\n",
      "#534 Loss: 0.19962972402572632\n",
      "#535 Loss: 0.19962972402572632\n",
      "#536 Loss: 0.19962972402572632\n",
      "#537 Loss: 0.19962970912456512\n",
      "#538 Loss: 0.19962969422340393\n",
      "#539 Loss: 0.19962969422340393\n",
      "#540 Loss: 0.19962967932224274\n",
      "#541 Loss: 0.19962966442108154\n",
      "#542 Loss: 0.19962964951992035\n",
      "#543 Loss: 0.19962961971759796\n",
      "#544 Loss: 0.19962961971759796\n",
      "#545 Loss: 0.19962961971759796\n",
      "#546 Loss: 0.19962961971759796\n",
      "#547 Loss: 0.19962960481643677\n",
      "#548 Loss: 0.19962958991527557\n",
      "#549 Loss: 0.19962958991527557\n",
      "#550 Loss: 0.19962958991527557\n",
      "#551 Loss: 0.19962956011295319\n",
      "#552 Loss: 0.19962956011295319\n",
      "#553 Loss: 0.199629545211792\n",
      "#554 Loss: 0.199629545211792\n",
      "#555 Loss: 0.1996295303106308\n",
      "#556 Loss: 0.1996295303106308\n",
      "#557 Loss: 0.1996295154094696\n",
      "#558 Loss: 0.1996295154094696\n",
      "#559 Loss: 0.1996295005083084\n",
      "#560 Loss: 0.19962947070598602\n",
      "#561 Loss: 0.19962947070598602\n",
      "#562 Loss: 0.19962945580482483\n",
      "#563 Loss: 0.19962945580482483\n",
      "#564 Loss: 0.19962945580482483\n",
      "#565 Loss: 0.19962944090366364\n",
      "#566 Loss: 0.19962944090366364\n",
      "#567 Loss: 0.19962944090366364\n",
      "#568 Loss: 0.19962942600250244\n",
      "#569 Loss: 0.19962941110134125\n",
      "#570 Loss: 0.19962939620018005\n",
      "#571 Loss: 0.19962939620018005\n",
      "#572 Loss: 0.19962939620018005\n",
      "#573 Loss: 0.19962938129901886\n",
      "#574 Loss: 0.19962935149669647\n",
      "#575 Loss: 0.19962935149669647\n",
      "#576 Loss: 0.19962935149669647\n",
      "#577 Loss: 0.19962933659553528\n",
      "#578 Loss: 0.19962932169437408\n",
      "#579 Loss: 0.19962932169437408\n",
      "#580 Loss: 0.1996293067932129\n",
      "#581 Loss: 0.1996293067932129\n",
      "#582 Loss: 0.1996292918920517\n",
      "#583 Loss: 0.1996292769908905\n",
      "#584 Loss: 0.1996292769908905\n",
      "#585 Loss: 0.1996292769908905\n",
      "#586 Loss: 0.1996292769908905\n",
      "#587 Loss: 0.1996292769908905\n",
      "#588 Loss: 0.1996292620897293\n",
      "#589 Loss: 0.19962924718856812\n",
      "#590 Loss: 0.19962921738624573\n",
      "#591 Loss: 0.19962921738624573\n",
      "#592 Loss: 0.19962921738624573\n",
      "#593 Loss: 0.19962921738624573\n",
      "#594 Loss: 0.19962918758392334\n",
      "#595 Loss: 0.19962918758392334\n",
      "#596 Loss: 0.19962918758392334\n",
      "#597 Loss: 0.19962915778160095\n",
      "#598 Loss: 0.19962915778160095\n",
      "#599 Loss: 0.19962915778160095\n",
      "#600 Loss: 0.19962914288043976\n",
      "#601 Loss: 0.19962912797927856\n",
      "#602 Loss: 0.19962911307811737\n",
      "#603 Loss: 0.19962911307811737\n",
      "#604 Loss: 0.19962911307811737\n",
      "#605 Loss: 0.19962909817695618\n",
      "#606 Loss: 0.19962909817695618\n",
      "#607 Loss: 0.19962909817695618\n",
      "#608 Loss: 0.19962908327579498\n",
      "#609 Loss: 0.19962908327579498\n",
      "#610 Loss: 0.19962908327579498\n",
      "#611 Loss: 0.1996290683746338\n",
      "#612 Loss: 0.1996290534734726\n",
      "#613 Loss: 0.1996290534734726\n",
      "#614 Loss: 0.1996290236711502\n",
      "#615 Loss: 0.1996290236711502\n",
      "#616 Loss: 0.199629008769989\n",
      "#617 Loss: 0.19962899386882782\n",
      "#618 Loss: 0.19962899386882782\n",
      "#619 Loss: 0.19962897896766663\n",
      "#620 Loss: 0.19962897896766663\n",
      "#621 Loss: 0.19962897896766663\n",
      "#622 Loss: 0.19962897896766663\n",
      "#623 Loss: 0.19962896406650543\n",
      "#624 Loss: 0.19962894916534424\n",
      "#625 Loss: 0.19962893426418304\n",
      "#626 Loss: 0.19962893426418304\n",
      "#627 Loss: 0.19962893426418304\n",
      "#628 Loss: 0.19962893426418304\n",
      "#629 Loss: 0.19962893426418304\n",
      "#630 Loss: 0.19962891936302185\n",
      "#631 Loss: 0.19962891936302185\n",
      "#632 Loss: 0.19962891936302185\n",
      "#633 Loss: 0.19962891936302185\n",
      "#634 Loss: 0.19962890446186066\n",
      "#635 Loss: 0.19962890446186066\n",
      "#636 Loss: 0.19962887465953827\n",
      "#637 Loss: 0.19962885975837708\n",
      "#638 Loss: 0.19962885975837708\n",
      "#639 Loss: 0.19962884485721588\n",
      "#640 Loss: 0.19962884485721588\n",
      "#641 Loss: 0.19962884485721588\n",
      "#642 Loss: 0.1996288299560547\n",
      "#643 Loss: 0.1996288299560547\n",
      "#644 Loss: 0.1996288299560547\n",
      "#645 Loss: 0.1996288299560547\n",
      "#646 Loss: 0.1996288299560547\n",
      "#647 Loss: 0.1996288150548935\n",
      "#648 Loss: 0.1996288001537323\n",
      "#649 Loss: 0.1996287852525711\n",
      "#650 Loss: 0.1996287703514099\n",
      "#651 Loss: 0.1996287703514099\n",
      "#652 Loss: 0.1996287703514099\n",
      "#653 Loss: 0.1996287703514099\n",
      "#654 Loss: 0.1996287703514099\n",
      "#655 Loss: 0.19962875545024872\n",
      "#656 Loss: 0.19962875545024872\n",
      "#657 Loss: 0.19962874054908752\n",
      "#658 Loss: 0.19962872564792633\n",
      "#659 Loss: 0.19962872564792633\n",
      "#660 Loss: 0.19962871074676514\n",
      "#661 Loss: 0.19962869584560394\n",
      "#662 Loss: 0.19962868094444275\n",
      "#663 Loss: 0.19962868094444275\n",
      "#664 Loss: 0.19962868094444275\n",
      "#665 Loss: 0.19962866604328156\n",
      "#666 Loss: 0.19962866604328156\n",
      "#667 Loss: 0.19962865114212036\n",
      "#668 Loss: 0.19962865114212036\n",
      "#669 Loss: 0.19962865114212036\n",
      "#670 Loss: 0.19962865114212036\n",
      "#671 Loss: 0.19962863624095917\n",
      "#672 Loss: 0.19962863624095917\n",
      "#673 Loss: 0.19962863624095917\n",
      "#674 Loss: 0.19962862133979797\n",
      "#675 Loss: 0.19962862133979797\n",
      "#676 Loss: 0.19962860643863678\n",
      "#677 Loss: 0.19962859153747559\n",
      "#678 Loss: 0.19962859153747559\n",
      "#679 Loss: 0.19962859153747559\n",
      "#680 Loss: 0.19962859153747559\n",
      "#681 Loss: 0.1996285766363144\n",
      "#682 Loss: 0.1996285766363144\n",
      "#683 Loss: 0.1996285766363144\n",
      "#684 Loss: 0.1996285617351532\n",
      "#685 Loss: 0.199628546833992\n",
      "#686 Loss: 0.199628546833992\n",
      "#687 Loss: 0.1996285319328308\n",
      "#688 Loss: 0.1996285319328308\n",
      "#689 Loss: 0.1996285319328308\n",
      "#690 Loss: 0.1996285319328308\n",
      "#691 Loss: 0.19962851703166962\n",
      "#692 Loss: 0.19962851703166962\n",
      "#693 Loss: 0.19962851703166962\n",
      "#694 Loss: 0.19962850213050842\n",
      "#695 Loss: 0.19962850213050842\n",
      "#696 Loss: 0.19962850213050842\n",
      "#697 Loss: 0.19962850213050842\n",
      "#698 Loss: 0.19962848722934723\n",
      "#699 Loss: 0.19962848722934723\n",
      "#700 Loss: 0.19962848722934723\n",
      "#701 Loss: 0.19962848722934723\n",
      "#702 Loss: 0.19962847232818604\n",
      "#703 Loss: 0.19962847232818604\n",
      "#704 Loss: 0.19962845742702484\n",
      "#705 Loss: 0.19962844252586365\n",
      "#706 Loss: 0.19962842762470245\n",
      "#707 Loss: 0.19962842762470245\n",
      "#708 Loss: 0.19962842762470245\n",
      "#709 Loss: 0.19962841272354126\n",
      "#710 Loss: 0.19962841272354126\n",
      "#711 Loss: 0.19962836802005768\n",
      "#712 Loss: 0.19962836802005768\n",
      "#713 Loss: 0.19962836802005768\n",
      "#714 Loss: 0.19962836802005768\n",
      "#715 Loss: 0.19962836802005768\n",
      "#716 Loss: 0.19962836802005768\n",
      "#717 Loss: 0.19962835311889648\n",
      "#718 Loss: 0.1996283382177353\n",
      "#719 Loss: 0.1996283382177353\n",
      "#720 Loss: 0.1996283382177353\n",
      "#721 Loss: 0.1996283233165741\n",
      "#722 Loss: 0.1996283233165741\n",
      "#723 Loss: 0.1996283084154129\n",
      "#724 Loss: 0.1996283084154129\n",
      "#725 Loss: 0.1996283084154129\n",
      "#726 Loss: 0.1996283084154129\n",
      "#727 Loss: 0.1996282935142517\n",
      "#728 Loss: 0.1996282935142517\n",
      "#729 Loss: 0.1996282935142517\n",
      "#730 Loss: 0.19962827861309052\n",
      "#731 Loss: 0.19962827861309052\n",
      "#732 Loss: 0.19962827861309052\n",
      "#733 Loss: 0.19962827861309052\n",
      "#734 Loss: 0.19962827861309052\n",
      "#735 Loss: 0.19962826371192932\n",
      "#736 Loss: 0.19962826371192932\n",
      "#737 Loss: 0.19962824881076813\n",
      "#738 Loss: 0.19962823390960693\n",
      "#739 Loss: 0.19962823390960693\n",
      "#740 Loss: 0.19962823390960693\n",
      "#741 Loss: 0.19962821900844574\n",
      "#742 Loss: 0.19962820410728455\n",
      "#743 Loss: 0.19962818920612335\n",
      "#744 Loss: 0.19962818920612335\n",
      "#745 Loss: 0.19962818920612335\n",
      "#746 Loss: 0.19962818920612335\n",
      "#747 Loss: 0.19962818920612335\n",
      "#748 Loss: 0.19962818920612335\n",
      "#749 Loss: 0.19962817430496216\n",
      "#750 Loss: 0.19962817430496216\n",
      "#751 Loss: 0.19962817430496216\n",
      "#752 Loss: 0.19962815940380096\n",
      "#753 Loss: 0.19962815940380096\n",
      "#754 Loss: 0.19962815940380096\n",
      "#755 Loss: 0.19962815940380096\n",
      "#756 Loss: 0.19962812960147858\n",
      "#757 Loss: 0.19962812960147858\n",
      "#758 Loss: 0.19962812960147858\n",
      "#759 Loss: 0.19962812960147858\n",
      "#760 Loss: 0.19962812960147858\n",
      "#761 Loss: 0.19962812960147858\n",
      "#762 Loss: 0.19962812960147858\n",
      "#763 Loss: 0.19962812960147858\n",
      "#764 Loss: 0.19962811470031738\n",
      "#765 Loss: 0.19962811470031738\n",
      "#766 Loss: 0.19962811470031738\n",
      "#767 Loss: 0.19962811470031738\n",
      "#768 Loss: 0.19962811470031738\n",
      "#769 Loss: 0.19962811470031738\n",
      "#770 Loss: 0.199628084897995\n",
      "#771 Loss: 0.199628084897995\n",
      "#772 Loss: 0.1996280550956726\n",
      "#773 Loss: 0.1996280550956726\n",
      "#774 Loss: 0.1996280401945114\n",
      "#775 Loss: 0.1996280401945114\n",
      "#776 Loss: 0.1996280401945114\n",
      "#777 Loss: 0.1996280401945114\n",
      "#778 Loss: 0.19962802529335022\n",
      "#779 Loss: 0.19962802529335022\n",
      "#780 Loss: 0.19962801039218903\n",
      "#781 Loss: 0.19962801039218903\n",
      "#782 Loss: 0.19962801039218903\n",
      "#783 Loss: 0.19962801039218903\n",
      "#784 Loss: 0.19962801039218903\n",
      "#785 Loss: 0.19962801039218903\n",
      "#786 Loss: 0.19962799549102783\n",
      "#787 Loss: 0.19962798058986664\n",
      "#788 Loss: 0.19962798058986664\n",
      "#789 Loss: 0.19962798058986664\n",
      "#790 Loss: 0.19962798058986664\n",
      "#791 Loss: 0.19962798058986664\n",
      "#792 Loss: 0.19962798058986664\n",
      "#793 Loss: 0.19962796568870544\n",
      "#794 Loss: 0.19962795078754425\n",
      "#795 Loss: 0.19962795078754425\n",
      "#796 Loss: 0.19962795078754425\n",
      "#797 Loss: 0.19962793588638306\n",
      "#798 Loss: 0.19962793588638306\n",
      "#799 Loss: 0.19962793588638306\n",
      "#800 Loss: 0.19962793588638306\n",
      "#801 Loss: 0.19962792098522186\n",
      "#802 Loss: 0.19962792098522186\n",
      "#803 Loss: 0.19962792098522186\n",
      "#804 Loss: 0.19962792098522186\n",
      "#805 Loss: 0.19962792098522186\n",
      "#806 Loss: 0.19962790608406067\n",
      "#807 Loss: 0.19962790608406067\n",
      "#808 Loss: 0.19962789118289948\n",
      "#809 Loss: 0.19962789118289948\n",
      "#810 Loss: 0.19962787628173828\n",
      "#811 Loss: 0.19962787628173828\n",
      "#812 Loss: 0.19962787628173828\n",
      "#813 Loss: 0.1996278613805771\n",
      "#814 Loss: 0.1996278613805771\n",
      "#815 Loss: 0.1996278613805771\n",
      "#816 Loss: 0.1996278613805771\n",
      "#817 Loss: 0.1996278315782547\n",
      "#818 Loss: 0.1996278315782547\n",
      "#819 Loss: 0.1996278315782547\n",
      "#820 Loss: 0.1996278315782547\n",
      "#821 Loss: 0.1996278315782547\n",
      "#822 Loss: 0.1996278315782547\n",
      "#823 Loss: 0.1996278315782547\n",
      "#824 Loss: 0.1996278315782547\n",
      "#825 Loss: 0.1996278166770935\n",
      "#826 Loss: 0.1996278166770935\n",
      "#827 Loss: 0.1996278017759323\n",
      "#828 Loss: 0.1996278017759323\n",
      "#829 Loss: 0.1996278017759323\n",
      "#830 Loss: 0.1996278017759323\n",
      "#831 Loss: 0.1996278017759323\n",
      "#832 Loss: 0.1996278017759323\n",
      "#833 Loss: 0.19962778687477112\n",
      "#834 Loss: 0.19962778687477112\n",
      "#835 Loss: 0.19962777197360992\n",
      "#836 Loss: 0.19962777197360992\n",
      "#837 Loss: 0.19962775707244873\n",
      "#838 Loss: 0.19962775707244873\n",
      "#839 Loss: 0.19962774217128754\n",
      "#840 Loss: 0.19962774217128754\n",
      "#841 Loss: 0.19962774217128754\n",
      "#842 Loss: 0.19962772727012634\n",
      "#843 Loss: 0.19962772727012634\n",
      "#844 Loss: 0.19962772727012634\n",
      "#845 Loss: 0.19962772727012634\n",
      "#846 Loss: 0.19962772727012634\n",
      "#847 Loss: 0.19962772727012634\n",
      "#848 Loss: 0.19962769746780396\n",
      "#849 Loss: 0.19962769746780396\n",
      "#850 Loss: 0.19962769746780396\n",
      "#851 Loss: 0.19962769746780396\n",
      "#852 Loss: 0.19962768256664276\n",
      "#853 Loss: 0.19962768256664276\n",
      "#854 Loss: 0.19962768256664276\n",
      "#855 Loss: 0.19962768256664276\n",
      "#856 Loss: 0.19962768256664276\n",
      "#857 Loss: 0.19962768256664276\n",
      "#858 Loss: 0.19962765276432037\n",
      "#859 Loss: 0.19962765276432037\n",
      "#860 Loss: 0.19962763786315918\n",
      "#861 Loss: 0.19962763786315918\n",
      "#862 Loss: 0.19962763786315918\n",
      "#863 Loss: 0.19962763786315918\n",
      "#864 Loss: 0.19962763786315918\n",
      "#865 Loss: 0.19962763786315918\n",
      "#866 Loss: 0.19962763786315918\n",
      "#867 Loss: 0.19962762296199799\n",
      "#868 Loss: 0.19962762296199799\n",
      "#869 Loss: 0.1996276080608368\n",
      "#870 Loss: 0.1996276080608368\n",
      "#871 Loss: 0.1996276080608368\n",
      "#872 Loss: 0.1996276080608368\n",
      "#873 Loss: 0.1996276080608368\n",
      "#874 Loss: 0.1996276080608368\n",
      "#875 Loss: 0.1996275931596756\n",
      "#876 Loss: 0.1996275931596756\n",
      "#877 Loss: 0.1996275931596756\n",
      "#878 Loss: 0.1996275931596756\n",
      "#879 Loss: 0.1996275931596756\n",
      "#880 Loss: 0.1996275931596756\n",
      "#881 Loss: 0.1996275931596756\n",
      "#882 Loss: 0.1996275633573532\n",
      "#883 Loss: 0.1996275633573532\n",
      "#884 Loss: 0.1996275633573532\n",
      "#885 Loss: 0.19962754845619202\n",
      "#886 Loss: 0.19962754845619202\n",
      "#887 Loss: 0.19962754845619202\n",
      "#888 Loss: 0.19962753355503082\n",
      "#889 Loss: 0.19962753355503082\n",
      "#890 Loss: 0.19962753355503082\n",
      "#891 Loss: 0.19962753355503082\n",
      "#892 Loss: 0.19962753355503082\n",
      "#893 Loss: 0.19962751865386963\n",
      "#894 Loss: 0.19962750375270844\n",
      "#895 Loss: 0.19962750375270844\n",
      "#896 Loss: 0.19962750375270844\n",
      "#897 Loss: 0.19962750375270844\n",
      "#898 Loss: 0.19962750375270844\n",
      "#899 Loss: 0.19962750375270844\n",
      "#900 Loss: 0.19962750375270844\n",
      "#901 Loss: 0.19962750375270844\n",
      "#902 Loss: 0.19962748885154724\n",
      "#903 Loss: 0.19962748885154724\n",
      "#904 Loss: 0.19962748885154724\n",
      "#905 Loss: 0.19962747395038605\n",
      "#906 Loss: 0.19962747395038605\n",
      "#907 Loss: 0.19962745904922485\n",
      "#908 Loss: 0.19962745904922485\n",
      "#909 Loss: 0.19962745904922485\n",
      "#910 Loss: 0.19962745904922485\n",
      "#911 Loss: 0.19962745904922485\n",
      "#912 Loss: 0.19962744414806366\n",
      "#913 Loss: 0.19962744414806366\n",
      "#914 Loss: 0.19962744414806366\n",
      "#915 Loss: 0.19962744414806366\n",
      "#916 Loss: 0.19962744414806366\n",
      "#917 Loss: 0.19962744414806366\n",
      "#918 Loss: 0.19962742924690247\n",
      "#919 Loss: 0.19962742924690247\n",
      "#920 Loss: 0.19962742924690247\n",
      "#921 Loss: 0.19962741434574127\n",
      "#922 Loss: 0.19962741434574127\n",
      "#923 Loss: 0.19962741434574127\n",
      "#924 Loss: 0.19962739944458008\n",
      "#925 Loss: 0.19962739944458008\n",
      "#926 Loss: 0.19962739944458008\n",
      "#927 Loss: 0.19962739944458008\n",
      "#928 Loss: 0.19962739944458008\n",
      "#929 Loss: 0.19962739944458008\n",
      "#930 Loss: 0.19962738454341888\n",
      "#931 Loss: 0.19962738454341888\n",
      "#932 Loss: 0.1996273696422577\n",
      "#933 Loss: 0.1996273696422577\n",
      "#934 Loss: 0.1996273696422577\n",
      "#935 Loss: 0.1996273547410965\n",
      "#936 Loss: 0.1996273398399353\n",
      "#937 Loss: 0.1996273398399353\n",
      "#938 Loss: 0.1996273398399353\n",
      "#939 Loss: 0.1996273398399353\n",
      "#940 Loss: 0.1996273249387741\n",
      "#941 Loss: 0.1996273249387741\n",
      "#942 Loss: 0.1996273249387741\n",
      "#943 Loss: 0.1996273249387741\n",
      "#944 Loss: 0.1996273249387741\n",
      "#945 Loss: 0.1996273249387741\n",
      "#946 Loss: 0.1996273249387741\n",
      "#947 Loss: 0.1996273249387741\n",
      "#948 Loss: 0.1996273249387741\n",
      "#949 Loss: 0.1996273249387741\n",
      "#950 Loss: 0.19962731003761292\n",
      "#951 Loss: 0.19962731003761292\n",
      "#952 Loss: 0.19962731003761292\n",
      "#953 Loss: 0.19962731003761292\n",
      "#954 Loss: 0.19962731003761292\n",
      "#955 Loss: 0.19962731003761292\n",
      "#956 Loss: 0.19962731003761292\n",
      "#957 Loss: 0.19962729513645172\n",
      "#958 Loss: 0.19962729513645172\n",
      "#959 Loss: 0.19962729513645172\n",
      "#960 Loss: 0.19962728023529053\n",
      "#961 Loss: 0.19962728023529053\n",
      "#962 Loss: 0.19962726533412933\n",
      "#963 Loss: 0.19962726533412933\n",
      "#964 Loss: 0.19962726533412933\n",
      "#965 Loss: 0.19962726533412933\n",
      "#966 Loss: 0.19962726533412933\n",
      "#967 Loss: 0.19962726533412933\n",
      "#968 Loss: 0.19962726533412933\n",
      "#969 Loss: 0.19962726533412933\n",
      "#970 Loss: 0.19962725043296814\n",
      "#971 Loss: 0.19962723553180695\n",
      "#972 Loss: 0.19962723553180695\n",
      "#973 Loss: 0.19962723553180695\n",
      "#974 Loss: 0.19962723553180695\n",
      "#975 Loss: 0.19962723553180695\n",
      "#976 Loss: 0.19962723553180695\n",
      "#977 Loss: 0.19962723553180695\n",
      "#978 Loss: 0.19962723553180695\n",
      "#979 Loss: 0.19962723553180695\n",
      "#980 Loss: 0.19962723553180695\n",
      "#981 Loss: 0.19962722063064575\n",
      "#982 Loss: 0.19962722063064575\n",
      "#983 Loss: 0.19962720572948456\n",
      "#984 Loss: 0.19962720572948456\n",
      "#985 Loss: 0.19962719082832336\n",
      "#986 Loss: 0.19962719082832336\n",
      "#987 Loss: 0.19962719082832336\n",
      "#988 Loss: 0.19962719082832336\n",
      "#989 Loss: 0.19962719082832336\n",
      "#990 Loss: 0.19962719082832336\n",
      "#991 Loss: 0.19962719082832336\n",
      "#992 Loss: 0.19962719082832336\n",
      "#993 Loss: 0.19962716102600098\n",
      "#994 Loss: 0.19962716102600098\n",
      "#995 Loss: 0.19962716102600098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#996 Loss: 0.19962716102600098\n",
      "#997 Loss: 0.19962716102600098\n",
      "#998 Loss: 0.19962714612483978\n",
      "#999 Loss: 0.19962714612483978\n",
      "Predicted data based on trained weights: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'xPredicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-3ebc9086d5ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-74-25e7c1e4e658>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted data based on trained weights: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Input (scaled): \\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxPredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Output: \\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxPredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xPredicted' is not defined"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "for i in range(1000):  # trains the NN 1,000 times\n",
    "    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(x)) ** 2).detach().item()))  # mean sum squared loss\n",
    "    NN.train(x, y)\n",
    "NN.saveWeights(NN)\n",
    "NN.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.float()\n",
    "y = y.float()\n",
    "x.type()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
