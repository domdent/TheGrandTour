{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dom\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sklearn\n",
    "\n",
    "\n",
    "readdata = pd.read_csv(\"C:/Users/Dom/MPhys/TheGrandTour/wine_data.txt\", sep=\"\\t\", header=None);\n",
    "data = np.array(readdata);\n",
    "data = np.delete(data, 0, 0)\n",
    "data = data.astype(float)\n",
    "data = np.swapaxes(data,0,1)\n",
    "\n",
    "\n",
    "# Need to seperate the classification dimension:\n",
    "classification = data[13]\n",
    "data = np.delete(data, 13, axis=0)\n",
    "\n",
    "\n",
    "# make list of colours for each number:\n",
    "data_colour = []\n",
    "for i in range(len(classification)):\n",
    "    if classification[i] == 1:\n",
    "        data_colour.append(\"r\")\n",
    "    elif classification[i] == 2:\n",
    "        data_colour.append(\"b\")\n",
    "    elif classification[i] == 3:\n",
    "        data_colour.append(\"g\")\n",
    "        \n",
    "# Normalizes the data        \n",
    "for i in range(0, np.shape(data)[0]):\n",
    "    data[i,:] = (data[i,:] / np.ndarray.max(data[i,:])) * 2 - 1\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "#VARIABLES\n",
    "stepSize = 0.01\n",
    "nSteps = 10000\n",
    "\n",
    "def getAlpha(d):\n",
    "    \"\"\"\n",
    "    NEEDS IMPLEMENTATION\n",
    "    Should produce 1xd(d-1)/2 array of position in grand tour.\n",
    "    \"\"\"\n",
    "    p = d*(d-1)/2     \n",
    "    primeList = []\n",
    "    count = 1\n",
    "    while len(primeList) < p:\n",
    "        count += 1\n",
    "        primeBool = False\n",
    "        for i in range(2, count - 1):\n",
    "            if count % i == 0:\n",
    "                primeBool = True\n",
    "        if primeBool == False:\n",
    "            irrational = (np.sqrt(count)%1)\n",
    "            primeList.append(irrational)\n",
    "            \n",
    "    primeList = np.asarray(primeList)\n",
    "    primeList = primeList.dot(stepSize)\n",
    "    \"\"\"\n",
    "    Irrational number generation using exponentials, not being used\n",
    "    p = int(d*(d-1)/2)\n",
    "    alpha = np.zeros(p) #alpha(t) parameters defining grand tour in G2,d\n",
    "    for i in range(0,p):\n",
    "        alpha[i] = (np.exp(i) % 1) * 2 * np.pi\n",
    "        \n",
    "    alpha = alpha.dot(0.001)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return primeList\n",
    "\n",
    "\n",
    "def getAngles(alpha,d):\n",
    "    \"\"\"\"\"\n",
    "    Inputs: \n",
    "    alpha = 1xd(d-1)/2 array defining position on grand tour\n",
    "    d = dimensions of data\n",
    "    Outputs a dxd array of angles required for the transformation\n",
    "    \"\"\"\n",
    "    theta = np.zeros((d,d));\n",
    "    i = 0;\n",
    "    k = 0;\n",
    "    \n",
    "    while i < d-1:\n",
    "        j = i + 1;\n",
    "        \n",
    "        while j < d:\n",
    "            theta[i][j] = alpha[k];\n",
    "            j += 1;\n",
    "            k += 1;\n",
    "    \n",
    "        i+= 1;\n",
    "        \n",
    "    return theta;\n",
    "\n",
    "\n",
    "def RotationMatrix(i, j, d, theta):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    i = first indicie of rotating plane\n",
    "    j = second indicie of rotating plane\n",
    "    d = dimensions of data\n",
    "    theta = dxd array of angle of rotation of rotating plane\n",
    "    Outputs a rotating matrix to rotate plane of ixj plane by theta_ij\n",
    "    \"\"\"\n",
    "    R = np.identity(d)\n",
    "    R[i,i] = np.cos(theta)\n",
    "    R[i,j] = -1*np.sin(theta)\n",
    "    R[j,i] = np.sin(theta)\n",
    "    R[j,j] = np.cos(theta)\n",
    "    return R\n",
    "\n",
    "\n",
    "def BetaFn(d, theta):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    d = dimensions of data\n",
    "    theta = dxd array of angle of rotation ixj plane\n",
    "    Outputs the full matrix transformation for all rotations\n",
    "    \"\"\"\n",
    "    b = RotationMatrix(1, 2, d, theta[1,2])\n",
    "    i = 1\n",
    "    j = 2\n",
    "    for i in range(d):\n",
    "        for j in range(d):\n",
    "            if j <= i:\n",
    "                continue\n",
    "            if i==1 and j==2:\n",
    "                continue\n",
    "            b = np.matmul(b, RotationMatrix(i, j, d, theta[i,j]))\n",
    "            \n",
    "    return b\n",
    "\n",
    "\n",
    "def GrandTour(data, nSteps):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    data = array of data points, dimensions x npoints\n",
    "    Outputs a 3D array number of points x t x dimensions, where t\n",
    "    the time step at that point in the tour\n",
    "    \"\"\"\n",
    "\n",
    "    d = np.shape(data)[0] #dimensions of data\n",
    "    nPoints = np.shape(data)[1] #number of data points\n",
    "    tData = np.zeros((nSteps,d,nPoints)) #initialise 3d matrix to store stransforemd data at each timestep\n",
    "    tBeta = np.zeros((nSteps,d,d))\n",
    "    Alpha = getAlpha(d)\n",
    "\n",
    "    \n",
    "    for t in range(0, nSteps):\n",
    "        \n",
    "        \n",
    "        alpha = Alpha.dot(t)\n",
    "        theta = getAngles(alpha, d)\n",
    "        b = BetaFn(d, theta)\n",
    "        a = np.matmul(b, data)\n",
    "        tData[t,:,:] = a\n",
    "        tBeta[t,:,:] = b\n",
    "        \n",
    "    return tData, tBeta\n",
    "\n",
    "\n",
    "tData, tBeta = GrandTour(data, nSteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetData = np.zeros((len(tData[0][0]), 3))\n",
    "for counter, i in enumerate(classification):\n",
    "    targetData[counter][int(i-1)] = 1\n",
    "targetData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00012836446171529927\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 5s 37ms/step - loss: 1.1946 - acc: 0.0845 - val_loss: 0.9200 - val_acc: 1.0000\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 168us/step - loss: 1.1726 - acc: 0.0845 - val_loss: 0.9573 - val_acc: 1.0000\n",
      "5.78614590079761\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 5s 39ms/step - loss: 1.1426 - acc: 0.0915 - val_loss: 1.0988 - val_acc: 0.3056\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 169us/step - loss: 1.1216 - acc: 0.0845 - val_loss: 1.1286 - val_acc: 0.1111\n",
      "11.756907977267822\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 42ms/step - loss: 1.0852 - acc: 0.4296 - val_loss: 1.1930 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 183us/step - loss: 1.0627 - acc: 0.4366 - val_loss: 1.2313 - val_acc: 0.0000e+00\n",
      "18.154360817915013\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 39ms/step - loss: 1.0765 - acc: 0.3099 - val_loss: 1.1578 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 175us/step - loss: 1.0547 - acc: 0.3028 - val_loss: 1.1937 - val_acc: 0.0000e+00\n",
      "24.178162214753744\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 42ms/step - loss: 1.0997 - acc: 0.4225 - val_loss: 1.1712 - val_acc: 0.0556\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 176us/step - loss: 1.0842 - acc: 0.4225 - val_loss: 1.1933 - val_acc: 0.0000e+00\n",
      "30.600204507926506\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 39ms/step - loss: 1.1191 - acc: 0.1268 - val_loss: 1.0632 - val_acc: 0.8611\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 183us/step - loss: 1.1053 - acc: 0.2817 - val_loss: 1.0834 - val_acc: 0.5278\n",
      "36.63427105031303\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 39ms/step - loss: 1.1099 - acc: 0.1127 - val_loss: 1.1074 - val_acc: 0.0556\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 169us/step - loss: 1.0893 - acc: 0.4155 - val_loss: 1.1302 - val_acc: 0.0000e+00\n",
      "42.71065410445367\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 7s 47ms/step - loss: 1.2243 - acc: 0.1056 - val_loss: 0.9302 - val_acc: 1.0000\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 204us/step - loss: 1.1916 - acc: 0.1056 - val_loss: 0.9636 - val_acc: 0.9444\n",
      "49.82071350219095\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 43ms/step - loss: 1.0887 - acc: 0.3662 - val_loss: 1.1003 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 169us/step - loss: 1.0684 - acc: 0.2887 - val_loss: 1.1210 - val_acc: 0.0000e+00\n",
      "56.614688541137895\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 43ms/step - loss: 1.0846 - acc: 0.4577 - val_loss: 1.0053 - val_acc: 0.9722\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 183us/step - loss: 1.0709 - acc: 0.5915 - val_loss: 1.0200 - val_acc: 0.8889\n",
      "63.271461298114794\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 43ms/step - loss: 1.0449 - acc: 0.5352 - val_loss: 1.3244 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 182us/step - loss: 1.0240 - acc: 0.6408 - val_loss: 1.3515 - val_acc: 0.0000e+00\n",
      "69.86137294543927\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 43ms/step - loss: 1.0939 - acc: 0.5352 - val_loss: 1.0575 - val_acc: 0.9722\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 182us/step - loss: 1.0785 - acc: 0.6268 - val_loss: 1.0657 - val_acc: 0.9722\n",
      "76.54793026892355\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 43ms/step - loss: 1.0603 - acc: 0.7535 - val_loss: 1.1601 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 169us/step - loss: 1.0451 - acc: 0.7394 - val_loss: 1.1676 - val_acc: 0.0000e+00\n",
      "83.11087443913482\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 43ms/step - loss: 1.0930 - acc: 0.5563 - val_loss: 1.1157 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 196us/step - loss: 1.0830 - acc: 0.6549 - val_loss: 1.1345 - val_acc: 0.0000e+00\n",
      "89.78201453435791\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 44ms/step - loss: 1.0448 - acc: 0.5000 - val_loss: 1.1692 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 176us/step - loss: 1.0328 - acc: 0.5000 - val_loss: 1.1929 - val_acc: 0.0000e+00\n",
      "96.64660133207303\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 44ms/step - loss: 1.0703 - acc: 0.3732 - val_loss: 1.1719 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 218us/step - loss: 1.0612 - acc: 0.3662 - val_loss: 1.1825 - val_acc: 0.0000e+00\n",
      "103.48319920705771\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 45ms/step - loss: 1.0920 - acc: 0.2465 - val_loss: 1.0536 - val_acc: 0.6944\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 189us/step - loss: 1.0769 - acc: 0.6408 - val_loss: 1.0651 - val_acc: 0.6944\n",
      "110.35028728841745\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 6s 45ms/step - loss: 1.1075 - acc: 0.1620 - val_loss: 1.1437 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 211us/step - loss: 1.0857 - acc: 0.4155 - val_loss: 1.1557 - val_acc: 0.0000e+00\n",
      "117.27411136784004\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 7s 49ms/step - loss: 1.0691 - acc: 0.5070 - val_loss: 1.0930 - val_acc: 0.1111\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 204us/step - loss: 1.0542 - acc: 0.5493 - val_loss: 1.1010 - val_acc: 0.0000e+00\n",
      "124.72884867288651\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 7s 51ms/step - loss: 1.1545 - acc: 0.0915 - val_loss: 0.9985 - val_acc: 0.8333\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 197us/step - loss: 1.1336 - acc: 0.1690 - val_loss: 1.0063 - val_acc: 0.5833\n",
      "132.7626198311424\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 7s 51ms/step - loss: 1.0867 - acc: 0.4577 - val_loss: 1.1703 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 197us/step - loss: 1.0758 - acc: 0.4648 - val_loss: 1.1712 - val_acc: 0.0000e+00\n",
      "141.17533965528307\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 7s 53ms/step - loss: 1.0955 - acc: 0.3239 - val_loss: 1.1255 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 197us/step - loss: 1.0761 - acc: 0.5000 - val_loss: 1.1327 - val_acc: 0.0000e+00\n",
      "149.1949721244902\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 7s 50ms/step - loss: 1.1310 - acc: 0.4085 - val_loss: 1.0760 - val_acc: 0.1389\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 225us/step - loss: 1.1127 - acc: 0.4085 - val_loss: 1.0850 - val_acc: 0.1389\n",
      "156.79314650469405\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 7s 49ms/step - loss: 1.0795 - acc: 0.4507 - val_loss: 1.1861 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 218us/step - loss: 1.0536 - acc: 0.6549 - val_loss: 1.2130 - val_acc: 0.0000e+00\n",
      "164.39877659915862\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 7s 50ms/step - loss: 1.0820 - acc: 0.5070 - val_loss: 1.1811 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 232us/step - loss: 1.0662 - acc: 0.5563 - val_loss: 1.1992 - val_acc: 0.0000e+00\n",
      "172.0536618150735\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 7s 51ms/step - loss: 1.1163 - acc: 0.2817 - val_loss: 1.1193 - val_acc: 0.1111\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 211us/step - loss: 1.0975 - acc: 0.3803 - val_loss: 1.1312 - val_acc: 0.0000e+00\n",
      "179.78876132670507\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 7s 53ms/step - loss: 1.0533 - acc: 0.2606 - val_loss: 1.1425 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 218us/step - loss: 1.0320 - acc: 0.4225 - val_loss: 1.1629 - val_acc: 0.0000e+00\n",
      "187.8238806764807\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 7s 49ms/step - loss: 1.0856 - acc: 0.4225 - val_loss: 1.0773 - val_acc: 0.3889\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 204us/step - loss: 1.0611 - acc: 0.7535 - val_loss: 1.0981 - val_acc: 0.3333\n",
      "195.36230468180202\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 53ms/step - loss: 1.1225 - acc: 0.2183 - val_loss: 1.1734 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 196us/step - loss: 1.0911 - acc: 0.4225 - val_loss: 1.1972 - val_acc: 0.0000e+00\n",
      "203.45567742885984\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 54ms/step - loss: 1.0668 - acc: 0.6338 - val_loss: 1.1863 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 225us/step - loss: 1.0467 - acc: 0.6761 - val_loss: 1.1891 - val_acc: 0.0000e+00\n",
      "211.60861019213826\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 57ms/step - loss: 1.1591 - acc: 0.0986 - val_loss: 1.0460 - val_acc: 0.8056\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 204us/step - loss: 1.1255 - acc: 0.1761 - val_loss: 1.0557 - val_acc: 0.7500\n",
      "220.23943545893212\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 7s 53ms/step - loss: 1.1465 - acc: 0.3380 - val_loss: 1.0525 - val_acc: 0.5833\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 204us/step - loss: 1.1193 - acc: 0.4014 - val_loss: 1.0672 - val_acc: 0.5556\n",
      "228.286109433621\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 7s 52ms/step - loss: 1.0803 - acc: 0.4718 - val_loss: 1.1419 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 197us/step - loss: 1.0599 - acc: 0.5000 - val_loss: 1.1677 - val_acc: 0.0000e+00\n",
      "236.13100468530286\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 53ms/step - loss: 1.1059 - acc: 0.2746 - val_loss: 1.0969 - val_acc: 0.1111\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 218us/step - loss: 1.0845 - acc: 0.4789 - val_loss: 1.1120 - val_acc: 0.0556\n",
      "244.22310253986592\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 54ms/step - loss: 1.2247 - acc: 0.0845 - val_loss: 1.0508 - val_acc: 0.6389\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 204us/step - loss: 1.1900 - acc: 0.1197 - val_loss: 1.0601 - val_acc: 0.6111\n",
      "252.48184663947674\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 56ms/step - loss: 1.0988 - acc: 0.2606 - val_loss: 1.1267 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 204us/step - loss: 1.0839 - acc: 0.3521 - val_loss: 1.1171 - val_acc: 0.0000e+00\n",
      "260.92692233074854\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 57ms/step - loss: 1.0909 - acc: 0.2746 - val_loss: 1.1949 - val_acc: 0.1111\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 232us/step - loss: 1.0723 - acc: 0.4155 - val_loss: 1.1981 - val_acc: 0.0000e+00\n",
      "269.54973940555584\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 55ms/step - loss: 1.0761 - acc: 0.4577 - val_loss: 1.3344 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 225us/step - loss: 1.0598 - acc: 0.4577 - val_loss: 1.3310 - val_acc: 0.0000e+00\n",
      "277.9376305524923\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 57ms/step - loss: 1.0891 - acc: 0.3169 - val_loss: 1.3246 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 211us/step - loss: 1.0713 - acc: 0.4014 - val_loss: 1.3043 - val_acc: 0.0000e+00\n",
      "286.5618866220892\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 58ms/step - loss: 1.0903 - acc: 0.4507 - val_loss: 1.0249 - val_acc: 0.1667\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 203us/step - loss: 1.0685 - acc: 0.4789 - val_loss: 1.0308 - val_acc: 0.1389\n",
      "295.3220150157247\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 58ms/step - loss: 1.0951 - acc: 0.3873 - val_loss: 1.1526 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 218us/step - loss: 1.0843 - acc: 0.4366 - val_loss: 1.1575 - val_acc: 0.0000e+00\n",
      "304.141805898347\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 60ms/step - loss: 1.0972 - acc: 0.4577 - val_loss: 1.1913 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 225us/step - loss: 1.0783 - acc: 0.4859 - val_loss: 1.2004 - val_acc: 0.0000e+00\n",
      "313.1465313150939\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 58ms/step - loss: 1.0775 - acc: 0.4296 - val_loss: 1.0172 - val_acc: 0.8333\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 210us/step - loss: 1.0589 - acc: 0.4930 - val_loss: 1.0178 - val_acc: 0.8333\n",
      "322.018765279747\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 9s 61ms/step - loss: 1.0751 - acc: 0.5493 - val_loss: 1.1075 - val_acc: 0.3333\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 224us/step - loss: 1.0566 - acc: 0.6549 - val_loss: 1.1128 - val_acc: 0.2222\n",
      "331.28298601994317\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 9s 63ms/step - loss: 1.0974 - acc: 0.0845 - val_loss: 1.0881 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 218us/step - loss: 1.0810 - acc: 0.4366 - val_loss: 1.1024 - val_acc: 0.0000e+00\n",
      "340.7425548612205\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 9s 64ms/step - loss: 1.1280 - acc: 0.3592 - val_loss: 1.0890 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 225us/step - loss: 1.1121 - acc: 0.4155 - val_loss: 1.0985 - val_acc: 0.0000e+00\n",
      "350.4284101915548\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 9s 63ms/step - loss: 1.1290 - acc: 0.3028 - val_loss: 1.1818 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 232us/step - loss: 1.1072 - acc: 0.2958 - val_loss: 1.1913 - val_acc: 0.0000e+00\n",
      "359.9650119320602\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 59ms/step - loss: 1.0705 - acc: 0.4930 - val_loss: 1.1290 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 239us/step - loss: 1.0499 - acc: 0.5775 - val_loss: 1.1384 - val_acc: 0.0000e+00\n",
      "368.8796462829736\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 8s 59ms/step - loss: 1.0380 - acc: 0.5000 - val_loss: 1.1195 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 225us/step - loss: 1.0156 - acc: 0.5000 - val_loss: 1.1286 - val_acc: 0.0000e+00\n",
      "377.77670564278503\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 8s 60ms/step - loss: 1.0288 - acc: 0.5000 - val_loss: 1.0966 - val_acc: 0.6111\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 225us/step - loss: 1.0031 - acc: 0.5493 - val_loss: 1.1181 - val_acc: 0.2500\n",
      "386.7392655218891\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 9s 63ms/step - loss: 1.0155 - acc: 0.5563 - val_loss: 1.1509 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 239us/step - loss: 0.9932 - acc: 0.5986 - val_loss: 1.1696 - val_acc: 0.0000e+00\n",
      "396.1902036764749\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 9s 62ms/step - loss: 1.0700 - acc: 0.5775 - val_loss: 1.1110 - val_acc: 0.0278\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 281us/step - loss: 1.0540 - acc: 0.5423 - val_loss: 1.1194 - val_acc: 0.0000e+00\n",
      "405.5687377105615\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 10s 72ms/step - loss: 1.0789 - acc: 0.4930 - val_loss: 1.1502 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 232us/step - loss: 1.0685 - acc: 0.6056 - val_loss: 1.1570 - val_acc: 0.0000e+00\n",
      "416.2872701838529\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 9s 65ms/step - loss: 1.0962 - acc: 0.5141 - val_loss: 1.0335 - val_acc: 0.1667\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 253us/step - loss: 1.0791 - acc: 0.5352 - val_loss: 1.0187 - val_acc: 0.4722\n",
      "426.01061194839747\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 10s 68ms/step - loss: 1.1607 - acc: 0.1268 - val_loss: 1.1962 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 239us/step - loss: 1.1356 - acc: 0.2746 - val_loss: 1.1993 - val_acc: 0.0000e+00\n",
      "436.20906960562934\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 9s 67ms/step - loss: 1.1170 - acc: 0.1268 - val_loss: 0.9525 - val_acc: 1.0000\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 239us/step - loss: 1.0936 - acc: 0.4930 - val_loss: 0.9510 - val_acc: 1.0000\n",
      "446.24317370920784\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 10s 68ms/step - loss: 1.1003 - acc: 0.3310 - val_loss: 1.1783 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 309us/step - loss: 1.0703 - acc: 0.5915 - val_loss: 1.1763 - val_acc: 0.0000e+00\n",
      "456.5189158891865\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 10s 71ms/step - loss: 1.1366 - acc: 0.0282 - val_loss: 1.0812 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 253us/step - loss: 1.1123 - acc: 0.4085 - val_loss: 1.0821 - val_acc: 0.0000e+00\n",
      "467.24044523505285\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 10s 69ms/step - loss: 1.0739 - acc: 0.5493 - val_loss: 1.1855 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 246us/step - loss: 1.0511 - acc: 0.5845 - val_loss: 1.1974 - val_acc: 0.0000e+00\n",
      "477.54805024593463\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 10s 69ms/step - loss: 1.0811 - acc: 0.4085 - val_loss: 1.1287 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 295us/step - loss: 1.0662 - acc: 0.7183 - val_loss: 1.1527 - val_acc: 0.0000e+00\n",
      "487.87325395158325\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 10s 68ms/step - loss: 1.1053 - acc: 0.3944 - val_loss: 1.0968 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 295us/step - loss: 1.0894 - acc: 0.5000 - val_loss: 1.1059 - val_acc: 0.0000e+00\n",
      "498.11776199478373\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 10s 69ms/step - loss: 1.0945 - acc: 0.2394 - val_loss: 1.0996 - val_acc: 0.4444\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 232us/step - loss: 1.0749 - acc: 0.4718 - val_loss: 1.1167 - val_acc: 0.2222\n",
      "508.61531650591934\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 10s 68ms/step - loss: 1.1035 - acc: 0.5211 - val_loss: 1.0562 - val_acc: 0.3611\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 225us/step - loss: 1.0761 - acc: 0.5000 - val_loss: 1.0872 - val_acc: 0.2500\n",
      "518.7305206929348\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 10s 72ms/step - loss: 1.0573 - acc: 0.5211 - val_loss: 1.1499 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 288us/step - loss: 1.0343 - acc: 0.6549 - val_loss: 1.1770 - val_acc: 0.0000e+00\n",
      "529.5600501350744\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 11s 76ms/step - loss: 1.1363 - acc: 0.1408 - val_loss: 1.0472 - val_acc: 0.6667\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 267us/step - loss: 1.1035 - acc: 0.3169 - val_loss: 1.0677 - val_acc: 0.4444\n",
      "540.906981713899\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 11s 75ms/step - loss: 1.1222 - acc: 0.3380 - val_loss: 1.0871 - val_acc: 0.3889\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 288us/step - loss: 1.1011 - acc: 0.4366 - val_loss: 1.1144 - val_acc: 0.0278\n",
      "552.19689430937\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 11s 77ms/step - loss: 1.0696 - acc: 0.5211 - val_loss: 1.0847 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 225us/step - loss: 1.0506 - acc: 0.7606 - val_loss: 1.1041 - val_acc: 0.0000e+00\n",
      "563.7458905138662\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 10s 71ms/step - loss: 1.0860 - acc: 0.4859 - val_loss: 1.0392 - val_acc: 0.8889\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 281us/step - loss: 1.0720 - acc: 0.5282 - val_loss: 1.0542 - val_acc: 0.8333\n",
      "574.4055263088799\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 10s 72ms/step - loss: 1.0845 - acc: 0.5493 - val_loss: 1.1673 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 281us/step - loss: 1.0704 - acc: 0.5775 - val_loss: 1.1965 - val_acc: 0.0000e+00\n",
      "585.2475187733025\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 10s 73ms/step - loss: 1.1143 - acc: 0.1690 - val_loss: 1.0336 - val_acc: 0.8056\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 274us/step - loss: 1.0991 - acc: 0.2676 - val_loss: 1.0589 - val_acc: 0.8056\n",
      "596.3344291719909\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 10s 72ms/step - loss: 1.0836 - acc: 0.4085 - val_loss: 1.0932 - val_acc: 0.4722\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 260us/step - loss: 1.0707 - acc: 0.4225 - val_loss: 1.1256 - val_acc: 0.4167\n",
      "607.1269896491566\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 11s 78ms/step - loss: 1.0670 - acc: 0.5000 - val_loss: 1.3824 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 267us/step - loss: 1.0544 - acc: 0.5000 - val_loss: 1.4143 - val_acc: 0.0000e+00\n",
      "618.7403241640264\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 11s 78ms/step - loss: 1.0673 - acc: 0.4507 - val_loss: 1.1750 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 267us/step - loss: 1.0535 - acc: 0.5000 - val_loss: 1.2083 - val_acc: 0.0000e+00\n",
      "630.3912450331705\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 11s 79ms/step - loss: 1.1125 - acc: 0.4155 - val_loss: 1.0207 - val_acc: 0.3333\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 0s 260us/step - loss: 1.0969 - acc: 0.4437 - val_loss: 1.0626 - val_acc: 0.1667\n",
      "642.0608170397988\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 11s 78ms/step - loss: 1.0839 - acc: 0.4718 - val_loss: 1.1083 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 316us/step - loss: 1.0680 - acc: 0.5000 - val_loss: 1.1404 - val_acc: 0.0000e+00\n",
      "653.7153594642534\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 12s 83ms/step - loss: 1.0961 - acc: 0.4296 - val_loss: 1.1141 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 295us/step - loss: 1.0777 - acc: 0.4789 - val_loss: 1.1501 - val_acc: 0.0000e+00\n",
      "666.1216362676282\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 12s 82ms/step - loss: 1.1187 - acc: 0.1479 - val_loss: 1.0871 - val_acc: 0.4722\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 288us/step - loss: 1.1064 - acc: 0.2958 - val_loss: 1.1009 - val_acc: 0.2500\n",
      "678.3422575807966\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 12s 82ms/step - loss: 1.0997 - acc: 0.2606 - val_loss: 1.0892 - val_acc: 0.3333\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 337us/step - loss: 1.0830 - acc: 0.3310 - val_loss: 1.1070 - val_acc: 0.0556\n",
      "690.5456251786892\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 12s 81ms/step - loss: 1.0747 - acc: 0.4366 - val_loss: 1.1480 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 288us/step - loss: 1.0611 - acc: 0.5915 - val_loss: 1.1545 - val_acc: 0.0000e+00\n",
      "702.7525344687754\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 11s 80ms/step - loss: 1.0996 - acc: 0.2183 - val_loss: 0.9935 - val_acc: 1.0000\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 302us/step - loss: 1.0835 - acc: 0.4437 - val_loss: 1.0068 - val_acc: 0.9444\n",
      "714.6240861325538\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 12s 84ms/step - loss: 1.0693 - acc: 0.4718 - val_loss: 1.3376 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 288us/step - loss: 1.0468 - acc: 0.5845 - val_loss: 1.3491 - val_acc: 0.0000e+00\n",
      "727.1203989363254\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 12s 82ms/step - loss: 1.0718 - acc: 0.5493 - val_loss: 1.0818 - val_acc: 0.7778\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 267us/step - loss: 1.0541 - acc: 0.5634 - val_loss: 1.0919 - val_acc: 0.2778\n",
      "739.4372367069848\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 12s 83ms/step - loss: 1.0936 - acc: 0.4155 - val_loss: 1.2112 - val_acc: 0.0278\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 288us/step - loss: 1.0736 - acc: 0.6761 - val_loss: 1.2096 - val_acc: 0.0000e+00\n",
      "751.8227326897606\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 12s 86ms/step - loss: 1.0365 - acc: 0.5915 - val_loss: 1.1610 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 295us/step - loss: 1.0119 - acc: 0.6620 - val_loss: 1.1832 - val_acc: 0.0000e+00\n",
      "764.6554176366935\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 12s 84ms/step - loss: 1.1214 - acc: 0.3662 - val_loss: 1.1198 - val_acc: 0.1111\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 267us/step - loss: 1.0997 - acc: 0.3873 - val_loss: 1.1297 - val_acc: 0.0278\n",
      "777.1593444222724\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 12s 86ms/step - loss: 1.1124 - acc: 0.2254 - val_loss: 0.9194 - val_acc: 1.0000\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 344us/step - loss: 1.0923 - acc: 0.2394 - val_loss: 0.9333 - val_acc: 1.0000\n",
      "789.983268859365\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 12s 86ms/step - loss: 1.1096 - acc: 0.3451 - val_loss: 1.1816 - val_acc: 0.0278\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 302us/step - loss: 1.0939 - acc: 0.4648 - val_loss: 1.1803 - val_acc: 0.0000e+00\n",
      "802.7934623104436\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 12s 86ms/step - loss: 1.1634 - acc: 0.1831 - val_loss: 1.0058 - val_acc: 0.0278\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 323us/step - loss: 1.1407 - acc: 0.1831 - val_loss: 1.0236 - val_acc: 0.0278\n",
      "815.6123834509035\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 12s 87ms/step - loss: 1.1015 - acc: 0.4155 - val_loss: 1.2704 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 281us/step - loss: 1.0831 - acc: 0.4014 - val_loss: 1.2971 - val_acc: 0.0000e+00\n",
      "828.5489174353947\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 12s 87ms/step - loss: 0.9907 - acc: 0.5000 - val_loss: 1.3078 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 316us/step - loss: 0.9765 - acc: 0.5000 - val_loss: 1.3309 - val_acc: 0.0000e+00\n",
      "841.4668524304644\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 14s 97ms/step - loss: 1.1094 - acc: 0.3592 - val_loss: 1.0612 - val_acc: 0.4444\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 274us/step - loss: 1.0903 - acc: 0.4577 - val_loss: 1.0776 - val_acc: 0.3889\n",
      "855.7966345901381\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 13s 91ms/step - loss: 1.0739 - acc: 0.5000 - val_loss: 1.0603 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 239us/step - loss: 1.0603 - acc: 0.5000 - val_loss: 1.0764 - val_acc: 0.0000e+00\n",
      "869.2805222974905\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 13s 89ms/step - loss: 1.1278 - acc: 0.0915 - val_loss: 1.0691 - val_acc: 0.9167\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 253us/step - loss: 1.1054 - acc: 0.3099 - val_loss: 1.0804 - val_acc: 0.3889\n",
      "882.429751454311\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 13s 89ms/step - loss: 1.0925 - acc: 0.2676 - val_loss: 1.1186 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 260us/step - loss: 1.0744 - acc: 0.4085 - val_loss: 1.1382 - val_acc: 0.0000e+00\n",
      "896.434451708706\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 13s 92ms/step - loss: 1.1412 - acc: 0.0282 - val_loss: 1.1935 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 239us/step - loss: 1.1137 - acc: 0.0915 - val_loss: 1.2014 - val_acc: 0.0000e+00\n",
      "910.0941556973399\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 13s 93ms/step - loss: 1.0753 - acc: 0.6056 - val_loss: 1.0963 - val_acc: 0.3333\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 246us/step - loss: 1.0511 - acc: 0.7394 - val_loss: 1.1174 - val_acc: 0.0278\n",
      "923.9465478711921\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 13s 93ms/step - loss: 1.0840 - acc: 0.4718 - val_loss: 1.0518 - val_acc: 0.5556\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 260us/step - loss: 1.0618 - acc: 0.5423 - val_loss: 1.0727 - val_acc: 0.4444\n",
      "937.739209362437\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 16s 111ms/step - loss: 1.0628 - acc: 0.6268 - val_loss: 1.1941 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 351us/step - loss: 1.0415 - acc: 0.6690 - val_loss: 1.2242 - val_acc: 0.0000e+00\n",
      "954.1685673359123\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 16s 109ms/step - loss: 1.0865 - acc: 0.5070 - val_loss: 1.1146 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 246us/step - loss: 1.0677 - acc: 0.5986 - val_loss: 1.1379 - val_acc: 0.0000e+00\n",
      "970.3130586771458\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 13s 93ms/step - loss: 1.1061 - acc: 0.4859 - val_loss: 1.0962 - val_acc: 0.1667\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 288us/step - loss: 1.0905 - acc: 0.4437 - val_loss: 1.1125 - val_acc: 0.0000e+00\n",
      "[0.08450704272574103, 0.0880281694863044, 0.4330985923887978, 0.30633802869370286, 0.42253521137254335, 0.20422535368674238, 0.2640845073570668, 0.10563380302677691, 0.3274647899916474, 0.5246478864844416, 0.5880281715325906, 0.5809859150732067, 0.7464788736591876, 0.6056338023971504, 0.5000000006296266, 0.36971831069865696, 0.44366197099148386, 0.2887323946810104, 0.5281690166030132, 0.13028169024578282, 0.4612676060535538, 0.41197183224516853, 0.40845070548460516, 0.5528169018282018, 0.5316901400055685, 0.33098591549295775, 0.3415492961943989, 0.5880281677548315, 0.32042253647052066, 0.6549295770450376, 0.13732394429159836, 0.36971830964927943, 0.4859154930626842, 0.37676056348521947, 0.10211267631868241, 0.30633802942826716, 0.34507042285002454, 0.45774647950286595, 0.35915493125646886, 0.4647887336536193, 0.4119718318254175, 0.47183098738462154, 0.4612676068930559, 0.60211267689584, 0.2605633801767524, 0.3873239440817229, 0.2992957754873894, 0.5352112676056338, 0.500000001679004, 0.5246478871140681, 0.5774647912509006, 0.5598591557690795, 0.5492957744380118, 0.5246478875338192, 0.20070422582433256, 0.3098591557690795, 0.4612676071029314, 0.21830985994196273, 0.5669014082408288, 0.5633802808506387, 0.4471830988014248, 0.35563380313171467, 0.5105633802816901, 0.5880281685943335, 0.22887324069587278, 0.38732394471134934, 0.6408450700027841, 0.5070422533112513, 0.5633802837888959, 0.2183098595222117, 0.41549295806129216, 0.500000000419751, 0.4753521139353094, 0.42957746541835895, 0.48591549442687504, 0.4542253525324271, 0.22183098696487052, 0.29577464841201273, 0.5140845095607597, 0.3309859165423353, 0.528169014924009, 0.5563380290085161, 0.5457746489367015, 0.6267605627506552, 0.3767605640099082, 0.23239436745643616, 0.40492957788453976, 0.1830985921264534, 0.40845070527472965, 0.500000000839502, 0.40845070443522763, 0.5000000002098755, 0.2007042257718637, 0.3380281696437111, 0.05985915571661062, 0.6725352095886016, 0.5070422524717492, 0.647887324783164, 0.5528169016183262, 0.46478873249930397]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "for i in range(999):\n",
    "    if i % 10 == 0:\n",
    "        \n",
    "        print(time.clock() - start)\n",
    "\n",
    "        xData = tData[i][0]\n",
    "        yData = tData[i][1]\n",
    "        trainingData = np.vstack((xData, yData)).T\n",
    "        # NN model code\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(20, activation=tf.nn.relu))\n",
    "        model.add(tf.keras.layers.Dense(20, activation=tf.nn.relu))\n",
    "        model.add(tf.keras.layers.Dense(3, activation=tf.nn.softmax))\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        nnet = model.fit(trainingData, targetData, epochs=2, validation_split=0.2)\n",
    "        accuracy.append(np.mean(nnet.history[\"acc\"]))\n",
    "    \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0915492961943989,\n",
       " 0.4225352129466097,\n",
       " 0.3591549302070913,\n",
       " 0.5845070426732721,\n",
       " 0.3873239424027188,\n",
       " 0.647887322264658,\n",
       " 0.3556338032366524,\n",
       " 0.4894366201380609,\n",
       " 0.5000000014691286,\n",
       " 0.6056338049156565]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0.7464788736591876\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ba855d9d30>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = list(range(0, len(accuracy)))\n",
    "\n",
    "new = plt.figure()\n",
    "plt.plot(x, accuracy, \"-\", marker=\"None\")\n",
    "\n",
    "max_accuracy = max(accuracy)\n",
    "acc = max_accuracy\n",
    "max_accuracy = [i for i, j in enumerate(accuracy) if j == max_accuracy]\n",
    "max_accuracy = max_accuracy[0]\n",
    "print(max_accuracy, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113 samples, validate on 29 samples\n",
      "Epoch 1/100\n",
      "113/113 [==============================] - 2s 13ms/step - loss: 1.1139 - acc: 0.3894 - val_loss: 1.1106 - val_acc: 0.3448\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.1048 - acc: 0.3628 - val_loss: 1.1020 - val_acc: 0.3448\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0962 - acc: 0.3628 - val_loss: 1.0947 - val_acc: 0.3448\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 1.0903 - acc: 0.3628 - val_loss: 1.0901 - val_acc: 0.3448\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0863 - acc: 0.3628 - val_loss: 1.0866 - val_acc: 0.3448\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0829 - acc: 0.3628 - val_loss: 1.0832 - val_acc: 0.3448\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0800 - acc: 0.3628 - val_loss: 1.0803 - val_acc: 0.3448\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 1.0776 - acc: 0.3628 - val_loss: 1.0771 - val_acc: 0.3448\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 0s 106us/step - loss: 1.0745 - acc: 0.3628 - val_loss: 1.0743 - val_acc: 0.3448\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 1.0716 - acc: 0.3628 - val_loss: 1.0714 - val_acc: 0.3448\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0688 - acc: 0.3628 - val_loss: 1.0686 - val_acc: 0.3448\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0668 - acc: 0.3628 - val_loss: 1.0658 - val_acc: 0.3448\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 1.0639 - acc: 0.3894 - val_loss: 1.0631 - val_acc: 0.3448\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0614 - acc: 0.3982 - val_loss: 1.0600 - val_acc: 0.3448\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0587 - acc: 0.3982 - val_loss: 1.0566 - val_acc: 0.3448\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0562 - acc: 0.4071 - val_loss: 1.0530 - val_acc: 0.3448\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0541 - acc: 0.4071 - val_loss: 1.0492 - val_acc: 0.3448\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 1.0515 - acc: 0.4159 - val_loss: 1.0457 - val_acc: 0.3793\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 1.0486 - acc: 0.4159 - val_loss: 1.0417 - val_acc: 0.4138\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 1.0462 - acc: 0.4159 - val_loss: 1.0377 - val_acc: 0.4138\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0431 - acc: 0.3894 - val_loss: 1.0340 - val_acc: 0.4138\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 1.0401 - acc: 0.3982 - val_loss: 1.0296 - val_acc: 0.4138\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 1.0370 - acc: 0.3982 - val_loss: 1.0244 - val_acc: 0.4138\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0335 - acc: 0.4159 - val_loss: 1.0188 - val_acc: 0.4138\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 1.0302 - acc: 0.4159 - val_loss: 1.0135 - val_acc: 0.4138\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 1.0261 - acc: 0.4248 - val_loss: 1.0079 - val_acc: 0.4138\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0224 - acc: 0.4336 - val_loss: 1.0015 - val_acc: 0.4483\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 1.0181 - acc: 0.4336 - val_loss: 0.9957 - val_acc: 0.4138\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0145 - acc: 0.4425 - val_loss: 0.9894 - val_acc: 0.4138\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0102 - acc: 0.4425 - val_loss: 0.9834 - val_acc: 0.4138\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0071 - acc: 0.4602 - val_loss: 0.9780 - val_acc: 0.4138\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 1.0026 - acc: 0.4602 - val_loss: 0.9723 - val_acc: 0.4138\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9986 - acc: 0.4602 - val_loss: 0.9662 - val_acc: 0.4828\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9945 - acc: 0.4425 - val_loss: 0.9604 - val_acc: 0.4828\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 0.9906 - acc: 0.4602 - val_loss: 0.9536 - val_acc: 0.4828\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9863 - acc: 0.4602 - val_loss: 0.9481 - val_acc: 0.4828\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 0.9827 - acc: 0.4602 - val_loss: 0.9423 - val_acc: 0.4828\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9792 - acc: 0.4602 - val_loss: 0.9368 - val_acc: 0.4828\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 0s 106us/step - loss: 0.9756 - acc: 0.4602 - val_loss: 0.9317 - val_acc: 0.4828\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9727 - acc: 0.4690 - val_loss: 0.9265 - val_acc: 0.4828\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9695 - acc: 0.4690 - val_loss: 0.9217 - val_acc: 0.4828\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9659 - acc: 0.4602 - val_loss: 0.9180 - val_acc: 0.4828\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9628 - acc: 0.4779 - val_loss: 0.9149 - val_acc: 0.4828\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9601 - acc: 0.4867 - val_loss: 0.9112 - val_acc: 0.4828\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9571 - acc: 0.4779 - val_loss: 0.9078 - val_acc: 0.5172\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9543 - acc: 0.4867 - val_loss: 0.9049 - val_acc: 0.5172\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9522 - acc: 0.5133 - val_loss: 0.9015 - val_acc: 0.5517\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9492 - acc: 0.5133 - val_loss: 0.8988 - val_acc: 0.5517\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9473 - acc: 0.5133 - val_loss: 0.8957 - val_acc: 0.5517\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 0.9443 - acc: 0.5221 - val_loss: 0.8940 - val_acc: 0.5517\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9419 - acc: 0.5221 - val_loss: 0.8923 - val_acc: 0.5517\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9398 - acc: 0.5221 - val_loss: 0.8902 - val_acc: 0.5517\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9374 - acc: 0.5133 - val_loss: 0.8886 - val_acc: 0.5517\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9350 - acc: 0.5221 - val_loss: 0.8870 - val_acc: 0.5862\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9333 - acc: 0.5310 - val_loss: 0.8857 - val_acc: 0.5862\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9316 - acc: 0.5398 - val_loss: 0.8837 - val_acc: 0.5862\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9294 - acc: 0.5398 - val_loss: 0.8821 - val_acc: 0.5862\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 0.9276 - acc: 0.5487 - val_loss: 0.8807 - val_acc: 0.5862\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9259 - acc: 0.5487 - val_loss: 0.8798 - val_acc: 0.5862\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9249 - acc: 0.5487 - val_loss: 0.8793 - val_acc: 0.5862\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 106us/step - loss: 0.9226 - acc: 0.5398 - val_loss: 0.8780 - val_acc: 0.5862\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9209 - acc: 0.5487 - val_loss: 0.8765 - val_acc: 0.5862\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9198 - acc: 0.5487 - val_loss: 0.8758 - val_acc: 0.5862\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9184 - acc: 0.5575 - val_loss: 0.8739 - val_acc: 0.5862\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9166 - acc: 0.5664 - val_loss: 0.8726 - val_acc: 0.6207\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9152 - acc: 0.5752 - val_loss: 0.8716 - val_acc: 0.6207\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 0.9141 - acc: 0.5664 - val_loss: 0.8706 - val_acc: 0.6552\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9129 - acc: 0.5664 - val_loss: 0.8700 - val_acc: 0.6552\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9119 - acc: 0.5929 - val_loss: 0.8700 - val_acc: 0.6897\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9107 - acc: 0.6106 - val_loss: 0.8693 - val_acc: 0.7241\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9098 - acc: 0.6637 - val_loss: 0.8688 - val_acc: 0.7241\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9085 - acc: 0.7345 - val_loss: 0.8683 - val_acc: 0.7586\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9075 - acc: 0.7434 - val_loss: 0.8680 - val_acc: 0.7586\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9064 - acc: 0.7345 - val_loss: 0.8675 - val_acc: 0.7586\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9050 - acc: 0.7434 - val_loss: 0.8668 - val_acc: 0.7586\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9040 - acc: 0.7611 - val_loss: 0.8659 - val_acc: 0.7586\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9030 - acc: 0.7522 - val_loss: 0.8654 - val_acc: 0.7586\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9021 - acc: 0.7522 - val_loss: 0.8647 - val_acc: 0.7586\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9012 - acc: 0.7522 - val_loss: 0.8632 - val_acc: 0.7931\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8996 - acc: 0.7699 - val_loss: 0.8623 - val_acc: 0.7931\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.8985 - acc: 0.7699 - val_loss: 0.8619 - val_acc: 0.7586\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.8969 - acc: 0.7611 - val_loss: 0.8620 - val_acc: 0.7586\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8959 - acc: 0.7080 - val_loss: 0.8612 - val_acc: 0.7586\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8951 - acc: 0.6903 - val_loss: 0.8606 - val_acc: 0.6897\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.8940 - acc: 0.6903 - val_loss: 0.8596 - val_acc: 0.7241\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8931 - acc: 0.6903 - val_loss: 0.8588 - val_acc: 0.7241\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8916 - acc: 0.6903 - val_loss: 0.8581 - val_acc: 0.7586\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8901 - acc: 0.6903 - val_loss: 0.8569 - val_acc: 0.7586\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.8889 - acc: 0.7168 - val_loss: 0.8561 - val_acc: 0.7586\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8875 - acc: 0.7699 - val_loss: 0.8550 - val_acc: 0.7931\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8866 - acc: 0.7699 - val_loss: 0.8540 - val_acc: 0.7931\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 0.8848 - acc: 0.7611 - val_loss: 0.8538 - val_acc: 0.7931\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.8836 - acc: 0.7611 - val_loss: 0.8532 - val_acc: 0.7931\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.8823 - acc: 0.7788 - val_loss: 0.8532 - val_acc: 0.7931\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8804 - acc: 0.7788 - val_loss: 0.8529 - val_acc: 0.7931\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.8796 - acc: 0.7788 - val_loss: 0.8527 - val_acc: 0.7586\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.8778 - acc: 0.7788 - val_loss: 0.8514 - val_acc: 0.7586\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8764 - acc: 0.7788 - val_loss: 0.8499 - val_acc: 0.7931\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8743 - acc: 0.7699 - val_loss: 0.8483 - val_acc: 0.7931\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.8735 - acc: 0.7611 - val_loss: 0.8473 - val_acc: 0.8276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3893805317646634,\n",
       " 0.36283185919829175,\n",
       " 0.3628318602532412,\n",
       " 0.3628318597257665,\n",
       " 0.3628318602532412,\n",
       " 0.362831858670817,\n",
       " 0.3628318602532412,\n",
       " 0.3628318602532412,\n",
       " 0.36283185906642307,\n",
       " 0.3628318597257665,\n",
       " 0.3628318597257665,\n",
       " 0.36283185853894834,\n",
       " 0.3893805328196129,\n",
       " 0.39823008915491864,\n",
       " 0.3982300903417368,\n",
       " 0.40707964733638596,\n",
       " 0.407079650237497,\n",
       " 0.4159292048585098,\n",
       " 0.4159292048585098,\n",
       " 0.41592920723214616,\n",
       " 0.38938053519324917,\n",
       " 0.3982300887593126,\n",
       " 0.3982300887593126,\n",
       " 0.4159292048585098,\n",
       " 0.4159292043310351,\n",
       " 0.42477876290810845,\n",
       " 0.4336283187159395,\n",
       " 0.4336283222763939,\n",
       " 0.44247787742488154,\n",
       " 0.44247787742488154,\n",
       " 0.46017699194165457,\n",
       " 0.4601769924691293,\n",
       " 0.4601769924691293,\n",
       " 0.44247788032599256,\n",
       " 0.46017699141417984,\n",
       " 0.46017699128231115,\n",
       " 0.46017699194165457,\n",
       " 0.46017699194165457,\n",
       " 0.4601769953702403,\n",
       " 0.46902654893630374,\n",
       " 0.4690265528923642,\n",
       " 0.46017699194165457,\n",
       " 0.47787610645842765,\n",
       " 0.48672566450802623,\n",
       " 0.47787610645842765,\n",
       " 0.48672566450802623,\n",
       " 0.5132743383930848,\n",
       " 0.513274339975509,\n",
       " 0.513274337074398,\n",
       " 0.5221238969701582,\n",
       " 0.5221238980251076,\n",
       " 0.5221238945965219,\n",
       " 0.5132743376018727,\n",
       " 0.5221238980251076,\n",
       " 0.5309734539648073,\n",
       " 0.5398230125418807,\n",
       " 0.5398230125418807,\n",
       " 0.5486725700640045,\n",
       " 0.5486725705914792,\n",
       " 0.5486725666354187,\n",
       " 0.5398230125418807,\n",
       " 0.5486725676903682,\n",
       " 0.5486725695365298,\n",
       " 0.5575221260037042,\n",
       " 0.5663716845807776,\n",
       " 0.575221243157851,\n",
       " 0.5663716822071413,\n",
       " 0.5663716822071413,\n",
       " 0.592920357674624,\n",
       " 0.6106194721913971,\n",
       " 0.6637168167966657,\n",
       " 0.7345132780286063,\n",
       " 0.743362833968306,\n",
       " 0.7345132769736569,\n",
       " 0.743362833968306,\n",
       " 0.7610619490125538,\n",
       " 0.7522123909629552,\n",
       " 0.7522123909629552,\n",
       " 0.7522123925453794,\n",
       " 0.7699115054797282,\n",
       " 0.7699115065346777,\n",
       " 0.7610619490125538,\n",
       " 0.7079646044072851,\n",
       " 0.6902654893630373,\n",
       " 0.6902654893630373,\n",
       " 0.6902654888355626,\n",
       " 0.6902654893630373,\n",
       " 0.6902654883080879,\n",
       " 0.716814161929409,\n",
       " 0.7699115054797282,\n",
       " 0.7699115065346777,\n",
       " 0.7610619474301296,\n",
       " 0.7610619495400285,\n",
       " 0.7787610619469026,\n",
       " 0.7787610635293268,\n",
       " 0.7787610645842763,\n",
       " 0.7787610635293268,\n",
       " 0.7787610630018521,\n",
       " 0.7699115054797282,\n",
       " 0.7610619495400285]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN model code\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(20, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(20, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(3, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "abcd = model.fit(trainingData, targetData, epochs=100, validation_split=0.2)\n",
    "\n",
    "abcd.history[\"acc\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = plt.figure()\n",
    "plt.plot(x, accuracy, \"-\", marker=\"None\")\n",
    "\n",
    "\n",
    "\n",
    "max_accuracy = max(accuracy)\n",
    "acc = max_accuracy\n",
    "max_accuracy = [i for i, j in enumerate(accuracy) if j == max_accuracy]\n",
    "max_accuracy = max_accuracy[0]\n",
    "print(max_accuracy, acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
