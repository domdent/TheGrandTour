{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dom\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sklearn\n",
    "\n",
    "\n",
    "readdata = pd.read_csv(\"C:/Users/Dom/MPhys/TheGrandTour/wine_data.txt\", sep=\"\\t\", header=None);\n",
    "data = np.array(readdata);\n",
    "data = np.delete(data, 0, 0)\n",
    "data = data.astype(float)\n",
    "data = np.swapaxes(data,0,1)\n",
    "\n",
    "\n",
    "# Need to seperate the classification dimension:\n",
    "classification = data[13]\n",
    "data = np.delete(data, 13, axis=0)\n",
    "\n",
    "\n",
    "# make list of colours for each number:\n",
    "data_colour = []\n",
    "for i in range(len(classification)):\n",
    "    if classification[i] == 1:\n",
    "        data_colour.append(\"r\")\n",
    "    elif classification[i] == 2:\n",
    "        data_colour.append(\"b\")\n",
    "    elif classification[i] == 3:\n",
    "        data_colour.append(\"g\")\n",
    "        \n",
    "# Normalizes the data        \n",
    "for i in range(0, np.shape(data)[0]):\n",
    "    data[i,:] = (data[i,:] / np.ndarray.max(data[i,:])) * 2 - 1\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "#VARIABLES\n",
    "stepSize = 0.01\n",
    "nSteps = 10000\n",
    "\n",
    "def getAlpha(d):\n",
    "    \"\"\"\n",
    "    NEEDS IMPLEMENTATION\n",
    "    Should produce 1xd(d-1)/2 array of position in grand tour.\n",
    "    \"\"\"\n",
    "    p = d*(d-1)/2     \n",
    "    primeList = []\n",
    "    count = 1\n",
    "    while len(primeList) < p:\n",
    "        count += 1\n",
    "        primeBool = False\n",
    "        for i in range(2, count - 1):\n",
    "            if count % i == 0:\n",
    "                primeBool = True\n",
    "        if primeBool == False:\n",
    "            irrational = (np.sqrt(count)%1)\n",
    "            primeList.append(irrational)\n",
    "            \n",
    "    primeList = np.asarray(primeList)\n",
    "    primeList = primeList.dot(stepSize)\n",
    "    \"\"\"\n",
    "    Irrational number generation using exponentials, not being used\n",
    "    p = int(d*(d-1)/2)\n",
    "    alpha = np.zeros(p) #alpha(t) parameters defining grand tour in G2,d\n",
    "    for i in range(0,p):\n",
    "        alpha[i] = (np.exp(i) % 1) * 2 * np.pi\n",
    "        \n",
    "    alpha = alpha.dot(0.001)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return primeList\n",
    "\n",
    "\n",
    "def getAngles(alpha,d):\n",
    "    \"\"\"\"\"\n",
    "    Inputs: \n",
    "    alpha = 1xd(d-1)/2 array defining position on grand tour\n",
    "    d = dimensions of data\n",
    "    Outputs a dxd array of angles required for the transformation\n",
    "    \"\"\"\n",
    "    theta = np.zeros((d,d));\n",
    "    i = 0;\n",
    "    k = 0;\n",
    "    \n",
    "    while i < d-1:\n",
    "        j = i + 1;\n",
    "        \n",
    "        while j < d:\n",
    "            theta[i][j] = alpha[k];\n",
    "            j += 1;\n",
    "            k += 1;\n",
    "    \n",
    "        i+= 1;\n",
    "        \n",
    "    return theta;\n",
    "\n",
    "\n",
    "def RotationMatrix(i, j, d, theta):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    i = first indicie of rotating plane\n",
    "    j = second indicie of rotating plane\n",
    "    d = dimensions of data\n",
    "    theta = dxd array of angle of rotation of rotating plane\n",
    "    Outputs a rotating matrix to rotate plane of ixj plane by theta_ij\n",
    "    \"\"\"\n",
    "    R = np.identity(d)\n",
    "    R[i,i] = np.cos(theta)\n",
    "    R[i,j] = -1*np.sin(theta)\n",
    "    R[j,i] = np.sin(theta)\n",
    "    R[j,j] = np.cos(theta)\n",
    "    return R\n",
    "\n",
    "\n",
    "def BetaFn(d, theta):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    d = dimensions of data\n",
    "    theta = dxd array of angle of rotation ixj plane\n",
    "    Outputs the full matrix transformation for all rotations\n",
    "    \"\"\"\n",
    "    b = RotationMatrix(1, 2, d, theta[1,2])\n",
    "    i = 1\n",
    "    j = 2\n",
    "    for i in range(d):\n",
    "        for j in range(d):\n",
    "            if j <= i:\n",
    "                continue\n",
    "            if i==1 and j==2:\n",
    "                continue\n",
    "            b = np.matmul(b, RotationMatrix(i, j, d, theta[i,j]))\n",
    "            \n",
    "    return b\n",
    "\n",
    "\n",
    "def GrandTour(data, nSteps):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    data = array of data points, dimensions x npoints\n",
    "    Outputs a 3D array number of points x t x dimensions, where t\n",
    "    the time step at that point in the tour\n",
    "    \"\"\"\n",
    "\n",
    "    d = np.shape(data)[0] #dimensions of data\n",
    "    nPoints = np.shape(data)[1] #number of data points\n",
    "    tData = np.zeros((nSteps,d,nPoints)) #initialise 3d matrix to store stransforemd data at each timestep\n",
    "    tBeta = np.zeros((nSteps,d,d))\n",
    "    Alpha = getAlpha(d)\n",
    "\n",
    "    \n",
    "    for t in range(0, nSteps):\n",
    "        \n",
    "        \n",
    "        alpha = Alpha.dot(t)\n",
    "        theta = getAngles(alpha, d)\n",
    "        b = BetaFn(d, theta)\n",
    "        a = np.matmul(b, data)\n",
    "        tData[t,:,:] = a\n",
    "        tBeta[t,:,:] = b\n",
    "        \n",
    "    return tData, tBeta\n",
    "\n",
    "\n",
    "tData, tBeta = GrandTour(data, nSteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetData = np.zeros((len(tData[0][0]), 3))\n",
    "for counter, i in enumerate(classification):\n",
    "    targetData[counter][int(i-1)] = 1\n",
    "targetData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 5s 34ms/step - loss: 1.2810 - acc: 0.0845 - val_loss: 0.9149 - val_acc: 0.9444\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 162us/step - loss: 1.2348 - acc: 0.0986 - val_loss: 0.9661 - val_acc: 0.8333\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 5s 34ms/step - loss: 1.1017 - acc: 0.3662 - val_loss: 1.0368 - val_acc: 0.9722\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 148us/step - loss: 1.0887 - acc: 0.4789 - val_loss: 1.0479 - val_acc: 0.9722\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 5s 34ms/step - loss: 1.1160 - acc: 0.2817 - val_loss: 1.0326 - val_acc: 0.9167\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 190us/step - loss: 1.0972 - acc: 0.4366 - val_loss: 1.0341 - val_acc: 0.8333\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 5s 35ms/step - loss: 1.0451 - acc: 0.5775 - val_loss: 1.1148 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 204us/step - loss: 1.0266 - acc: 0.5915 - val_loss: 1.1217 - val_acc: 0.0000e+00\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 5s 34ms/step - loss: 1.1094 - acc: 0.2535 - val_loss: 1.0555 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 169us/step - loss: 1.0895 - acc: 0.5211 - val_loss: 1.0469 - val_acc: 0.0278\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 5s 35ms/step - loss: 0.9935 - acc: 0.6408 - val_loss: 1.0816 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 155us/step - loss: 0.9730 - acc: 0.6549 - val_loss: 1.0893 - val_acc: 0.0000e+00\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 5s 35ms/step - loss: 1.1015 - acc: 0.2746 - val_loss: 1.0981 - val_acc: 0.1667\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 155us/step - loss: 1.0773 - acc: 0.4366 - val_loss: 1.1172 - val_acc: 0.0833\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 5s 35ms/step - loss: 1.0667 - acc: 0.4789 - val_loss: 1.2171 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 162us/step - loss: 1.0534 - acc: 0.5000 - val_loss: 1.2530 - val_acc: 0.0000e+00\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 5s 36ms/step - loss: 1.0871 - acc: 0.5000 - val_loss: 1.2797 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 169us/step - loss: 1.0701 - acc: 0.5000 - val_loss: 1.3042 - val_acc: 0.0000e+00\n",
      "Train on 142 samples, validate on 36 samples\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 5s 36ms/step - loss: 1.0706 - acc: 0.6056 - val_loss: 1.0899 - val_acc: 0.2222\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 0s 169us/step - loss: 1.0488 - acc: 0.6056 - val_loss: 1.1110 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "accuracy = []\n",
    "start = time.clock\n",
    "for i in range(999):\n",
    "    if i % 10 == 0:\n",
    "\n",
    "        xData = tData[i][0]\n",
    "        yData = tData[i][1]\n",
    "        trainingData = np.vstack((xData, yData)).T\n",
    "        # NN model code\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(20, activation=tf.nn.relu))\n",
    "        model.add(tf.keras.layers.Dense(20, activation=tf.nn.relu))\n",
    "        model.add(tf.keras.layers.Dense(3, activation=tf.nn.softmax))\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        nnet = model.fit(trainingData, targetData, epochs=2, validation_split=0.2)\n",
    "        accuracy.append(np.mean(nnet.history[\"acc\"]))\n",
    "    \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0915492961943989,\n",
       " 0.4225352129466097,\n",
       " 0.3591549302070913,\n",
       " 0.5845070426732721,\n",
       " 0.3873239424027188,\n",
       " 0.647887322264658,\n",
       " 0.3556338032366524,\n",
       " 0.4894366201380609,\n",
       " 0.5000000014691286,\n",
       " 0.6056338049156565]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.647887322264658\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ba8407fd68>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = list(range(0, len(accuracy)))\n",
    "\n",
    "new = plt.figure()\n",
    "plt.plot(x, accuracy, \"-\", marker=\"None\")\n",
    "\n",
    "max_accuracy = max(accuracy)\n",
    "acc = max_accuracy\n",
    "max_accuracy = [i for i, j in enumerate(accuracy) if j == max_accuracy]\n",
    "max_accuracy = max_accuracy[0]\n",
    "print(max_accuracy, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113 samples, validate on 29 samples\n",
      "Epoch 1/100\n",
      "113/113 [==============================] - 2s 13ms/step - loss: 1.1139 - acc: 0.3894 - val_loss: 1.1106 - val_acc: 0.3448\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.1048 - acc: 0.3628 - val_loss: 1.1020 - val_acc: 0.3448\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0962 - acc: 0.3628 - val_loss: 1.0947 - val_acc: 0.3448\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 1.0903 - acc: 0.3628 - val_loss: 1.0901 - val_acc: 0.3448\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0863 - acc: 0.3628 - val_loss: 1.0866 - val_acc: 0.3448\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0829 - acc: 0.3628 - val_loss: 1.0832 - val_acc: 0.3448\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0800 - acc: 0.3628 - val_loss: 1.0803 - val_acc: 0.3448\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 1.0776 - acc: 0.3628 - val_loss: 1.0771 - val_acc: 0.3448\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 0s 106us/step - loss: 1.0745 - acc: 0.3628 - val_loss: 1.0743 - val_acc: 0.3448\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 1.0716 - acc: 0.3628 - val_loss: 1.0714 - val_acc: 0.3448\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0688 - acc: 0.3628 - val_loss: 1.0686 - val_acc: 0.3448\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0668 - acc: 0.3628 - val_loss: 1.0658 - val_acc: 0.3448\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 1.0639 - acc: 0.3894 - val_loss: 1.0631 - val_acc: 0.3448\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0614 - acc: 0.3982 - val_loss: 1.0600 - val_acc: 0.3448\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0587 - acc: 0.3982 - val_loss: 1.0566 - val_acc: 0.3448\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0562 - acc: 0.4071 - val_loss: 1.0530 - val_acc: 0.3448\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0541 - acc: 0.4071 - val_loss: 1.0492 - val_acc: 0.3448\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 1.0515 - acc: 0.4159 - val_loss: 1.0457 - val_acc: 0.3793\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 1.0486 - acc: 0.4159 - val_loss: 1.0417 - val_acc: 0.4138\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 1.0462 - acc: 0.4159 - val_loss: 1.0377 - val_acc: 0.4138\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0431 - acc: 0.3894 - val_loss: 1.0340 - val_acc: 0.4138\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 1.0401 - acc: 0.3982 - val_loss: 1.0296 - val_acc: 0.4138\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 1.0370 - acc: 0.3982 - val_loss: 1.0244 - val_acc: 0.4138\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0335 - acc: 0.4159 - val_loss: 1.0188 - val_acc: 0.4138\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 1.0302 - acc: 0.4159 - val_loss: 1.0135 - val_acc: 0.4138\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 1.0261 - acc: 0.4248 - val_loss: 1.0079 - val_acc: 0.4138\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0224 - acc: 0.4336 - val_loss: 1.0015 - val_acc: 0.4483\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 1.0181 - acc: 0.4336 - val_loss: 0.9957 - val_acc: 0.4138\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0145 - acc: 0.4425 - val_loss: 0.9894 - val_acc: 0.4138\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0102 - acc: 0.4425 - val_loss: 0.9834 - val_acc: 0.4138\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 1.0071 - acc: 0.4602 - val_loss: 0.9780 - val_acc: 0.4138\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 1.0026 - acc: 0.4602 - val_loss: 0.9723 - val_acc: 0.4138\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9986 - acc: 0.4602 - val_loss: 0.9662 - val_acc: 0.4828\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9945 - acc: 0.4425 - val_loss: 0.9604 - val_acc: 0.4828\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 0.9906 - acc: 0.4602 - val_loss: 0.9536 - val_acc: 0.4828\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9863 - acc: 0.4602 - val_loss: 0.9481 - val_acc: 0.4828\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 0.9827 - acc: 0.4602 - val_loss: 0.9423 - val_acc: 0.4828\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9792 - acc: 0.4602 - val_loss: 0.9368 - val_acc: 0.4828\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 0s 106us/step - loss: 0.9756 - acc: 0.4602 - val_loss: 0.9317 - val_acc: 0.4828\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9727 - acc: 0.4690 - val_loss: 0.9265 - val_acc: 0.4828\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9695 - acc: 0.4690 - val_loss: 0.9217 - val_acc: 0.4828\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9659 - acc: 0.4602 - val_loss: 0.9180 - val_acc: 0.4828\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9628 - acc: 0.4779 - val_loss: 0.9149 - val_acc: 0.4828\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9601 - acc: 0.4867 - val_loss: 0.9112 - val_acc: 0.4828\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9571 - acc: 0.4779 - val_loss: 0.9078 - val_acc: 0.5172\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9543 - acc: 0.4867 - val_loss: 0.9049 - val_acc: 0.5172\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9522 - acc: 0.5133 - val_loss: 0.9015 - val_acc: 0.5517\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9492 - acc: 0.5133 - val_loss: 0.8988 - val_acc: 0.5517\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9473 - acc: 0.5133 - val_loss: 0.8957 - val_acc: 0.5517\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 0.9443 - acc: 0.5221 - val_loss: 0.8940 - val_acc: 0.5517\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9419 - acc: 0.5221 - val_loss: 0.8923 - val_acc: 0.5517\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9398 - acc: 0.5221 - val_loss: 0.8902 - val_acc: 0.5517\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9374 - acc: 0.5133 - val_loss: 0.8886 - val_acc: 0.5517\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9350 - acc: 0.5221 - val_loss: 0.8870 - val_acc: 0.5862\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9333 - acc: 0.5310 - val_loss: 0.8857 - val_acc: 0.5862\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9316 - acc: 0.5398 - val_loss: 0.8837 - val_acc: 0.5862\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9294 - acc: 0.5398 - val_loss: 0.8821 - val_acc: 0.5862\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 0.9276 - acc: 0.5487 - val_loss: 0.8807 - val_acc: 0.5862\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9259 - acc: 0.5487 - val_loss: 0.8798 - val_acc: 0.5862\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9249 - acc: 0.5487 - val_loss: 0.8793 - val_acc: 0.5862\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 106us/step - loss: 0.9226 - acc: 0.5398 - val_loss: 0.8780 - val_acc: 0.5862\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9209 - acc: 0.5487 - val_loss: 0.8765 - val_acc: 0.5862\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9198 - acc: 0.5487 - val_loss: 0.8758 - val_acc: 0.5862\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9184 - acc: 0.5575 - val_loss: 0.8739 - val_acc: 0.5862\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9166 - acc: 0.5664 - val_loss: 0.8726 - val_acc: 0.6207\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9152 - acc: 0.5752 - val_loss: 0.8716 - val_acc: 0.6207\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 0.9141 - acc: 0.5664 - val_loss: 0.8706 - val_acc: 0.6552\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9129 - acc: 0.5664 - val_loss: 0.8700 - val_acc: 0.6552\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9119 - acc: 0.5929 - val_loss: 0.8700 - val_acc: 0.6897\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9107 - acc: 0.6106 - val_loss: 0.8693 - val_acc: 0.7241\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9098 - acc: 0.6637 - val_loss: 0.8688 - val_acc: 0.7241\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9085 - acc: 0.7345 - val_loss: 0.8683 - val_acc: 0.7586\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9075 - acc: 0.7434 - val_loss: 0.8680 - val_acc: 0.7586\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9064 - acc: 0.7345 - val_loss: 0.8675 - val_acc: 0.7586\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9050 - acc: 0.7434 - val_loss: 0.8668 - val_acc: 0.7586\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9040 - acc: 0.7611 - val_loss: 0.8659 - val_acc: 0.7586\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.9030 - acc: 0.7522 - val_loss: 0.8654 - val_acc: 0.7586\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.9021 - acc: 0.7522 - val_loss: 0.8647 - val_acc: 0.7586\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.9012 - acc: 0.7522 - val_loss: 0.8632 - val_acc: 0.7931\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8996 - acc: 0.7699 - val_loss: 0.8623 - val_acc: 0.7931\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.8985 - acc: 0.7699 - val_loss: 0.8619 - val_acc: 0.7586\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.8969 - acc: 0.7611 - val_loss: 0.8620 - val_acc: 0.7586\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8959 - acc: 0.7080 - val_loss: 0.8612 - val_acc: 0.7586\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8951 - acc: 0.6903 - val_loss: 0.8606 - val_acc: 0.6897\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.8940 - acc: 0.6903 - val_loss: 0.8596 - val_acc: 0.7241\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8931 - acc: 0.6903 - val_loss: 0.8588 - val_acc: 0.7241\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8916 - acc: 0.6903 - val_loss: 0.8581 - val_acc: 0.7586\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8901 - acc: 0.6903 - val_loss: 0.8569 - val_acc: 0.7586\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.8889 - acc: 0.7168 - val_loss: 0.8561 - val_acc: 0.7586\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8875 - acc: 0.7699 - val_loss: 0.8550 - val_acc: 0.7931\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8866 - acc: 0.7699 - val_loss: 0.8540 - val_acc: 0.7931\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 0s 71us/step - loss: 0.8848 - acc: 0.7611 - val_loss: 0.8538 - val_acc: 0.7931\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.8836 - acc: 0.7611 - val_loss: 0.8532 - val_acc: 0.7931\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.8823 - acc: 0.7788 - val_loss: 0.8532 - val_acc: 0.7931\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8804 - acc: 0.7788 - val_loss: 0.8529 - val_acc: 0.7931\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.8796 - acc: 0.7788 - val_loss: 0.8527 - val_acc: 0.7586\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 0s 88us/step - loss: 0.8778 - acc: 0.7788 - val_loss: 0.8514 - val_acc: 0.7586\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8764 - acc: 0.7788 - val_loss: 0.8499 - val_acc: 0.7931\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 0s 79us/step - loss: 0.8743 - acc: 0.7699 - val_loss: 0.8483 - val_acc: 0.7931\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 0s 97us/step - loss: 0.8735 - acc: 0.7611 - val_loss: 0.8473 - val_acc: 0.8276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3893805317646634,\n",
       " 0.36283185919829175,\n",
       " 0.3628318602532412,\n",
       " 0.3628318597257665,\n",
       " 0.3628318602532412,\n",
       " 0.362831858670817,\n",
       " 0.3628318602532412,\n",
       " 0.3628318602532412,\n",
       " 0.36283185906642307,\n",
       " 0.3628318597257665,\n",
       " 0.3628318597257665,\n",
       " 0.36283185853894834,\n",
       " 0.3893805328196129,\n",
       " 0.39823008915491864,\n",
       " 0.3982300903417368,\n",
       " 0.40707964733638596,\n",
       " 0.407079650237497,\n",
       " 0.4159292048585098,\n",
       " 0.4159292048585098,\n",
       " 0.41592920723214616,\n",
       " 0.38938053519324917,\n",
       " 0.3982300887593126,\n",
       " 0.3982300887593126,\n",
       " 0.4159292048585098,\n",
       " 0.4159292043310351,\n",
       " 0.42477876290810845,\n",
       " 0.4336283187159395,\n",
       " 0.4336283222763939,\n",
       " 0.44247787742488154,\n",
       " 0.44247787742488154,\n",
       " 0.46017699194165457,\n",
       " 0.4601769924691293,\n",
       " 0.4601769924691293,\n",
       " 0.44247788032599256,\n",
       " 0.46017699141417984,\n",
       " 0.46017699128231115,\n",
       " 0.46017699194165457,\n",
       " 0.46017699194165457,\n",
       " 0.4601769953702403,\n",
       " 0.46902654893630374,\n",
       " 0.4690265528923642,\n",
       " 0.46017699194165457,\n",
       " 0.47787610645842765,\n",
       " 0.48672566450802623,\n",
       " 0.47787610645842765,\n",
       " 0.48672566450802623,\n",
       " 0.5132743383930848,\n",
       " 0.513274339975509,\n",
       " 0.513274337074398,\n",
       " 0.5221238969701582,\n",
       " 0.5221238980251076,\n",
       " 0.5221238945965219,\n",
       " 0.5132743376018727,\n",
       " 0.5221238980251076,\n",
       " 0.5309734539648073,\n",
       " 0.5398230125418807,\n",
       " 0.5398230125418807,\n",
       " 0.5486725700640045,\n",
       " 0.5486725705914792,\n",
       " 0.5486725666354187,\n",
       " 0.5398230125418807,\n",
       " 0.5486725676903682,\n",
       " 0.5486725695365298,\n",
       " 0.5575221260037042,\n",
       " 0.5663716845807776,\n",
       " 0.575221243157851,\n",
       " 0.5663716822071413,\n",
       " 0.5663716822071413,\n",
       " 0.592920357674624,\n",
       " 0.6106194721913971,\n",
       " 0.6637168167966657,\n",
       " 0.7345132780286063,\n",
       " 0.743362833968306,\n",
       " 0.7345132769736569,\n",
       " 0.743362833968306,\n",
       " 0.7610619490125538,\n",
       " 0.7522123909629552,\n",
       " 0.7522123909629552,\n",
       " 0.7522123925453794,\n",
       " 0.7699115054797282,\n",
       " 0.7699115065346777,\n",
       " 0.7610619490125538,\n",
       " 0.7079646044072851,\n",
       " 0.6902654893630373,\n",
       " 0.6902654893630373,\n",
       " 0.6902654888355626,\n",
       " 0.6902654893630373,\n",
       " 0.6902654883080879,\n",
       " 0.716814161929409,\n",
       " 0.7699115054797282,\n",
       " 0.7699115065346777,\n",
       " 0.7610619474301296,\n",
       " 0.7610619495400285,\n",
       " 0.7787610619469026,\n",
       " 0.7787610635293268,\n",
       " 0.7787610645842763,\n",
       " 0.7787610635293268,\n",
       " 0.7787610630018521,\n",
       " 0.7699115054797282,\n",
       " 0.7610619495400285]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN model code\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(20, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(20, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(3, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "abcd = model.fit(trainingData, targetData, epochs=100, validation_split=0.2)\n",
    "\n",
    "abcd.history[\"acc\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = plt.figure()\n",
    "plt.plot(x, accuracy, \"-\", marker=\"None\")\n",
    "\n",
    "\n",
    "\n",
    "max_accuracy = max(accuracy)\n",
    "acc = max_accuracy\n",
    "max_accuracy = [i for i, j in enumerate(accuracy) if j == max_accuracy]\n",
    "max_accuracy = max_accuracy[0]\n",
    "print(max_accuracy, acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
