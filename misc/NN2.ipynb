{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/dom/Documents/MPhys/TheGrandTour/wine_data.txt\", sep=\"\\t\");\n",
    "data = np.array(df);\n",
    "data = np.delete(data, 0, 0)\n",
    "data = data.astype(float)\n",
    "data = np.swapaxes(data,0,1)\n",
    "\n",
    "\n",
    "classification = data[13]\n",
    "data = np.delete(data, 13, axis=0)\n",
    "# Normalizes the data        \n",
    "for i in range(0, np.shape(data)[0]):\n",
    "    data[i,:] = (data[i,:] / np.ndarray.max(data[i,:])) * 2 - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepSize = 0.01\n",
    "nSteps = 10000\n",
    "\n",
    "def getAlpha(d):\n",
    "    \"\"\"\n",
    "    NEEDS IMPLEMENTATION\n",
    "    Should produce 1xd(d-1)/2 array of position in grand tour.\n",
    "    \"\"\"\n",
    "    p = d*(d-1)/2     \n",
    "    primeList = []\n",
    "    count = 1\n",
    "    while len(primeList) < p:\n",
    "        count += 1\n",
    "        primeBool = False\n",
    "        for i in range(2, count - 1):\n",
    "            if count % i == 0:\n",
    "                primeBool = True\n",
    "        if primeBool == False:\n",
    "            irrational = (np.sqrt(count)%1)\n",
    "            primeList.append(irrational)\n",
    "            \n",
    "    primeList = np.asarray(primeList)\n",
    "    primeList = primeList.dot(stepSize)\n",
    "    \"\"\"\n",
    "    Irrational number generation using exponentials, not being used\n",
    "    p = int(d*(d-1)/2)\n",
    "    alpha = np.zeros(p) #alpha(t) parameters defining grand tour in G2,d\n",
    "    for i in range(0,p):\n",
    "        alpha[i] = (np.exp(i) % 1) * 2 * np.pi\n",
    "        \n",
    "    alpha = alpha.dot(0.001)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return primeList\n",
    "\n",
    "\n",
    "def getAngles(alpha,d):\n",
    "    \"\"\"\"\"\n",
    "    Inputs: \n",
    "    alpha = 1xd(d-1)/2 array defining position on grand tour\n",
    "    d = dimensions of data\n",
    "    Outputs a dxd array of angles required for the transformation\n",
    "    \"\"\"\n",
    "    theta = np.zeros((d,d));\n",
    "    i = 0;\n",
    "    k = 0;\n",
    "    \n",
    "    while i < d-1:\n",
    "        j = i + 1;\n",
    "        \n",
    "        while j < d:\n",
    "            theta[i][j] = alpha[k];\n",
    "            j += 1;\n",
    "            k += 1;\n",
    "    \n",
    "        i+= 1;\n",
    "        \n",
    "    return theta;\n",
    "\n",
    "\n",
    "def RotationMatrix(i, j, d, theta):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    i = first indicie of rotating plane\n",
    "    j = second indicie of rotating plane\n",
    "    d = dimensions of data\n",
    "    theta = dxd array of angle of rotation of rotating plane\n",
    "    Outputs a rotating matrix to rotate plane of ixj plane by theta_ij\n",
    "    \"\"\"\n",
    "    R = np.identity(d)\n",
    "    R[i,i] = np.cos(theta)\n",
    "    R[i,j] = -1*np.sin(theta)\n",
    "    R[j,i] = np.sin(theta)\n",
    "    R[j,j] = np.cos(theta)\n",
    "    return R\n",
    "\n",
    "\n",
    "def BetaFn(d, theta):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    d = dimensions of data\n",
    "    theta = dxd array of angle of rotation ixj plane\n",
    "    Outputs the full matrix transformation for all rotations\n",
    "    \"\"\"\n",
    "    b = RotationMatrix(1, 2, d, theta[1,2])\n",
    "    i = 1\n",
    "    j = 2\n",
    "    for i in range(d):\n",
    "        for j in range(d):\n",
    "            if j <= i:\n",
    "                continue\n",
    "            if i==1 and j==2:\n",
    "                continue\n",
    "            b = np.matmul(b, RotationMatrix(i, j, d, theta[i,j]))\n",
    "            \n",
    "    return b\n",
    "\n",
    "\n",
    "def GrandTour(data, nSteps):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    data = array of data points, dimensions x npoints\n",
    "    Outputs a 3D array number of points x t x dimensions, where t\n",
    "    the time step at that point in the tour\n",
    "    \"\"\"\n",
    "\n",
    "    d = np.shape(data)[0] #dimensions of data\n",
    "    nPoints = np.shape(data)[1] #number of data points\n",
    "    tData = np.zeros((nSteps, d, nPoints)) #initialise 3d matrix to store stransforemd data at each timestep\n",
    "    tBeta = np.zeros((nSteps, d, d))\n",
    "    Alpha = getAlpha(d)\n",
    "\n",
    "    \n",
    "    for t in range(0, nSteps):\n",
    "        \n",
    "        \n",
    "        alpha = Alpha.dot(t)\n",
    "        theta = getAngles(alpha, d)\n",
    "        b = BetaFn(d, theta)\n",
    "        a = np.matmul(b, data)\n",
    "        tData[t,:,:] = a\n",
    "        tBeta[t,:,:] = b\n",
    "        \n",
    "    return tData, tBeta\n",
    "\n",
    "\n",
    "tData, tBeta = GrandTour(data, nSteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetData = np.zeros((len(tData[0][0]), 3))\n",
    "for counter, i in enumerate(classification):\n",
    "    targetData[counter][int(i-1)] = 1\n",
    "targetData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 177])\n",
      "torch.Size([177, 3])\n",
      "torch.Size([13, 177])\n",
      "torch.Size([177, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(data)\n",
    "y = torch.from_numpy(targetData)\n",
    "x = x.float()\n",
    "y = y.float()\n",
    "print(x.size())\n",
    "print(y.size())\n",
    "#x.transpose_(0, 1)\n",
    "print(x.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 177])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([177, 13])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.size())\n",
    "x.transpose_(0, 1)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([177, 13])\n",
      "torch.Size([177, 3])\n"
     ]
    }
   ],
   "source": [
    "#print(x[0])\n",
    "#print(x[1])\n",
    "print(x.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        # parameters\n",
    "        # TODO: parameters can be parameterized instead of declaring them here\n",
    "        self.inputSize = x.size()[1]\n",
    "        self.outputSize = y.size()[1]\n",
    "        self.hiddenSize = 20\n",
    "        \n",
    "        # weights with + 1 for bias nodes\n",
    "        self.W1 = torch.randn(self.inputSize + 1, self.hiddenSize)\n",
    "        self.W2 = torch.randn(self.hiddenSize + 1, self.outputSize) \n",
    "        print(self.W1.size())\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #print(X.size())\n",
    "        #print(self.W1.size())\n",
    "        X_w_bias = torch.cat((X, torch.ones(x.size()[0], 1)), 1)\n",
    "        #print(\"X_w_bias: \" + str(X_w_bias.size()))\n",
    "        self.z = torch.matmul(X_w_bias, self.W1) # 3 X 3 \".dot\" does not broadcast in PyTorch\n",
    "        self.z2 = self.sigmoid(self.z) # activation function\n",
    "        z2_w_bias = torch.cat((self.z2, torch.ones(self.z2.size()[0], 1)), 1)\n",
    "        #print(\"z2_w_bias: \" + str(z2_w_bias.size()))\n",
    "\n",
    "        self.z3 = torch.matmul(z2_w_bias, self.W2)\n",
    "        o = self.sigmoid(self.z3) # final activation function            \n",
    "        return o\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        # derivative of sigmoid\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error\n",
    "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
    "\n",
    "        z2_w_bias = torch.cat((self.z2, torch.ones(self.z2.size()[0], 1)), 1)\n",
    "\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(z2_w_bias)\n",
    "        \n",
    "        X_w_bias = torch.cat((X, torch.ones(x.size()[0], 1)), 1)\n",
    "        \n",
    "        self.z2_delta = self.z2_delta.narrow(1, 0, 20)\n",
    "\n",
    "        #print(self.z2_delta.size(), self.o_delta.size())\n",
    "        #print(\"self.z2_delta: \" + str(self.z2_delta.size()))\n",
    "        #print(\"torch.t(X).size(): \" + str(torch.t(X).size()))\n",
    "        #print(\"W1 size: \" + str(self.W1.size()))\n",
    "        self.W1 += torch.matmul(torch.t(X_w_bias), self.z2_delta)\n",
    "        self.W2 += torch.matmul(torch.t(z2_w_bias), self.o_delta)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        # forward + backward pass for training\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "        \n",
    "    def saveWeights(self, model):\n",
    "        # we will use the PyTorch internal storage functions\n",
    "        torch.save(model, \"NN\")\n",
    "        # you can reload model with all the weights and so forth with:\n",
    "        # torch.load(\"NN\")\n",
    "        \n",
    "    def predict(self, X):\n",
    "        print (\"Predicted data based on trained weights: \")\n",
    "        print (\"Input (scaled): \\n\" + str(X))\n",
    "        foward_fn = self.forward(X)\n",
    "        print (\"Output: \\n\" + str(foward_fn))\n",
    "        return forward_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 20])\n",
      "#0 Loss: 0.45687001943588257\n",
      "#1 Loss: 0.3333333432674408\n",
      "#2 Loss: 0.3333333432674408\n",
      "#3 Loss: 0.3333333432674408\n",
      "#4 Loss: 0.3333333432674408\n",
      "#5 Loss: 0.3333333432674408\n",
      "#6 Loss: 0.3333333432674408\n",
      "#7 Loss: 0.3333333432674408\n",
      "#8 Loss: 0.3333333432674408\n",
      "#9 Loss: 0.3333333432674408\n",
      "#10 Loss: 0.3333333432674408\n",
      "#11 Loss: 0.3333333432674408\n",
      "#12 Loss: 0.3333333432674408\n",
      "#13 Loss: 0.3333333432674408\n",
      "#14 Loss: 0.3333333432674408\n",
      "#15 Loss: 0.3333333432674408\n",
      "#16 Loss: 0.3333333432674408\n",
      "#17 Loss: 0.3333333432674408\n",
      "#18 Loss: 0.3333333432674408\n",
      "#19 Loss: 0.3333333432674408\n",
      "#20 Loss: 0.3333333432674408\n",
      "#21 Loss: 0.3333333432674408\n",
      "#22 Loss: 0.3333333432674408\n",
      "#23 Loss: 0.3333333432674408\n",
      "#24 Loss: 0.3333333432674408\n",
      "#25 Loss: 0.3333333432674408\n",
      "#26 Loss: 0.3333333432674408\n",
      "#27 Loss: 0.3333333432674408\n",
      "#28 Loss: 0.3333333432674408\n",
      "#29 Loss: 0.3333333432674408\n",
      "#30 Loss: 0.3333333432674408\n",
      "#31 Loss: 0.3333333432674408\n",
      "#32 Loss: 0.3333333432674408\n",
      "#33 Loss: 0.3333333432674408\n",
      "#34 Loss: 0.3333333432674408\n",
      "#35 Loss: 0.3333333432674408\n",
      "#36 Loss: 0.3333333432674408\n",
      "#37 Loss: 0.3333333432674408\n",
      "#38 Loss: 0.3333333432674408\n",
      "#39 Loss: 0.3333333432674408\n",
      "#40 Loss: 0.3333333432674408\n",
      "#41 Loss: 0.3333333432674408\n",
      "#42 Loss: 0.3333333432674408\n",
      "#43 Loss: 0.3333333432674408\n",
      "#44 Loss: 0.3333333432674408\n",
      "#45 Loss: 0.3333333432674408\n",
      "#46 Loss: 0.3333333432674408\n",
      "#47 Loss: 0.3333333432674408\n",
      "#48 Loss: 0.3333333432674408\n",
      "#49 Loss: 0.3333333432674408\n",
      "#50 Loss: 0.3333333432674408\n",
      "#51 Loss: 0.3333333432674408\n",
      "#52 Loss: 0.3333333432674408\n",
      "#53 Loss: 0.3333333432674408\n",
      "#54 Loss: 0.3333333432674408\n",
      "#55 Loss: 0.3333333432674408\n",
      "#56 Loss: 0.3333333432674408\n",
      "#57 Loss: 0.3333333432674408\n",
      "#58 Loss: 0.3333333432674408\n",
      "#59 Loss: 0.3333333432674408\n",
      "#60 Loss: 0.3333333432674408\n",
      "#61 Loss: 0.3333333432674408\n",
      "#62 Loss: 0.3333333432674408\n",
      "#63 Loss: 0.3333333432674408\n",
      "#64 Loss: 0.3333333432674408\n",
      "#65 Loss: 0.3333333432674408\n",
      "#66 Loss: 0.3333333432674408\n",
      "#67 Loss: 0.3333333432674408\n",
      "#68 Loss: 0.3333333432674408\n",
      "#69 Loss: 0.3333333432674408\n",
      "#70 Loss: 0.3333333432674408\n",
      "#71 Loss: 0.3333333432674408\n",
      "#72 Loss: 0.3333333432674408\n",
      "#73 Loss: 0.3333333432674408\n",
      "#74 Loss: 0.3333333432674408\n",
      "#75 Loss: 0.3333333432674408\n",
      "#76 Loss: 0.3333333432674408\n",
      "#77 Loss: 0.3333333432674408\n",
      "#78 Loss: 0.3333333432674408\n",
      "#79 Loss: 0.3333333432674408\n",
      "#80 Loss: 0.3333333432674408\n",
      "#81 Loss: 0.3333333432674408\n",
      "#82 Loss: 0.3333333432674408\n",
      "#83 Loss: 0.3333333432674408\n",
      "#84 Loss: 0.3333333432674408\n",
      "#85 Loss: 0.3333333432674408\n",
      "#86 Loss: 0.3333333432674408\n",
      "#87 Loss: 0.3333333432674408\n",
      "#88 Loss: 0.3333333432674408\n",
      "#89 Loss: 0.3333333432674408\n",
      "#90 Loss: 0.3333333432674408\n",
      "#91 Loss: 0.3333333432674408\n",
      "#92 Loss: 0.3333333432674408\n",
      "#93 Loss: 0.3333333432674408\n",
      "#94 Loss: 0.3333333432674408\n",
      "#95 Loss: 0.3333333432674408\n",
      "#96 Loss: 0.3333333432674408\n",
      "#97 Loss: 0.3333333432674408\n",
      "#98 Loss: 0.3333333432674408\n",
      "#99 Loss: 0.3333333432674408\n",
      "#100 Loss: 0.3333333432674408\n",
      "#101 Loss: 0.3333333432674408\n",
      "#102 Loss: 0.3333333432674408\n",
      "#103 Loss: 0.3333333432674408\n",
      "#104 Loss: 0.3333333432674408\n",
      "#105 Loss: 0.3333333432674408\n",
      "#106 Loss: 0.3333333432674408\n",
      "#107 Loss: 0.3333333432674408\n",
      "#108 Loss: 0.3333333432674408\n",
      "#109 Loss: 0.3333333432674408\n",
      "#110 Loss: 0.3333333432674408\n",
      "#111 Loss: 0.3333333432674408\n",
      "#112 Loss: 0.3333333432674408\n",
      "#113 Loss: 0.3333333432674408\n",
      "#114 Loss: 0.3333333432674408\n",
      "#115 Loss: 0.3333333432674408\n",
      "#116 Loss: 0.3333333432674408\n",
      "#117 Loss: 0.3333333432674408\n",
      "#118 Loss: 0.3333333432674408\n",
      "#119 Loss: 0.3333333432674408\n",
      "#120 Loss: 0.3333333432674408\n",
      "#121 Loss: 0.3333333432674408\n",
      "#122 Loss: 0.3333333432674408\n",
      "#123 Loss: 0.3333333432674408\n",
      "#124 Loss: 0.3333333432674408\n",
      "#125 Loss: 0.3333333432674408\n",
      "#126 Loss: 0.3333333432674408\n",
      "#127 Loss: 0.3333333432674408\n",
      "#128 Loss: 0.3333333432674408\n",
      "#129 Loss: 0.3333333432674408\n",
      "#130 Loss: 0.3333333432674408\n",
      "#131 Loss: 0.3333333432674408\n",
      "#132 Loss: 0.3333333432674408\n",
      "#133 Loss: 0.3333333432674408\n",
      "#134 Loss: 0.3333333432674408\n",
      "#135 Loss: 0.3333333432674408\n",
      "#136 Loss: 0.3333333432674408\n",
      "#137 Loss: 0.3333333432674408\n",
      "#138 Loss: 0.3333333432674408\n",
      "#139 Loss: 0.3333333432674408\n",
      "#140 Loss: 0.3333333432674408\n",
      "#141 Loss: 0.3333333432674408\n",
      "#142 Loss: 0.3333333432674408\n",
      "#143 Loss: 0.3333333432674408\n",
      "#144 Loss: 0.3333333432674408\n",
      "#145 Loss: 0.3333333432674408\n",
      "#146 Loss: 0.3333333432674408\n",
      "#147 Loss: 0.3333333432674408\n",
      "#148 Loss: 0.3333333432674408\n",
      "#149 Loss: 0.3333333432674408\n",
      "#150 Loss: 0.3333333432674408\n",
      "#151 Loss: 0.3333333432674408\n",
      "#152 Loss: 0.3333333432674408\n",
      "#153 Loss: 0.3333333432674408\n",
      "#154 Loss: 0.3333333432674408\n",
      "#155 Loss: 0.3333333432674408\n",
      "#156 Loss: 0.3333333432674408\n",
      "#157 Loss: 0.3333333432674408\n",
      "#158 Loss: 0.3333333432674408\n",
      "#159 Loss: 0.3333333432674408\n",
      "#160 Loss: 0.3333333432674408\n",
      "#161 Loss: 0.3333333432674408\n",
      "#162 Loss: 0.3333333432674408\n",
      "#163 Loss: 0.3333333432674408\n",
      "#164 Loss: 0.3333333432674408\n",
      "#165 Loss: 0.3333333432674408\n",
      "#166 Loss: 0.3333333432674408\n",
      "#167 Loss: 0.3333333432674408\n",
      "#168 Loss: 0.3333333432674408\n",
      "#169 Loss: 0.3333333432674408\n",
      "#170 Loss: 0.3333333432674408\n",
      "#171 Loss: 0.3333333432674408\n",
      "#172 Loss: 0.3333333432674408\n",
      "#173 Loss: 0.3333333432674408\n",
      "#174 Loss: 0.3333333432674408\n",
      "#175 Loss: 0.3333333432674408\n",
      "#176 Loss: 0.3333333432674408\n",
      "#177 Loss: 0.3333333432674408\n",
      "#178 Loss: 0.3333333432674408\n",
      "#179 Loss: 0.3333333432674408\n",
      "#180 Loss: 0.3333333432674408\n",
      "#181 Loss: 0.3333333432674408\n",
      "#182 Loss: 0.3333333432674408\n",
      "#183 Loss: 0.3333333432674408\n",
      "#184 Loss: 0.3333333432674408\n",
      "#185 Loss: 0.3333333432674408\n",
      "#186 Loss: 0.3333333432674408\n",
      "#187 Loss: 0.3333333432674408\n",
      "#188 Loss: 0.3333333432674408\n",
      "#189 Loss: 0.3333333432674408\n",
      "#190 Loss: 0.3333333432674408\n",
      "#191 Loss: 0.3333333432674408\n",
      "#192 Loss: 0.3333333432674408\n",
      "#193 Loss: 0.3333333432674408\n",
      "#194 Loss: 0.3333333432674408\n",
      "#195 Loss: 0.3333333432674408\n",
      "#196 Loss: 0.3333333432674408\n",
      "#197 Loss: 0.3333333432674408\n",
      "#198 Loss: 0.3333333432674408\n",
      "#199 Loss: 0.3333333432674408\n",
      "#200 Loss: 0.3333333432674408\n",
      "#201 Loss: 0.3333333432674408\n",
      "#202 Loss: 0.3333333432674408\n",
      "#203 Loss: 0.3333333432674408\n",
      "#204 Loss: 0.3333333432674408\n",
      "#205 Loss: 0.3333333432674408\n",
      "#206 Loss: 0.3333333432674408\n",
      "#207 Loss: 0.3333333432674408\n",
      "#208 Loss: 0.3333333432674408\n",
      "#209 Loss: 0.3333333432674408\n",
      "#210 Loss: 0.3333333432674408\n",
      "#211 Loss: 0.3333333432674408\n",
      "#212 Loss: 0.3333333432674408\n",
      "#213 Loss: 0.3333333432674408\n",
      "#214 Loss: 0.3333333432674408\n",
      "#215 Loss: 0.3333333432674408\n",
      "#216 Loss: 0.3333333432674408\n",
      "#217 Loss: 0.3333333432674408\n",
      "#218 Loss: 0.3333333432674408\n",
      "#219 Loss: 0.3333333432674408\n",
      "#220 Loss: 0.3333333432674408\n",
      "#221 Loss: 0.3333333432674408\n",
      "#222 Loss: 0.3333333432674408\n",
      "#223 Loss: 0.3333333432674408\n",
      "#224 Loss: 0.3333333432674408\n",
      "#225 Loss: 0.3333333432674408\n",
      "#226 Loss: 0.3333333432674408\n",
      "#227 Loss: 0.3333333432674408\n",
      "#228 Loss: 0.3333333432674408\n",
      "#229 Loss: 0.3333333432674408\n",
      "#230 Loss: 0.3333333432674408\n",
      "#231 Loss: 0.3333333432674408\n",
      "#232 Loss: 0.3333333432674408\n",
      "#233 Loss: 0.3333333432674408\n",
      "#234 Loss: 0.3333333432674408\n",
      "#235 Loss: 0.3333333432674408\n",
      "#236 Loss: 0.3333333432674408\n",
      "#237 Loss: 0.3333333432674408\n",
      "#238 Loss: 0.3333333432674408\n",
      "#239 Loss: 0.3333333432674408\n",
      "#240 Loss: 0.3333333432674408\n",
      "#241 Loss: 0.3333333432674408\n",
      "#242 Loss: 0.3333333432674408\n",
      "#243 Loss: 0.3333333432674408\n",
      "#244 Loss: 0.3333333432674408\n",
      "#245 Loss: 0.3333333432674408\n",
      "#246 Loss: 0.3333333432674408\n",
      "#247 Loss: 0.3333333432674408\n",
      "#248 Loss: 0.3333333432674408\n",
      "#249 Loss: 0.3333333432674408\n",
      "#250 Loss: 0.3333333432674408\n",
      "#251 Loss: 0.3333333432674408\n",
      "#252 Loss: 0.3333333432674408\n",
      "#253 Loss: 0.3333333432674408\n",
      "#254 Loss: 0.3333333432674408\n",
      "#255 Loss: 0.3333333432674408\n",
      "#256 Loss: 0.3333333432674408\n",
      "#257 Loss: 0.3333333432674408\n",
      "#258 Loss: 0.3333333432674408\n",
      "#259 Loss: 0.3333333432674408\n",
      "#260 Loss: 0.3333333432674408\n",
      "#261 Loss: 0.3333333432674408\n",
      "#262 Loss: 0.3333333432674408\n",
      "#263 Loss: 0.3333333432674408\n",
      "#264 Loss: 0.3333333432674408\n",
      "#265 Loss: 0.3333333432674408\n",
      "#266 Loss: 0.3333333432674408\n",
      "#267 Loss: 0.3333333432674408\n",
      "#268 Loss: 0.3333333432674408\n",
      "#269 Loss: 0.3333333432674408\n",
      "#270 Loss: 0.3333333432674408\n",
      "#271 Loss: 0.3333333432674408\n",
      "#272 Loss: 0.3333333432674408\n",
      "#273 Loss: 0.3333333432674408\n",
      "#274 Loss: 0.3333333432674408\n",
      "#275 Loss: 0.3333333432674408\n",
      "#276 Loss: 0.3333333432674408\n",
      "#277 Loss: 0.3333333432674408\n",
      "#278 Loss: 0.3333333432674408\n",
      "#279 Loss: 0.3333333432674408\n",
      "#280 Loss: 0.3333333432674408\n",
      "#281 Loss: 0.3333333432674408\n",
      "#282 Loss: 0.3333333432674408\n",
      "#283 Loss: 0.3333333432674408\n",
      "#284 Loss: 0.3333333432674408\n",
      "#285 Loss: 0.3333333432674408\n",
      "#286 Loss: 0.3333333432674408\n",
      "#287 Loss: 0.3333333432674408\n",
      "#288 Loss: 0.3333333432674408\n",
      "#289 Loss: 0.3333333432674408\n",
      "#290 Loss: 0.3333333432674408\n",
      "#291 Loss: 0.3333333432674408\n",
      "#292 Loss: 0.3333333432674408\n",
      "#293 Loss: 0.3333333432674408\n",
      "#294 Loss: 0.3333333432674408\n",
      "#295 Loss: 0.3333333432674408\n",
      "#296 Loss: 0.3333333432674408\n",
      "#297 Loss: 0.3333333432674408\n",
      "#298 Loss: 0.3333333432674408\n",
      "#299 Loss: 0.3333333432674408\n",
      "#300 Loss: 0.3333333432674408\n",
      "#301 Loss: 0.3333333432674408\n",
      "#302 Loss: 0.3333333432674408\n",
      "#303 Loss: 0.3333333432674408\n",
      "#304 Loss: 0.3333333432674408\n",
      "#305 Loss: 0.3333333432674408\n",
      "#306 Loss: 0.3333333432674408\n",
      "#307 Loss: 0.3333333432674408\n",
      "#308 Loss: 0.3333333432674408\n",
      "#309 Loss: 0.3333333432674408\n",
      "#310 Loss: 0.3333333432674408\n",
      "#311 Loss: 0.3333333432674408\n",
      "#312 Loss: 0.3333333432674408\n",
      "#313 Loss: 0.3333333432674408\n",
      "#314 Loss: 0.3333333432674408\n",
      "#315 Loss: 0.3333333432674408\n",
      "#316 Loss: 0.3333333432674408\n",
      "#317 Loss: 0.3333333432674408\n",
      "#318 Loss: 0.3333333432674408\n",
      "#319 Loss: 0.3333333432674408\n",
      "#320 Loss: 0.3333333432674408\n",
      "#321 Loss: 0.3333333432674408\n",
      "#322 Loss: 0.3333333432674408\n",
      "#323 Loss: 0.3333333432674408\n",
      "#324 Loss: 0.3333333432674408\n",
      "#325 Loss: 0.3333333432674408\n",
      "#326 Loss: 0.3333333432674408\n",
      "#327 Loss: 0.3333333432674408\n",
      "#328 Loss: 0.3333333432674408\n",
      "#329 Loss: 0.3333333432674408\n",
      "#330 Loss: 0.3333333432674408\n",
      "#331 Loss: 0.3333333432674408\n",
      "#332 Loss: 0.3333333432674408\n",
      "#333 Loss: 0.3333333432674408\n",
      "#334 Loss: 0.3333333432674408\n",
      "#335 Loss: 0.3333333432674408\n",
      "#336 Loss: 0.3333333432674408\n",
      "#337 Loss: 0.3333333432674408\n",
      "#338 Loss: 0.3333333432674408\n",
      "#339 Loss: 0.3333333432674408\n",
      "#340 Loss: 0.3333333432674408\n",
      "#341 Loss: 0.3333333432674408\n",
      "#342 Loss: 0.3333333432674408\n",
      "#343 Loss: 0.3333333432674408\n",
      "#344 Loss: 0.3333333432674408\n",
      "#345 Loss: 0.3333333432674408\n",
      "#346 Loss: 0.3333333432674408\n",
      "#347 Loss: 0.3333333432674408\n",
      "#348 Loss: 0.3333333432674408\n",
      "#349 Loss: 0.3333333432674408\n",
      "#350 Loss: 0.3333333432674408\n",
      "#351 Loss: 0.3333333432674408\n",
      "#352 Loss: 0.3333333432674408\n",
      "#353 Loss: 0.3333333432674408\n",
      "#354 Loss: 0.3333333432674408\n",
      "#355 Loss: 0.3333333432674408\n",
      "#356 Loss: 0.3333333432674408\n",
      "#357 Loss: 0.3333333432674408\n",
      "#358 Loss: 0.3333333432674408\n",
      "#359 Loss: 0.3333333432674408\n",
      "#360 Loss: 0.3333333432674408\n",
      "#361 Loss: 0.3333333432674408\n",
      "#362 Loss: 0.3333333432674408\n",
      "#363 Loss: 0.3333333432674408\n",
      "#364 Loss: 0.3333333432674408\n",
      "#365 Loss: 0.3333333432674408\n",
      "#366 Loss: 0.3333333432674408\n",
      "#367 Loss: 0.3333333432674408\n",
      "#368 Loss: 0.3333333432674408\n",
      "#369 Loss: 0.3333333432674408\n",
      "#370 Loss: 0.3333333432674408\n",
      "#371 Loss: 0.3333333432674408\n",
      "#372 Loss: 0.3333333432674408\n",
      "#373 Loss: 0.3333333432674408\n",
      "#374 Loss: 0.3333333432674408\n",
      "#375 Loss: 0.3333333432674408\n",
      "#376 Loss: 0.3333333432674408\n",
      "#377 Loss: 0.3333333432674408\n",
      "#378 Loss: 0.3333333432674408\n",
      "#379 Loss: 0.3333333432674408\n",
      "#380 Loss: 0.3333333432674408\n",
      "#381 Loss: 0.3333333432674408\n",
      "#382 Loss: 0.3333333432674408\n",
      "#383 Loss: 0.3333333432674408\n",
      "#384 Loss: 0.3333333432674408\n",
      "#385 Loss: 0.3333333432674408\n",
      "#386 Loss: 0.3333333432674408\n",
      "#387 Loss: 0.3333333432674408\n",
      "#388 Loss: 0.3333333432674408\n",
      "#389 Loss: 0.3333333432674408\n",
      "#390 Loss: 0.3333333432674408\n",
      "#391 Loss: 0.3333333432674408\n",
      "#392 Loss: 0.3333333432674408\n",
      "#393 Loss: 0.3333333432674408\n",
      "#394 Loss: 0.3333333432674408\n",
      "#395 Loss: 0.3333333432674408\n",
      "#396 Loss: 0.3333333432674408\n",
      "#397 Loss: 0.3333333432674408\n",
      "#398 Loss: 0.3333333432674408\n",
      "#399 Loss: 0.3333333432674408\n",
      "#400 Loss: 0.3333333432674408\n",
      "#401 Loss: 0.3333333432674408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#402 Loss: 0.3333333432674408\n",
      "#403 Loss: 0.3333333432674408\n",
      "#404 Loss: 0.3333333432674408\n",
      "#405 Loss: 0.3333333432674408\n",
      "#406 Loss: 0.3333333432674408\n",
      "#407 Loss: 0.3333333432674408\n",
      "#408 Loss: 0.3333333432674408\n",
      "#409 Loss: 0.3333333432674408\n",
      "#410 Loss: 0.3333333432674408\n",
      "#411 Loss: 0.3333333432674408\n",
      "#412 Loss: 0.3333333432674408\n",
      "#413 Loss: 0.3333333432674408\n",
      "#414 Loss: 0.3333333432674408\n",
      "#415 Loss: 0.3333333432674408\n",
      "#416 Loss: 0.3333333432674408\n",
      "#417 Loss: 0.3333333432674408\n",
      "#418 Loss: 0.3333333432674408\n",
      "#419 Loss: 0.3333333432674408\n",
      "#420 Loss: 0.3333333432674408\n",
      "#421 Loss: 0.3333333432674408\n",
      "#422 Loss: 0.3333333432674408\n",
      "#423 Loss: 0.3333333432674408\n",
      "#424 Loss: 0.3333333432674408\n",
      "#425 Loss: 0.3333333432674408\n",
      "#426 Loss: 0.3333333432674408\n",
      "#427 Loss: 0.3333333432674408\n",
      "#428 Loss: 0.3333333432674408\n",
      "#429 Loss: 0.3333333432674408\n",
      "#430 Loss: 0.3333333432674408\n",
      "#431 Loss: 0.3333333432674408\n",
      "#432 Loss: 0.3333333432674408\n",
      "#433 Loss: 0.3333333432674408\n",
      "#434 Loss: 0.3333333432674408\n",
      "#435 Loss: 0.3333333432674408\n",
      "#436 Loss: 0.3333333432674408\n",
      "#437 Loss: 0.3333333432674408\n",
      "#438 Loss: 0.3333333432674408\n",
      "#439 Loss: 0.3333333432674408\n",
      "#440 Loss: 0.3333333432674408\n",
      "#441 Loss: 0.3333333432674408\n",
      "#442 Loss: 0.3333333432674408\n",
      "#443 Loss: 0.3333333432674408\n",
      "#444 Loss: 0.3333333432674408\n",
      "#445 Loss: 0.3333333432674408\n",
      "#446 Loss: 0.3333333432674408\n",
      "#447 Loss: 0.3333333432674408\n",
      "#448 Loss: 0.3333333432674408\n",
      "#449 Loss: 0.3333333432674408\n",
      "#450 Loss: 0.3333333432674408\n",
      "#451 Loss: 0.3333333432674408\n",
      "#452 Loss: 0.3333333432674408\n",
      "#453 Loss: 0.3333333432674408\n",
      "#454 Loss: 0.3333333432674408\n",
      "#455 Loss: 0.3333333432674408\n",
      "#456 Loss: 0.3333333432674408\n",
      "#457 Loss: 0.3333333432674408\n",
      "#458 Loss: 0.3333333432674408\n",
      "#459 Loss: 0.3333333432674408\n",
      "#460 Loss: 0.3333333432674408\n",
      "#461 Loss: 0.3333333432674408\n",
      "#462 Loss: 0.3333333432674408\n",
      "#463 Loss: 0.3333333432674408\n",
      "#464 Loss: 0.3333333432674408\n",
      "#465 Loss: 0.3333333432674408\n",
      "#466 Loss: 0.3333333432674408\n",
      "#467 Loss: 0.3333333432674408\n",
      "#468 Loss: 0.3333333432674408\n",
      "#469 Loss: 0.3333333432674408\n",
      "#470 Loss: 0.3333333432674408\n",
      "#471 Loss: 0.3333333432674408\n",
      "#472 Loss: 0.3333333432674408\n",
      "#473 Loss: 0.3333333432674408\n",
      "#474 Loss: 0.3333333432674408\n",
      "#475 Loss: 0.3333333432674408\n",
      "#476 Loss: 0.3333333432674408\n",
      "#477 Loss: 0.3333333432674408\n",
      "#478 Loss: 0.3333333432674408\n",
      "#479 Loss: 0.3333333432674408\n",
      "#480 Loss: 0.3333333432674408\n",
      "#481 Loss: 0.3333333432674408\n",
      "#482 Loss: 0.3333333432674408\n",
      "#483 Loss: 0.3333333432674408\n",
      "#484 Loss: 0.3333333432674408\n",
      "#485 Loss: 0.3333333432674408\n",
      "#486 Loss: 0.3333333432674408\n",
      "#487 Loss: 0.3333333432674408\n",
      "#488 Loss: 0.3333333432674408\n",
      "#489 Loss: 0.3333333432674408\n",
      "#490 Loss: 0.3333333432674408\n",
      "#491 Loss: 0.3333333432674408\n",
      "#492 Loss: 0.3333333432674408\n",
      "#493 Loss: 0.3333333432674408\n",
      "#494 Loss: 0.3333333432674408\n",
      "#495 Loss: 0.3333333432674408\n",
      "#496 Loss: 0.3333333432674408\n",
      "#497 Loss: 0.3333333432674408\n",
      "#498 Loss: 0.3333333432674408\n",
      "#499 Loss: 0.3333333432674408\n",
      "#500 Loss: 0.3333333432674408\n",
      "#501 Loss: 0.3333333432674408\n",
      "#502 Loss: 0.3333333432674408\n",
      "#503 Loss: 0.3333333432674408\n",
      "#504 Loss: 0.3333333432674408\n",
      "#505 Loss: 0.3333333432674408\n",
      "#506 Loss: 0.3333333432674408\n",
      "#507 Loss: 0.3333333432674408\n",
      "#508 Loss: 0.3333333432674408\n",
      "#509 Loss: 0.3333333432674408\n",
      "#510 Loss: 0.3333333432674408\n",
      "#511 Loss: 0.3333333432674408\n",
      "#512 Loss: 0.3333333432674408\n",
      "#513 Loss: 0.3333333432674408\n",
      "#514 Loss: 0.3333333432674408\n",
      "#515 Loss: 0.3333333432674408\n",
      "#516 Loss: 0.3333333432674408\n",
      "#517 Loss: 0.3333333432674408\n",
      "#518 Loss: 0.3333333432674408\n",
      "#519 Loss: 0.3333333432674408\n",
      "#520 Loss: 0.3333333432674408\n",
      "#521 Loss: 0.3333333432674408\n",
      "#522 Loss: 0.3333333432674408\n",
      "#523 Loss: 0.3333333432674408\n",
      "#524 Loss: 0.3333333432674408\n",
      "#525 Loss: 0.3333333432674408\n",
      "#526 Loss: 0.3333333432674408\n",
      "#527 Loss: 0.3333333432674408\n",
      "#528 Loss: 0.3333333432674408\n",
      "#529 Loss: 0.3333333432674408\n",
      "#530 Loss: 0.3333333432674408\n",
      "#531 Loss: 0.3333333432674408\n",
      "#532 Loss: 0.3333333432674408\n",
      "#533 Loss: 0.3333333432674408\n",
      "#534 Loss: 0.3333333432674408\n",
      "#535 Loss: 0.3333333432674408\n",
      "#536 Loss: 0.3333333432674408\n",
      "#537 Loss: 0.3333333432674408\n",
      "#538 Loss: 0.3333333432674408\n",
      "#539 Loss: 0.3333333432674408\n",
      "#540 Loss: 0.3333333432674408\n",
      "#541 Loss: 0.3333333432674408\n",
      "#542 Loss: 0.3333333432674408\n",
      "#543 Loss: 0.3333333432674408\n",
      "#544 Loss: 0.3333333432674408\n",
      "#545 Loss: 0.3333333432674408\n",
      "#546 Loss: 0.3333333432674408\n",
      "#547 Loss: 0.3333333432674408\n",
      "#548 Loss: 0.3333333432674408\n",
      "#549 Loss: 0.3333333432674408\n",
      "#550 Loss: 0.3333333432674408\n",
      "#551 Loss: 0.3333333432674408\n",
      "#552 Loss: 0.3333333432674408\n",
      "#553 Loss: 0.3333333432674408\n",
      "#554 Loss: 0.3333333432674408\n",
      "#555 Loss: 0.3333333432674408\n",
      "#556 Loss: 0.3333333432674408\n",
      "#557 Loss: 0.3333333432674408\n",
      "#558 Loss: 0.3333333432674408\n",
      "#559 Loss: 0.3333333432674408\n",
      "#560 Loss: 0.3333333432674408\n",
      "#561 Loss: 0.3333333432674408\n",
      "#562 Loss: 0.3333333432674408\n",
      "#563 Loss: 0.3333333432674408\n",
      "#564 Loss: 0.3333333432674408\n",
      "#565 Loss: 0.3333333432674408\n",
      "#566 Loss: 0.3333333432674408\n",
      "#567 Loss: 0.3333333432674408\n",
      "#568 Loss: 0.3333333432674408\n",
      "#569 Loss: 0.3333333432674408\n",
      "#570 Loss: 0.3333333432674408\n",
      "#571 Loss: 0.3333333432674408\n",
      "#572 Loss: 0.3333333432674408\n",
      "#573 Loss: 0.3333333432674408\n",
      "#574 Loss: 0.3333333432674408\n",
      "#575 Loss: 0.3333333432674408\n",
      "#576 Loss: 0.3333333432674408\n",
      "#577 Loss: 0.3333333432674408\n",
      "#578 Loss: 0.3333333432674408\n",
      "#579 Loss: 0.3333333432674408\n",
      "#580 Loss: 0.3333333432674408\n",
      "#581 Loss: 0.3333333432674408\n",
      "#582 Loss: 0.3333333432674408\n",
      "#583 Loss: 0.3333333432674408\n",
      "#584 Loss: 0.3333333432674408\n",
      "#585 Loss: 0.3333333432674408\n",
      "#586 Loss: 0.3333333432674408\n",
      "#587 Loss: 0.3333333432674408\n",
      "#588 Loss: 0.3333333432674408\n",
      "#589 Loss: 0.3333333432674408\n",
      "#590 Loss: 0.3333333432674408\n",
      "#591 Loss: 0.3333333432674408\n",
      "#592 Loss: 0.3333333432674408\n",
      "#593 Loss: 0.3333333432674408\n",
      "#594 Loss: 0.3333333432674408\n",
      "#595 Loss: 0.3333333432674408\n",
      "#596 Loss: 0.3333333432674408\n",
      "#597 Loss: 0.3333333432674408\n",
      "#598 Loss: 0.3333333432674408\n",
      "#599 Loss: 0.3333333432674408\n",
      "#600 Loss: 0.3333333432674408\n",
      "#601 Loss: 0.3333333432674408\n",
      "#602 Loss: 0.3333333432674408\n",
      "#603 Loss: 0.3333333432674408\n",
      "#604 Loss: 0.3333333432674408\n",
      "#605 Loss: 0.3333333432674408\n",
      "#606 Loss: 0.3333333432674408\n",
      "#607 Loss: 0.3333333432674408\n",
      "#608 Loss: 0.3333333432674408\n",
      "#609 Loss: 0.3333333432674408\n",
      "#610 Loss: 0.3333333432674408\n",
      "#611 Loss: 0.3333333432674408\n",
      "#612 Loss: 0.3333333432674408\n",
      "#613 Loss: 0.3333333432674408\n",
      "#614 Loss: 0.3333333432674408\n",
      "#615 Loss: 0.3333333432674408\n",
      "#616 Loss: 0.3333333432674408\n",
      "#617 Loss: 0.3333333432674408\n",
      "#618 Loss: 0.3333333432674408\n",
      "#619 Loss: 0.3333333432674408\n",
      "#620 Loss: 0.3333333432674408\n",
      "#621 Loss: 0.3333333432674408\n",
      "#622 Loss: 0.3333333432674408\n",
      "#623 Loss: 0.3333333432674408\n",
      "#624 Loss: 0.3333333432674408\n",
      "#625 Loss: 0.3333333432674408\n",
      "#626 Loss: 0.3333333432674408\n",
      "#627 Loss: 0.3333333432674408\n",
      "#628 Loss: 0.3333333432674408\n",
      "#629 Loss: 0.3333333432674408\n",
      "#630 Loss: 0.3333333432674408\n",
      "#631 Loss: 0.3333333432674408\n",
      "#632 Loss: 0.3333333432674408\n",
      "#633 Loss: 0.3333333432674408\n",
      "#634 Loss: 0.3333333432674408\n",
      "#635 Loss: 0.3333333432674408\n",
      "#636 Loss: 0.3333333432674408\n",
      "#637 Loss: 0.3333333432674408\n",
      "#638 Loss: 0.3333333432674408\n",
      "#639 Loss: 0.3333333432674408\n",
      "#640 Loss: 0.3333333432674408\n",
      "#641 Loss: 0.3333333432674408\n",
      "#642 Loss: 0.3333333432674408\n",
      "#643 Loss: 0.3333333432674408\n",
      "#644 Loss: 0.3333333432674408\n",
      "#645 Loss: 0.3333333432674408\n",
      "#646 Loss: 0.3333333432674408\n",
      "#647 Loss: 0.3333333432674408\n",
      "#648 Loss: 0.3333333432674408\n",
      "#649 Loss: 0.3333333432674408\n",
      "#650 Loss: 0.3333333432674408\n",
      "#651 Loss: 0.3333333432674408\n",
      "#652 Loss: 0.3333333432674408\n",
      "#653 Loss: 0.3333333432674408\n",
      "#654 Loss: 0.3333333432674408\n",
      "#655 Loss: 0.3333333432674408\n",
      "#656 Loss: 0.3333333432674408\n",
      "#657 Loss: 0.3333333432674408\n",
      "#658 Loss: 0.3333333432674408\n",
      "#659 Loss: 0.3333333432674408\n",
      "#660 Loss: 0.3333333432674408\n",
      "#661 Loss: 0.3333333432674408\n",
      "#662 Loss: 0.3333333432674408\n",
      "#663 Loss: 0.3333333432674408\n",
      "#664 Loss: 0.3333333432674408\n",
      "#665 Loss: 0.3333333432674408\n",
      "#666 Loss: 0.3333333432674408\n",
      "#667 Loss: 0.3333333432674408\n",
      "#668 Loss: 0.3333333432674408\n",
      "#669 Loss: 0.3333333432674408\n",
      "#670 Loss: 0.3333333432674408\n",
      "#671 Loss: 0.3333333432674408\n",
      "#672 Loss: 0.3333333432674408\n",
      "#673 Loss: 0.3333333432674408\n",
      "#674 Loss: 0.3333333432674408\n",
      "#675 Loss: 0.3333333432674408\n",
      "#676 Loss: 0.3333333432674408\n",
      "#677 Loss: 0.3333333432674408\n",
      "#678 Loss: 0.3333333432674408\n",
      "#679 Loss: 0.3333333432674408\n",
      "#680 Loss: 0.3333333432674408\n",
      "#681 Loss: 0.3333333432674408\n",
      "#682 Loss: 0.3333333432674408\n",
      "#683 Loss: 0.3333333432674408\n",
      "#684 Loss: 0.3333333432674408\n",
      "#685 Loss: 0.3333333432674408\n",
      "#686 Loss: 0.3333333432674408\n",
      "#687 Loss: 0.3333333432674408\n",
      "#688 Loss: 0.3333333432674408\n",
      "#689 Loss: 0.3333333432674408\n",
      "#690 Loss: 0.3333333432674408\n",
      "#691 Loss: 0.3333333432674408\n",
      "#692 Loss: 0.3333333432674408\n",
      "#693 Loss: 0.3333333432674408\n",
      "#694 Loss: 0.3333333432674408\n",
      "#695 Loss: 0.3333333432674408\n",
      "#696 Loss: 0.3333333432674408\n",
      "#697 Loss: 0.3333333432674408\n",
      "#698 Loss: 0.3333333432674408\n",
      "#699 Loss: 0.3333333432674408\n",
      "#700 Loss: 0.3333333432674408\n",
      "#701 Loss: 0.3333333432674408\n",
      "#702 Loss: 0.3333333432674408\n",
      "#703 Loss: 0.3333333432674408\n",
      "#704 Loss: 0.3333333432674408\n",
      "#705 Loss: 0.3333333432674408\n",
      "#706 Loss: 0.3333333432674408\n",
      "#707 Loss: 0.3333333432674408\n",
      "#708 Loss: 0.3333333432674408\n",
      "#709 Loss: 0.3333333432674408\n",
      "#710 Loss: 0.3333333432674408\n",
      "#711 Loss: 0.3333333432674408\n",
      "#712 Loss: 0.3333333432674408\n",
      "#713 Loss: 0.3333333432674408\n",
      "#714 Loss: 0.3333333432674408\n",
      "#715 Loss: 0.3333333432674408\n",
      "#716 Loss: 0.3333333432674408\n",
      "#717 Loss: 0.3333333432674408\n",
      "#718 Loss: 0.3333333432674408\n",
      "#719 Loss: 0.3333333432674408\n",
      "#720 Loss: 0.3333333432674408\n",
      "#721 Loss: 0.3333333432674408\n",
      "#722 Loss: 0.3333333432674408\n",
      "#723 Loss: 0.3333333432674408\n",
      "#724 Loss: 0.3333333432674408\n",
      "#725 Loss: 0.3333333432674408\n",
      "#726 Loss: 0.3333333432674408\n",
      "#727 Loss: 0.3333333432674408\n",
      "#728 Loss: 0.3333333432674408\n",
      "#729 Loss: 0.3333333432674408\n",
      "#730 Loss: 0.3333333432674408\n",
      "#731 Loss: 0.3333333432674408\n",
      "#732 Loss: 0.3333333432674408\n",
      "#733 Loss: 0.3333333432674408\n",
      "#734 Loss: 0.3333333432674408\n",
      "#735 Loss: 0.3333333432674408\n",
      "#736 Loss: 0.3333333432674408\n",
      "#737 Loss: 0.3333333432674408\n",
      "#738 Loss: 0.3333333432674408\n",
      "#739 Loss: 0.3333333432674408\n",
      "#740 Loss: 0.3333333432674408\n",
      "#741 Loss: 0.3333333432674408\n",
      "#742 Loss: 0.3333333432674408\n",
      "#743 Loss: 0.3333333432674408\n",
      "#744 Loss: 0.3333333432674408\n",
      "#745 Loss: 0.3333333432674408\n",
      "#746 Loss: 0.3333333432674408\n",
      "#747 Loss: 0.3333333432674408\n",
      "#748 Loss: 0.3333333432674408\n",
      "#749 Loss: 0.3333333432674408\n",
      "#750 Loss: 0.3333333432674408\n",
      "#751 Loss: 0.3333333432674408\n",
      "#752 Loss: 0.3333333432674408\n",
      "#753 Loss: 0.3333333432674408\n",
      "#754 Loss: 0.3333333432674408\n",
      "#755 Loss: 0.3333333432674408\n",
      "#756 Loss: 0.3333333432674408\n",
      "#757 Loss: 0.3333333432674408\n",
      "#758 Loss: 0.3333333432674408\n",
      "#759 Loss: 0.3333333432674408\n",
      "#760 Loss: 0.3333333432674408\n",
      "#761 Loss: 0.3333333432674408\n",
      "#762 Loss: 0.3333333432674408\n",
      "#763 Loss: 0.3333333432674408\n",
      "#764 Loss: 0.3333333432674408\n",
      "#765 Loss: 0.3333333432674408\n",
      "#766 Loss: 0.3333333432674408\n",
      "#767 Loss: 0.3333333432674408\n",
      "#768 Loss: 0.3333333432674408\n",
      "#769 Loss: 0.3333333432674408\n",
      "#770 Loss: 0.3333333432674408\n",
      "#771 Loss: 0.3333333432674408\n",
      "#772 Loss: 0.3333333432674408\n",
      "#773 Loss: 0.3333333432674408\n",
      "#774 Loss: 0.3333333432674408\n",
      "#775 Loss: 0.3333333432674408\n",
      "#776 Loss: 0.3333333432674408\n",
      "#777 Loss: 0.3333333432674408\n",
      "#778 Loss: 0.3333333432674408\n",
      "#779 Loss: 0.3333333432674408\n",
      "#780 Loss: 0.3333333432674408\n",
      "#781 Loss: 0.3333333432674408\n",
      "#782 Loss: 0.3333333432674408\n",
      "#783 Loss: 0.3333333432674408\n",
      "#784 Loss: 0.3333333432674408\n",
      "#785 Loss: 0.3333333432674408\n",
      "#786 Loss: 0.3333333432674408\n",
      "#787 Loss: 0.3333333432674408\n",
      "#788 Loss: 0.3333333432674408\n",
      "#789 Loss: 0.3333333432674408\n",
      "#790 Loss: 0.3333333432674408\n",
      "#791 Loss: 0.3333333432674408\n",
      "#792 Loss: 0.3333333432674408\n",
      "#793 Loss: 0.3333333432674408\n",
      "#794 Loss: 0.3333333432674408\n",
      "#795 Loss: 0.3333333432674408\n",
      "#796 Loss: 0.3333333432674408\n",
      "#797 Loss: 0.3333333432674408\n",
      "#798 Loss: 0.3333333432674408\n",
      "#799 Loss: 0.3333333432674408\n",
      "#800 Loss: 0.3333333432674408\n",
      "#801 Loss: 0.3333333432674408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#802 Loss: 0.3333333432674408\n",
      "#803 Loss: 0.3333333432674408\n",
      "#804 Loss: 0.3333333432674408\n",
      "#805 Loss: 0.3333333432674408\n",
      "#806 Loss: 0.3333333432674408\n",
      "#807 Loss: 0.3333333432674408\n",
      "#808 Loss: 0.3333333432674408\n",
      "#809 Loss: 0.3333333432674408\n",
      "#810 Loss: 0.3333333432674408\n",
      "#811 Loss: 0.3333333432674408\n",
      "#812 Loss: 0.3333333432674408\n",
      "#813 Loss: 0.3333333432674408\n",
      "#814 Loss: 0.3333333432674408\n",
      "#815 Loss: 0.3333333432674408\n",
      "#816 Loss: 0.3333333432674408\n",
      "#817 Loss: 0.3333333432674408\n",
      "#818 Loss: 0.3333333432674408\n",
      "#819 Loss: 0.3333333432674408\n",
      "#820 Loss: 0.3333333432674408\n",
      "#821 Loss: 0.3333333432674408\n",
      "#822 Loss: 0.3333333432674408\n",
      "#823 Loss: 0.3333333432674408\n",
      "#824 Loss: 0.3333333432674408\n",
      "#825 Loss: 0.3333333432674408\n",
      "#826 Loss: 0.3333333432674408\n",
      "#827 Loss: 0.3333333432674408\n",
      "#828 Loss: 0.3333333432674408\n",
      "#829 Loss: 0.3333333432674408\n",
      "#830 Loss: 0.3333333432674408\n",
      "#831 Loss: 0.3333333432674408\n",
      "#832 Loss: 0.3333333432674408\n",
      "#833 Loss: 0.3333333432674408\n",
      "#834 Loss: 0.3333333432674408\n",
      "#835 Loss: 0.3333333432674408\n",
      "#836 Loss: 0.3333333432674408\n",
      "#837 Loss: 0.3333333432674408\n",
      "#838 Loss: 0.3333333432674408\n",
      "#839 Loss: 0.3333333432674408\n",
      "#840 Loss: 0.3333333432674408\n",
      "#841 Loss: 0.3333333432674408\n",
      "#842 Loss: 0.3333333432674408\n",
      "#843 Loss: 0.3333333432674408\n",
      "#844 Loss: 0.3333333432674408\n",
      "#845 Loss: 0.3333333432674408\n",
      "#846 Loss: 0.3333333432674408\n",
      "#847 Loss: 0.3333333432674408\n",
      "#848 Loss: 0.3333333432674408\n",
      "#849 Loss: 0.3333333432674408\n",
      "#850 Loss: 0.3333333432674408\n",
      "#851 Loss: 0.3333333432674408\n",
      "#852 Loss: 0.3333333432674408\n",
      "#853 Loss: 0.3333333432674408\n",
      "#854 Loss: 0.3333333432674408\n",
      "#855 Loss: 0.3333333432674408\n",
      "#856 Loss: 0.3333333432674408\n",
      "#857 Loss: 0.3333333432674408\n",
      "#858 Loss: 0.3333333432674408\n",
      "#859 Loss: 0.3333333432674408\n",
      "#860 Loss: 0.3333333432674408\n",
      "#861 Loss: 0.3333333432674408\n",
      "#862 Loss: 0.3333333432674408\n",
      "#863 Loss: 0.3333333432674408\n",
      "#864 Loss: 0.3333333432674408\n",
      "#865 Loss: 0.3333333432674408\n",
      "#866 Loss: 0.3333333432674408\n",
      "#867 Loss: 0.3333333432674408\n",
      "#868 Loss: 0.3333333432674408\n",
      "#869 Loss: 0.3333333432674408\n",
      "#870 Loss: 0.3333333432674408\n",
      "#871 Loss: 0.3333333432674408\n",
      "#872 Loss: 0.3333333432674408\n",
      "#873 Loss: 0.3333333432674408\n",
      "#874 Loss: 0.3333333432674408\n",
      "#875 Loss: 0.3333333432674408\n",
      "#876 Loss: 0.3333333432674408\n",
      "#877 Loss: 0.3333333432674408\n",
      "#878 Loss: 0.3333333432674408\n",
      "#879 Loss: 0.3333333432674408\n",
      "#880 Loss: 0.3333333432674408\n",
      "#881 Loss: 0.3333333432674408\n",
      "#882 Loss: 0.3333333432674408\n",
      "#883 Loss: 0.3333333432674408\n",
      "#884 Loss: 0.3333333432674408\n",
      "#885 Loss: 0.3333333432674408\n",
      "#886 Loss: 0.3333333432674408\n",
      "#887 Loss: 0.3333333432674408\n",
      "#888 Loss: 0.3333333432674408\n",
      "#889 Loss: 0.3333333432674408\n",
      "#890 Loss: 0.3333333432674408\n",
      "#891 Loss: 0.3333333432674408\n",
      "#892 Loss: 0.3333333432674408\n",
      "#893 Loss: 0.3333333432674408\n",
      "#894 Loss: 0.3333333432674408\n",
      "#895 Loss: 0.3333333432674408\n",
      "#896 Loss: 0.3333333432674408\n",
      "#897 Loss: 0.3333333432674408\n",
      "#898 Loss: 0.3333333432674408\n",
      "#899 Loss: 0.3333333432674408\n",
      "#900 Loss: 0.3333333432674408\n",
      "#901 Loss: 0.3333333432674408\n",
      "#902 Loss: 0.3333333432674408\n",
      "#903 Loss: 0.3333333432674408\n",
      "#904 Loss: 0.3333333432674408\n",
      "#905 Loss: 0.3333333432674408\n",
      "#906 Loss: 0.3333333432674408\n",
      "#907 Loss: 0.3333333432674408\n",
      "#908 Loss: 0.3333333432674408\n",
      "#909 Loss: 0.3333333432674408\n",
      "#910 Loss: 0.3333333432674408\n",
      "#911 Loss: 0.3333333432674408\n",
      "#912 Loss: 0.3333333432674408\n",
      "#913 Loss: 0.3333333432674408\n",
      "#914 Loss: 0.3333333432674408\n",
      "#915 Loss: 0.3333333432674408\n",
      "#916 Loss: 0.3333333432674408\n",
      "#917 Loss: 0.3333333432674408\n",
      "#918 Loss: 0.3333333432674408\n",
      "#919 Loss: 0.3333333432674408\n",
      "#920 Loss: 0.3333333432674408\n",
      "#921 Loss: 0.3333333432674408\n",
      "#922 Loss: 0.3333333432674408\n",
      "#923 Loss: 0.3333333432674408\n",
      "#924 Loss: 0.3333333432674408\n",
      "#925 Loss: 0.3333333432674408\n",
      "#926 Loss: 0.3333333432674408\n",
      "#927 Loss: 0.3333333432674408\n",
      "#928 Loss: 0.3333333432674408\n",
      "#929 Loss: 0.3333333432674408\n",
      "#930 Loss: 0.3333333432674408\n",
      "#931 Loss: 0.3333333432674408\n",
      "#932 Loss: 0.3333333432674408\n",
      "#933 Loss: 0.3333333432674408\n",
      "#934 Loss: 0.3333333432674408\n",
      "#935 Loss: 0.3333333432674408\n",
      "#936 Loss: 0.3333333432674408\n",
      "#937 Loss: 0.3333333432674408\n",
      "#938 Loss: 0.3333333432674408\n",
      "#939 Loss: 0.3333333432674408\n",
      "#940 Loss: 0.3333333432674408\n",
      "#941 Loss: 0.3333333432674408\n",
      "#942 Loss: 0.3333333432674408\n",
      "#943 Loss: 0.3333333432674408\n",
      "#944 Loss: 0.3333333432674408\n",
      "#945 Loss: 0.3333333432674408\n",
      "#946 Loss: 0.3333333432674408\n",
      "#947 Loss: 0.3333333432674408\n",
      "#948 Loss: 0.3333333432674408\n",
      "#949 Loss: 0.3333333432674408\n",
      "#950 Loss: 0.3333333432674408\n",
      "#951 Loss: 0.3333333432674408\n",
      "#952 Loss: 0.3333333432674408\n",
      "#953 Loss: 0.3333333432674408\n",
      "#954 Loss: 0.3333333432674408\n",
      "#955 Loss: 0.3333333432674408\n",
      "#956 Loss: 0.3333333432674408\n",
      "#957 Loss: 0.3333333432674408\n",
      "#958 Loss: 0.3333333432674408\n",
      "#959 Loss: 0.3333333432674408\n",
      "#960 Loss: 0.3333333432674408\n",
      "#961 Loss: 0.3333333432674408\n",
      "#962 Loss: 0.3333333432674408\n",
      "#963 Loss: 0.3333333432674408\n",
      "#964 Loss: 0.3333333432674408\n",
      "#965 Loss: 0.3333333432674408\n",
      "#966 Loss: 0.3333333432674408\n",
      "#967 Loss: 0.3333333432674408\n",
      "#968 Loss: 0.3333333432674408\n",
      "#969 Loss: 0.3333333432674408\n",
      "#970 Loss: 0.3333333432674408\n",
      "#971 Loss: 0.3333333432674408\n",
      "#972 Loss: 0.3333333432674408\n",
      "#973 Loss: 0.3333333432674408\n",
      "#974 Loss: 0.3333333432674408\n",
      "#975 Loss: 0.3333333432674408\n",
      "#976 Loss: 0.3333333432674408\n",
      "#977 Loss: 0.3333333432674408\n",
      "#978 Loss: 0.3333333432674408\n",
      "#979 Loss: 0.3333333432674408\n",
      "#980 Loss: 0.3333333432674408\n",
      "#981 Loss: 0.3333333432674408\n",
      "#982 Loss: 0.3333333432674408\n",
      "#983 Loss: 0.3333333432674408\n",
      "#984 Loss: 0.3333333432674408\n",
      "#985 Loss: 0.3333333432674408\n",
      "#986 Loss: 0.3333333432674408\n",
      "#987 Loss: 0.3333333432674408\n",
      "#988 Loss: 0.3333333432674408\n",
      "#989 Loss: 0.3333333432674408\n",
      "#990 Loss: 0.3333333432674408\n",
      "#991 Loss: 0.3333333432674408\n",
      "#992 Loss: 0.3333333432674408\n",
      "#993 Loss: 0.3333333432674408\n",
      "#994 Loss: 0.3333333432674408\n",
      "#995 Loss: 0.3333333432674408\n",
      "#996 Loss: 0.3333333432674408\n",
      "#997 Loss: 0.3333333432674408\n",
      "#998 Loss: 0.3333333432674408\n",
      "#999 Loss: 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "for i in range(1000):  # trains the NN 1,000 times\n",
    "    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(x)) ** 2).detach().item()))  # mean sum squared loss\n",
    "    NN.train(x, y)\n",
    "NN.saveWeights(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "accuracy: 0.4011299435028249\n"
     ]
    }
   ],
   "source": [
    "predictions = NN.forward(x)\n",
    "print(\"------\")\n",
    "#print(predictions)\n",
    "#print(type(predictions))\n",
    "correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    #print(\"prediction: \" + str(predictions[i]))\n",
    "    #print(\"actual: \" + str(y[i]))\n",
    "    \n",
    "    pred = predictions[i].tolist()\n",
    "    actual = y[i].tolist()\n",
    "    if pred.index(max(pred)) == actual.index(max(actual)):\n",
    "        correct += 1\n",
    "\n",
    "print(\"accuracy: \" + str(correct / len(predictions)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "x = x.float()\n",
    "y = y.float()\n",
    "print(x.type())\n",
    "print(y.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7802,  0.7748,  0.9380,  ...,  0.7896,  0.7761,  0.9056],\n",
       "        [-0.3862, -0.1862, -0.3276,  ...,  0.4759, -0.1069,  0.4138],\n",
       "        [ 0.3251,  0.6533,  0.5480,  ...,  0.3994,  0.4675,  0.6966],\n",
       "        ...,\n",
       "        [ 0.2281,  0.2047,  0.0058,  ..., -0.3099, -0.2982, -0.2865],\n",
       "        [ 0.7000,  0.5850,  0.7250,  ..., -0.2200, -0.1900, -0.2000],\n",
       "        [ 0.2500,  0.4107,  0.7619,  ..., -0.0060,  0.0000, -0.3333]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add bias values to input and hidden layer!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "tensor([[ 0.7802, -0.3862,  0.3251,  ...,  0.2281,  0.7000,  0.2500],\n",
      "        [ 0.7748, -0.1862,  0.6533,  ...,  0.2047,  0.5850,  0.4107],\n",
      "        [ 0.9380, -0.3276,  0.5480,  ...,  0.0058,  0.7250,  0.7619],\n",
      "        ...,\n",
      "        [ 0.7896,  0.4759,  0.3994,  ..., -0.3099, -0.2200, -0.0060],\n",
      "        [ 0.7761, -0.1069,  0.4675,  ..., -0.2982, -0.1900,  0.0000],\n",
      "        [ 0.9056,  0.4138,  0.6966,  ..., -0.2865, -0.2000, -0.3333]])\n",
      "Output: \n",
      "tensor([[7.9406e-17, 2.3666e-07, 3.7494e-11],\n",
      "        [5.1035e-16, 1.3248e-07, 4.3855e-11],\n",
      "        [8.4645e-14, 1.6225e-07, 3.9584e-08],\n",
      "        [2.6061e-17, 1.2229e-07, 3.9222e-09],\n",
      "        [9.8707e-15, 2.7804e-07, 2.2305e-09],\n",
      "        [6.8918e-15, 4.7084e-07, 1.3709e-09],\n",
      "        [9.3432e-15, 3.2216e-07, 5.8709e-08],\n",
      "        [1.7848e-16, 2.2602e-07, 4.1539e-11],\n",
      "        [6.3615e-16, 2.4281e-07, 4.1475e-10],\n",
      "        [6.3757e-16, 9.5969e-08, 2.5043e-11],\n",
      "        [1.3418e-14, 4.1473e-07, 7.5559e-09],\n",
      "        [7.6585e-15, 2.9686e-07, 1.1376e-09],\n",
      "        [2.4899e-17, 2.1178e-07, 5.9712e-13],\n",
      "        [1.3179e-15, 1.4396e-07, 9.6571e-12],\n",
      "        [5.7452e-14, 2.5784e-07, 1.0342e-07],\n",
      "        [1.0394e-14, 2.0118e-07, 2.1166e-08],\n",
      "        [8.1238e-16, 2.6521e-07, 2.0237e-09],\n",
      "        [2.5017e-14, 2.0027e-07, 5.6343e-09],\n",
      "        [1.7961e-16, 1.0138e-07, 1.6257e-09],\n",
      "        [5.0404e-18, 1.1344e-07, 1.4889e-11],\n",
      "        [1.9371e-16, 1.2843e-07, 1.1974e-08],\n",
      "        [1.5492e-17, 2.0958e-07, 6.0525e-12],\n",
      "        [1.7462e-16, 3.9401e-07, 2.6175e-10],\n",
      "        [1.6039e-17, 2.7028e-07, 4.6046e-11],\n",
      "        [3.0631e-17, 3.4587e-07, 1.1600e-09],\n",
      "        [1.6692e-15, 3.7227e-07, 1.0077e-09],\n",
      "        [2.1184e-14, 5.1860e-07, 4.1316e-08],\n",
      "        [1.7510e-17, 1.8305e-07, 1.7129e-11],\n",
      "        [3.2209e-16, 3.8660e-07, 2.2303e-10],\n",
      "        [1.0011e-15, 1.6209e-07, 1.4182e-10],\n",
      "        [5.7251e-14, 1.9331e-07, 2.4242e-08],\n",
      "        [4.0034e-17, 4.1361e-07, 2.6706e-11],\n",
      "        [5.5357e-16, 6.3927e-07, 1.4854e-09],\n",
      "        [1.1370e-15, 2.5346e-07, 5.0143e-09],\n",
      "        [4.4729e-17, 1.9952e-07, 1.2002e-10],\n",
      "        [2.4139e-16, 2.3685e-07, 1.7045e-09],\n",
      "        [2.9352e-15, 2.7802e-07, 8.4956e-09],\n",
      "        [5.0479e-17, 2.5305e-07, 4.6590e-11],\n",
      "        [1.0439e-16, 1.1456e-07, 1.3900e-09],\n",
      "        [1.5623e-17, 2.0847e-07, 3.3492e-11],\n",
      "        [2.3065e-15, 1.4999e-07, 1.0604e-07],\n",
      "        [7.7454e-17, 6.9196e-08, 2.6309e-11],\n",
      "        [9.3875e-17, 9.6335e-08, 1.4794e-07],\n",
      "        [2.6910e-17, 2.2445e-07, 6.4689e-11],\n",
      "        [3.2634e-14, 1.6287e-07, 1.2576e-05],\n",
      "        [7.2661e-17, 1.0294e-07, 2.4233e-11],\n",
      "        [4.9973e-17, 1.1787e-07, 2.2828e-11],\n",
      "        [1.4481e-15, 2.2271e-07, 1.4579e-09],\n",
      "        [8.9242e-15, 3.0616e-07, 8.1713e-09],\n",
      "        [4.2192e-16, 1.0116e-07, 4.7190e-12],\n",
      "        [7.8347e-16, 1.5943e-07, 3.1478e-11],\n",
      "        [1.4842e-16, 1.4842e-07, 3.8921e-11],\n",
      "        [3.6335e-14, 4.0428e-07, 3.5580e-08],\n",
      "        [1.7498e-15, 2.2256e-07, 1.4072e-08],\n",
      "        [6.9261e-15, 1.7947e-07, 1.2099e-08],\n",
      "        [3.4085e-16, 1.9966e-07, 1.1013e-09],\n",
      "        [3.8231e-15, 1.7228e-07, 2.0233e-09],\n",
      "        [4.0261e-15, 1.0226e-07, 1.1649e-09],\n",
      "        [3.6904e-16, 3.0853e-07, 6.6973e-05],\n",
      "        [2.9267e-17, 4.8037e-07, 2.0408e-05],\n",
      "        [1.2568e-16, 1.3834e-07, 3.2039e-02],\n",
      "        [3.4037e-17, 3.7662e-07, 6.2483e-08],\n",
      "        [3.3190e-19, 5.4980e-08, 6.7901e-13],\n",
      "        [9.2789e-19, 1.7619e-07, 1.1637e-08],\n",
      "        [3.7678e-17, 4.4216e-07, 1.9224e-10],\n",
      "        [2.6310e-18, 3.2605e-07, 5.8660e-13],\n",
      "        [2.5922e-17, 1.7611e-06, 5.1648e-09],\n",
      "        [1.1033e-15, 1.0756e-06, 5.1579e-05],\n",
      "        [7.4028e-18, 1.2624e-07, 1.9377e-09],\n",
      "        [6.5422e-15, 2.9569e-07, 2.6562e-02],\n",
      "        [9.2167e-19, 9.4035e-08, 9.2948e-12],\n",
      "        [6.2614e-17, 3.2897e-07, 9.4062e-07],\n",
      "        [1.0920e-18, 5.5668e-08, 2.1807e-11],\n",
      "        [6.3667e-17, 1.2649e-07, 1.8932e-10],\n",
      "        [2.1243e-18, 7.5210e-08, 6.2153e-07],\n",
      "        [9.7845e-18, 4.3334e-07, 4.8782e-09],\n",
      "        [4.3457e-18, 4.8852e-08, 2.4717e-05],\n",
      "        [3.1613e-17, 4.8346e-07, 2.4303e-09],\n",
      "        [3.4574e-19, 6.6339e-08, 1.9942e-10],\n",
      "        [9.1352e-19, 5.9980e-07, 2.9626e-11],\n",
      "        [1.3778e-17, 2.8806e-07, 6.4754e-11],\n",
      "        [5.2609e-17, 2.7362e-06, 1.7742e-09],\n",
      "        [6.8117e-18, 1.2406e-08, 1.2346e-02],\n",
      "        [4.6228e-17, 8.1989e-07, 6.2678e-10],\n",
      "        [4.5130e-18, 6.3978e-07, 3.0195e-10],\n",
      "        [4.6527e-18, 6.0770e-07, 5.2069e-09],\n",
      "        [1.3061e-17, 1.4061e-06, 3.0618e-09],\n",
      "        [6.6844e-17, 1.3400e-06, 7.8982e-08],\n",
      "        [4.6360e-17, 4.5702e-06, 1.0660e-09],\n",
      "        [5.6380e-18, 7.3783e-07, 4.4956e-08],\n",
      "        [2.0278e-17, 1.1129e-06, 1.2392e-06],\n",
      "        [7.7836e-18, 6.5112e-07, 1.6029e-06],\n",
      "        [4.2364e-19, 7.7835e-08, 5.6501e-11],\n",
      "        [6.1015e-19, 5.3315e-08, 1.1941e-10],\n",
      "        [2.6911e-18, 9.2776e-08, 2.4438e-11],\n",
      "        [2.0435e-14, 1.2616e-07, 7.1313e-03],\n",
      "        [1.1879e-18, 2.0422e-07, 9.0230e-12],\n",
      "        [9.6035e-19, 9.7982e-08, 2.5019e-13],\n",
      "        [3.0417e-19, 7.8616e-08, 8.2664e-13],\n",
      "        [2.8287e-18, 1.6660e-07, 1.0636e-10],\n",
      "        [4.5266e-17, 1.1015e-06, 4.8124e-08],\n",
      "        [4.0257e-18, 1.8936e-07, 1.9085e-08],\n",
      "        [4.8738e-18, 5.1838e-07, 1.3249e-08],\n",
      "        [2.1593e-17, 1.4656e-06, 9.0218e-10],\n",
      "        [7.4560e-19, 8.8662e-07, 3.4570e-08],\n",
      "        [2.0683e-17, 1.5956e-06, 3.7543e-09],\n",
      "        [2.1861e-17, 7.5489e-07, 9.8447e-07],\n",
      "        [4.5922e-18, 1.4220e-06, 1.7015e-09],\n",
      "        [6.3200e-18, 3.2109e-07, 2.9361e-12],\n",
      "        [5.1212e-18, 8.3141e-08, 3.1864e-11],\n",
      "        [8.1753e-19, 8.4106e-08, 3.1257e-09],\n",
      "        [1.5635e-18, 1.0749e-07, 4.1649e-07],\n",
      "        [1.2828e-17, 1.7535e-06, 3.7172e-09],\n",
      "        [4.4277e-18, 1.5058e-06, 2.8121e-09],\n",
      "        [1.0144e-18, 1.2221e-06, 5.8645e-12],\n",
      "        [1.1822e-17, 1.5841e-06, 2.6937e-09],\n",
      "        [9.8577e-19, 3.4777e-07, 2.3723e-09],\n",
      "        [1.5324e-17, 1.0423e-08, 1.7417e-02],\n",
      "        [2.9944e-18, 1.5189e-07, 3.1628e-09],\n",
      "        [3.6420e-18, 1.9604e-07, 1.6371e-10],\n",
      "        [4.8599e-19, 2.8132e-07, 8.6008e-12],\n",
      "        [4.4174e-19, 2.9976e-08, 5.0883e-08],\n",
      "        [1.2058e-17, 1.0849e-07, 4.1602e-08],\n",
      "        [1.5335e-18, 7.2696e-08, 1.5942e-11],\n",
      "        [1.2681e-18, 4.3104e-07, 6.0655e-10],\n",
      "        [4.7156e-18, 9.3497e-07, 1.2069e-09],\n",
      "        [6.8110e-18, 8.6197e-07, 5.8111e-08],\n",
      "        [3.8073e-18, 1.1281e-06, 1.1571e-09],\n",
      "        [3.9472e-17, 3.8415e-08, 2.4062e-05],\n",
      "        [6.7330e-14, 2.6964e-08, 9.7153e-01],\n",
      "        [2.1274e-14, 4.8751e-09, 9.9976e-01],\n",
      "        [4.3574e-14, 1.1739e-08, 9.9993e-01],\n",
      "        [6.5252e-14, 6.1506e-09, 9.9972e-01],\n",
      "        [3.7940e-14, 3.4609e-07, 9.6677e-01],\n",
      "        [4.3462e-14, 7.9816e-08, 9.9987e-01],\n",
      "        [6.6324e-15, 3.2598e-09, 9.9986e-01],\n",
      "        [8.3192e-16, 2.0625e-09, 9.9965e-01],\n",
      "        [5.3879e-15, 1.2318e-08, 9.9937e-01],\n",
      "        [1.7586e-14, 4.8285e-08, 9.8669e-01],\n",
      "        [1.0902e-14, 4.2179e-08, 9.9537e-01],\n",
      "        [2.9266e-13, 5.2037e-08, 9.9971e-01],\n",
      "        [2.6508e-15, 1.0172e-08, 9.9649e-01],\n",
      "        [1.5363e-15, 5.0809e-09, 9.8192e-01],\n",
      "        [1.1024e-13, 6.2383e-09, 9.9999e-01],\n",
      "        [3.0226e-14, 1.1666e-08, 9.9962e-01],\n",
      "        [1.6425e-14, 2.9616e-09, 9.9999e-01],\n",
      "        [3.3061e-14, 4.7025e-09, 9.9999e-01],\n",
      "        [1.0434e-13, 1.3576e-08, 9.9999e-01],\n",
      "        [4.2126e-14, 4.0968e-09, 1.0000e+00],\n",
      "        [4.9023e-14, 3.5763e-09, 9.9999e-01],\n",
      "        [1.1104e-13, 6.9867e-09, 1.0000e+00],\n",
      "        [2.2656e-13, 1.7620e-08, 9.9981e-01],\n",
      "        [1.8343e-13, 3.9400e-08, 9.9999e-01],\n",
      "        [2.2311e-13, 1.5598e-07, 9.9997e-01],\n",
      "        [3.5887e-14, 5.8063e-09, 9.9999e-01],\n",
      "        [4.0424e-14, 5.4969e-09, 9.9999e-01],\n",
      "        [1.6673e-13, 7.5842e-08, 9.9999e-01],\n",
      "        [3.7562e-12, 2.7130e-07, 9.9998e-01],\n",
      "        [1.7090e-12, 1.9128e-07, 9.9999e-01],\n",
      "        [1.7266e-14, 6.9981e-09, 9.9998e-01],\n",
      "        [1.5414e-14, 8.2320e-09, 9.9962e-01],\n",
      "        [2.8107e-15, 2.1507e-08, 9.9745e-01],\n",
      "        [2.0435e-14, 6.0779e-09, 9.9990e-01],\n",
      "        [1.5336e-13, 2.3319e-08, 1.0000e+00],\n",
      "        [9.1680e-15, 4.6966e-09, 9.9995e-01],\n",
      "        [3.1141e-14, 4.0097e-09, 1.0000e+00],\n",
      "        [8.2202e-14, 1.0218e-08, 1.0000e+00],\n",
      "        [1.5553e-13, 1.9825e-08, 9.9999e-01],\n",
      "        [6.8547e-14, 3.8238e-09, 9.9999e-01],\n",
      "        [1.8144e-14, 1.1173e-08, 9.9994e-01],\n",
      "        [1.2861e-13, 5.7634e-08, 1.0000e+00],\n",
      "        [2.8865e-13, 2.8483e-08, 9.9999e-01],\n",
      "        [3.6773e-14, 3.7596e-09, 9.9999e-01],\n",
      "        [4.1262e-14, 5.2505e-09, 9.9999e-01],\n",
      "        [5.8590e-14, 4.5932e-09, 1.0000e+00],\n",
      "        [2.0229e-13, 3.5942e-08, 1.0000e+00],\n",
      "        [3.0843e-14, 5.4640e-09, 9.9999e-01]])\n"
     ]
    }
   ],
   "source": [
    "NN.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/home/dom/Documents/MPhys/TheGrandTour/wine_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([177, 13])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([177, 14])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, torch.ones(x.size()[0], 1)), 1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randn(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 20])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 19])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.narrow(1, 0, 19).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
